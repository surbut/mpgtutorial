---
title: "Aladynoulli: Borrowing Information to Predict Disease"
subtitle: "A Statistical Journey in Medicine"
author: "Sarah Urbut, MD PhD and Giovanni Parmigiani, PhD"
format: 
  revealjs:
    theme: simple
    transition: slide
    slide-number: true
editor: 
  markdown: 
    wrap: 72
---

## My Journey: Bridging Multiple Worlds

::::: columns
::: {.column width="60%"}
-   Cardiologist, scientist, geneticist, and statistician ... confused
-   PhD focused on **mixture models**: like me?
-   Medicine and science enhance our understanding of disease
-   Combining statistical expertise with clinical experience
-   Came to Boston ... Met the best mentors of all time!
:::

::: {.column width="40%"}
![](aladynoulli-diverse-diagnoses.svg)
:::
:::::

## The Statistical Paradox in Medicine

::::: columns
::: {.column width="60%"}
-   One of my first consults as a cardiology fellow revealed a
    **paradox**:
-   A 38-year-old with clear signs of a heart attack
-   Yet just a day earlier, they wouldn't have qualified for prevention
-   Why? Our reliance on **short-term and narrowly defined risk**
-   A fundamental flaw: **current models fail to capture how disease
    evolves dynamically atop a genetic background**
:::

::: {.column width="40%"}
![A pager - the doctor's ball and chain](pager.svg)
:::
:::::

## The Challenge

:::: columns
::: {.column width="100%"}
People develop multiple diseases over their lifetime, but:

-   Diseases don't occur in isolation
-   Comorbidities follow patterns, but these vary by individual
-   Genetics influences these patterns (but isn't destiny)
-   Standard approaches miss the dynamic nature of these relationships

To both predict and understand, we need to consider past, present, and
future.

**How do we model the complete trajectory of an individual's health,
incorporating both genetics and evolving clinical status?**
:::
::::

## What Are "Latent Variables"?

::::: columns
::: {.column width="60%"}
-   Things we can't directly observe
-   Must be inferred from what we can measure
-   Examples in everyday life:
    -   Intelligence (measured by tests)
    -   Happiness (measured by behaviors)
    -   Disease patterns (measured by symptoms and test results)
-   Powerful statistical concept for finding hidden structure
:::

::: {.column width="40%"}
![](latent-variable-diagram.svg)
:::
:::::

## A Simple Medical Example

:::: columns
::: {.column width="100%"}
```         
Observable:               Hidden:                  Observable:
                          _________
Fever                    |         |
                   -----> DISEASE  |------> Lab Values  
Cough               |    |_________|    |
                    |                   |
Pain  --------------                    -------> Imaging Findings
```

The disease is a **latent variable** we infer from observable symptoms
and test results
:::
::::

## Latent variables

```{r}
#| label: pca-plot
#| fig-cap: "Finding directions of maximum variation with PCA"

# Step 1: Simulate Data
set.seed(123)
n <- 10
troponin <- rnorm(n, mean = 5, sd = 1)
ckmb <- troponin * 1.2 + rnorm(n, mean = 0, sd = 0.5)
data <- data.frame(Troponin = troponin, CKMB = ckmb)

# Step 2: Run PCA
pca_result <- prcomp(data, scale. = TRUE)

# Step 3: Plot
library(ggplot2)

# Get mean of the data for arrows
center <- colMeans(scale(data))

# PC directions
pc1 <- pca_result$rotation[,1]
pc2 <- pca_result$rotation[,2]

# Scale for plotting arrows
arrow_length <- 3

# Plot
ggplot(data, aes(x = Troponin, y = CKMB)) +
  geom_point(size = 3) +
  geom_segment(aes(x = mean(Troponin), y = mean(CKMB),
                   xend = mean(Troponin) + arrow_length * pc1[1],
                   yend = mean(CKMB) + arrow_length * pc1[2]),
               arrow = arrow(length = unit(0.3, "cm")),
               color = "blue", size = 1.2) +
  geom_segment(aes(x = mean(Troponin), y = mean(CKMB),
                   xend = mean(Troponin) + arrow_length * pc2[1],
                   yend = mean(CKMB) + arrow_length * pc2[2]),
               arrow = arrow(length = unit(0.3, "cm")),
               color = "red", size = 1.2) +
  theme_minimal() +
  labs(title = "PCA: Finding the Directions of Maximum Difference",
       x = "Troponin",
       y = "CK-MB") +
  annotate("text", x = mean(troponin) + 2.5 * pc1[1], y = mean(ckmb) + 2.5 * pc1[2],
           label = "PC1", color = "blue", size = 5, hjust = 0) +
  annotate("text", x = mean(troponin) + 2.5 * pc2[1], y = mean(ckmb) + 2.5 * pc2[2],
           label = "PC2", color = "red", size = 5, hjust = 0)
```

## PC1

You’ll see PC1 has a bigger spread — it separates patients more strongly.

Summary: More spread → better ability to separate patients by severity.

Summary: More spread → better ability to separate patients by severity.

```{r}
# Step 1: Simulate Data
set.seed(123)
n <- 10
troponin <- rnorm(n, mean = 5, sd = 1)
ckmb <- troponin * 1.2 + rnorm(n, mean = 0, sd = 0.5)
data <- data.frame(Troponin = troponin, CKMB = ckmb)

# Step 2: Run PCA
pca_result <- prcomp(data, scale. = TRUE)

# Step 3: Calculate the ranges
range_troponin <- diff(range(data$Troponin))
range_ckmb <- diff(range(data$CKMB))
range_pc1 <- diff(range(pca_result$x[,1]))

# Step 4: Put into a clean table
summary_stats <- data.frame(
  Variable = c("Troponin", "CKMB", "PC1 Score"),
  Range = c(range_troponin, range_ckmb, range_pc1)
)


# Step 6: Plot it
library(ggplot2)

ggplot(summary_stats, aes(x = Variable, y = Range, fill = Variable)) +
  geom_col(width = 0.5) +
  labs(title = "Spread of Patient Values by Variable",
       y = "Range (Max - Min)",
       x = "") +
  theme_minimal() +
  theme(legend.position = "none")

```

::: notes
Words you say: First direction stretches patients apart the most.
PC1 captures more information than using just Troponin or CK-MB alone.
:::

## Understanding Latent Variables

![](images/pc-comparison-diagram.svg){width="600"}

::: notes
When two (or more) biomarkers are highly correlated, they contain
partially redundant information. Find the direction that maximizes
variation, which often means combining information from both markers,
resolving more differences than either alone. This is why these
statistical techniques are so powerful for medical applications. Rather
than looking at dozens of individual biomarkers in isolation, you can
create composite scores that capture more of the underlying biological
signal (like cardiac damage) Reduce noise by combining multiple
measurements and create better separation between patients with
different disease severity this concept extends beautifully to
time-varying data, binary outcomes, and the complex patterns of
comorbidities with Aladynoulli model!
:::

## Correlated biomarkers

Troponin and CK-MB

![](latent-variable-simple.svg)

## Disease "Signatures"

::::: columns
::: {.column width="60%"}
We introduce and define **signatures** as:

-   **Latent patterns of disease co-occurrence that vary in time**
-   Learned from data in an unsupervised way
-   Example signatures:
    -   Cardiovascular (hypercholesterolemia → CAD → heart failure)
    -   Metabolic (diabetes, obesity, fatty liver)
    -   Neoplastic (various cancers, anemia)
-   Discovered through statistical methods, not predefined categories
:::

::: {.column width="40%"}
![](disease-signatures-diagram.svg)
:::
:::::

## How Signatures Change Over Time

:::: columns
::: {.column width="100%"}
![Diseases don't all happen at once - they follow patterns over
time](cvdsig.svg)
:::
::::

## Patient Trajectories Vary

::::: columns
::: {.column width="60%"}
**Patients don't sit still** - their signature profiles change over
time:

**Example patient journey:** - Rheumatologic diagnoses at age 30 -
Premature coronary artery disease at age 40 - Increased neoplastic risk
by age 50

This story is distinct from other patients, **revealing heterogeneity
underlying a shared phenotype**
:::

::: {.column width="40%"}
![](patient-trajectory-diagram.svg)
:::
:::::

## Borrowing Information

::::: columns
::: {.column width="60%"}
**Key Statistical Insight:**

We can "borrow information" from similar patients to make better
predictions

-   If we see Patient A with pattern X develop disease Y
-   And Patient B shows early signs of pattern X
-   We can better predict B's risk of disease Y
:::

::: {.column width="40%"}
![Connecting the dots across patients](histories.svg)
:::
:::::

## The Bayesian Approach

:::: columns
::: {.column width="100%"}
Mathematical way to update our beliefs as new information arrives:

$$\text{Updated Belief} \propto \text{Prior Knowledge} \times \text{New Data}$$

In practical terms: - Start with what we know about disease patterns in
general - Update based on this specific patient's data - Continually
refine as more information comes in
:::
::::

## A Simple Bayesian Example

Predicting heart attack risk:

1.  **Prior**: 5% of people with high blood pressure have heart attacks
2.  **New Data**: Our patient has chest pain
3.  **Updated Belief**: Risk is now 15% given both factors

This is "borrowing information" from population studies to help with
individual prediction.

## The Mathematical Model (Simplified)

Turning patient stories into math helps us formalize the problem:

-   **ϕ (phi)** - Time-varying disease signatures
    -   How different diseases evolve over time within a signature
    -   Centered around population incidence
-   **λ (lambda)** - Individual's time-varying signature loading
    -   Anchored on germline genetic predisposition
    -   Shows which signatures are important for a specific person
-   Both evolve smoothly over time (using Gaussian processes)
-   Helps us "borrow information" across years

## The Mathematical Model (Simplified)

![](bayesian-model-diagram.svg)

## The Bayesian Magic

::::: columns
::: {.column width="60%"}
Our approach works because:

1.  We start with prior beliefs (based on population patterns)
2.  **Each new diagnosis refines our model parameters in real time**
3.  We integrate prior knowledge with patient-specific data
4.  The past informs the present, and the present becomes the past to
    inform the future

Four key innovations:

1.  **Bridges discovery and prediction**
2.  **Disease signatures evolve over time and across diseases**
3.  **Anchoring in genomics** produces biologically meaningful
    parameters
4.  **Bayesian updating adapts estimates in real time**
:::

::: {.column width="40%"}
![](bayesian-updating-diagram.svg)
:::
:::::

## Why This Matters

::::: columns
::: {.column width="60%"}
-   Earlier detection of disease
-   More personalized prevention strategies
-   Better understanding of disease mechanisms
-   Identifying new subtypes of diseases
-   Improving clinical trials by targeting the right patients
:::

::: {.column width="40%"}
![Better prevention and prediction](fig5.svg)
:::
:::::

## Case Study: Early Heart Disease

:::: columns
::: {.column width="100%"}
![A patient's signature profile can predict risk years before disease
appears](figure3_patient_example_2.svg)
:::
::::

## Why This Work Matters

:::: columns
::: {.column width="100%"}
This work is the essence of translational medicine:

-   Bridges basic, methodologic, and clinical science
-   Requires both math and biology appreciation
-   Transcends single-discipline medicine
-   Transforms how we predict, prevent, and understand disease
    progression

What statistics has taught me: - Complex systems can be modeled but
require careful assumptions - Uncertainty should be embraced, not
ignored\
- Borrowing information is powerful but must be done carefully - The
best models combine mathematical elegance with real-world knowledge -
Clinical intuition + statistical rigor = better medicine
:::
::::

## Resources & Questions

::::: columns
::: {.column width="60%"}
-   Paper:
    [medrxiv.org/content/10.1101/2024.09.29.24314557V1](https://www.medrxiv.org/content/10.1101/2024.09.29.24314557V1)
-   Code:
    [github.com/surbut/Aladynoulli2](https://github.com/surbut/Aladynoulli2)
-   Interactive app:
    [Surbut.shinyapps.io/berndiffapp](https://Surbut.shinyapps.io/berndiffapp)
:::

::: {.column width="40%"}
![QR code to resources](https://i.imgur.com/dummy-qr.jpg)
:::
:::::

## Thank You!

Statistics can help us understand health in ways that transform
medicine.

(And yes, it's worth learning for your Stat 102 class!)

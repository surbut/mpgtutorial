---
title: "A Practical Primer on Bayesian Statistics"
author: "Sarah Urbut, MD PhD"
date: "March 29, 2025"
format: 
  revealjs:
    theme: simple
    slide-number: true
    incremental: false
    transition: slide
    code-fold: true
    code-tools: true
    highlight-style: github
    css: custom.css
---

```{r setup, include=FALSE}
# Load necessary packages
library(tidyverse)
library(ggplot2)
library(reshape2)
library(viridis)
library(gtools)     # For Dirichlet distribution
library(gridExtra)  # For arranging plots

# Set a clean theme for all plots
theme_set(theme_minimal(base_size = 12) + 
          theme(plot.title = element_text(hjust = 0.5),
                plot.subtitle = element_text(hjust = 0.5)))

# Set seed for reproducibility
set.seed(42)
```

## Overview

In this seminar, we'll cover:

1. Introduction to Bayesian thinking and posterior distributions
2. The p-value fallacy and Bayesian alternatives
3. Conjugate priors for genetic models (Beta-Binomial, Dirichlet)
4. Mixture models for population structure 
5. Clinical trials with flat priors - lessons for genomics
6. Multivariate normal mixtures (mash)

# Introduction to Bayesian Thinking

## Why Bayesian for Population Genomics?

Population genomics presents unique challenges:

- **Multiple testing** across thousands/millions of variants
- **Complex patterns** across populations and traits
- **Prior knowledge** from evolution and previous studies
- **Decision-making** under uncertainty

Bayesian approaches offer elegant solutions to these challenges.

## Probabilistic Interpretation of Estimates

::: {.columns}
::: {.column width="50%"}
In the Bayesian framework:

- **Parameters are random variables** with distributions, not fixed values
- **Uncertainty is represented directly** through probability distributions
- **All evidence is integrated coherently** within a probability framework
- **Natural quantification of uncertainty** without hypothetical repeated sampling
- **Interpretation is direct and intuitive** for researchers and clinicians
:::

::: {.column width="50%"}
```{r}
# Create example data for frequentist vs Bayesian comparison
x <- seq(0.2, 0.8, length = 1000)

# Bayesian posterior (beta distribution)
post_y <- dbeta(x, 70, 30)

# Create data frame for plotting
plot_data <- data.frame(
  x = x,
  y = post_y
)

# Point estimate
point_est <- 0.7
ci_lower <- 0.61
ci_upper <- 0.79
cred_lower <- qbeta(0.025, 70, 30)
cred_upper <- qbeta(0.975, 70, 30)

# Get maximum y value for annotation positioning
max_y_val <- max(plot_data$y)

# Plot
ggplot(plot_data, aes(x = x, y = y)) +
  geom_line(size = 1.2, color = "darkblue") +
  geom_vline(xintercept = point_est, color = "red", linetype = "dashed") +
  geom_segment(aes(x = ci_lower, xend = ci_upper, y = max_y_val/10, yend = max_y_val/10), 
               color = "darkred", size = 2) +
  geom_segment(aes(x = cred_lower, xend = cred_upper, y = max_y_val/5, yend = max_y_val/5), 
               color = "darkblue", size = 2) +
  annotate("text", x = point_est + 0.05, y = max_y_val/1.5, 
           label = "Point Estimate", color = "red") +
  annotate("text", x = ci_lower - 0.05, y = max_y_val/10, 
           label = "95% CI", color = "darkred") +
  annotate("text", x = cred_upper + 0.05, y = max_y_val/5, 
           label = "95% Credible Interval", color = "darkblue") +
  labs(title = "Bayesian Posterior Distribution",
       subtitle = "Directly interpretable probability statements about parameters",
       x = "Parameter Value", 
       y = "Posterior Density") +
  theme_minimal()
```
:::
:::

## Bayesian vs. Frequentist Intervals

| Bayesian Credible Interval | Frequentist Confidence Interval |
|----------------------------|--------------------------------|
| "95% probability the parameter is between a and b" | "If we repeated the experiment many times, 95% of intervals constructed would contain the true parameter" |
| Directly interpretable as probability statement about the parameter | Cannot be interpreted as probability statement about the parameter |
| Incorporates prior information | No mechanism to incorporate prior information |
| Can be asymmetric, reflecting asymmetric uncertainty | Typically symmetric by construction |
| Conditioning on the observed data | Based on hypothetical repeated sampling |

## Overview

In this primer, we'll explore key Bayesian concepts critical for modern genomics:

1.  **P-values vs. Posterior Probabilities**: 
Why Bayesian thinking helps avoid misinterpretations
2.  **Conjugate Models**: Elegant solutions for population genetic inference
3.  **Mixture Models**: Powerful tools for complex genomic data
4.  **Bayesian Clinical & Adaptive Designs**: Learning and adapting as data accumulates

## Bayes' Theorem - The Core Idea

$$P(H|D) = \frac{P(D|H) \times P(H)}{P(D)}$$

Where:

- $P(H|D)$ is the **posterior probability** - what we want to know
- $P(D|H)$ is the **likelihood** - how probable the data is under our hypothesis
- $P(H)$ is the **prior probability** - what we knew before
- $P(D)$ is the **evidence** - a normalizing constant

Simply: **Posterior ∝ Likelihood × Prior**

## Bayesian Updating: Visual Intuition

```{r bayesian-updating}
# Create data for three distributions
x <- seq(0.001, 0.999, length = 1000)
prior <- data.frame(x = x, y = dbeta(x, 2, 3), Distribution = "Prior: Beta(2,3)")
likelihood <- data.frame(x = x, y = dbeta(x, 7, 3), Distribution = "Likelihood (Data)")
posterior <- data.frame(x = x, y = dbeta(x, 9, 6), Distribution = "Posterior: Beta(9,6)")

# Combine data
all_distributions <- rbind(prior, likelihood, posterior)

# Plot all three distributions
ggplot(all_distributions, aes(x = x, y = y, color = Distribution)) +
  geom_line(size = 1.2) +
  scale_color_manual(values = c("darkgreen", "purple", "red")) +
  labs(title = "Bayesian Updating of Allele Frequency Estimate",
       subtitle = "Combining prior knowledge with new data",
       x = "Allele Frequency", 
       y = "Density") +
  theme(legend.position = "bottom")
```

# P-values vs. Posterior Probabilities 

## The Question

As scientists, we want to know:

> "What is the probability that my hypothesis is true, given my data?"

But traditional p-values answer a different question:

> "What is the probability of observing data this extreme or more extreme, if the null hypothesis is true?"

This mismatch causes persistent misinterpretations.

## P-values vs. Bayes Factors: Definitions

**P-value**:
- $P(data|H_0)$ - probability of data given null hypothesis
- Measures compatibility of data with null hypothesis
- Does not directly measure evidence for alternative

**Bayes Factor**:
- $BF_{10} = \frac{P(data|H_1)}{P(data|H_0)}$ - ratio of likelihoods
- Directly compares evidence for alternative vs. null
- Tells you how much to update your beliefs

## The P-value Fallacy

::: {.columns}
::: {.column width="50%"}
**Scenario**: Testing a SNP for disease association

**Traditional approach**:
- Obtain p = 0.001
- Declare "significant association"
- Publish result

**The fallacy**:
- p = 0.001 means "1 in 1000 chance of seeing this data if no association exists"
- NOT "999 in 1000 chance the association is real"
:::

::: {.column width="50%"}
```{r}
# Parameters
prior_prob <- 1/1000  # Prior probability of true association
p_value <- 0.001      # Observed p-value

# Convert p-value to z-score
z <- qnorm(p_value/2, lower.tail = FALSE)

# Calculate Bayes factor (approximate)
bf <- exp(z^2/2)

# Calculate posterior probability
posterior <- (prior_prob * bf) / (prior_prob * bf + (1 - prior_prob))

# Create data for visualization
df <- data.frame(
  Stage = factor(c("Prior", "p-value", "Posterior"), 
                levels = c("Prior", "p-value", "Posterior")),
  Probability = c(prior_prob, 1-p_value, posterior),
  Label = c(paste0(round(prior_prob*100, 2), "%"), 
           paste0(round((1-p_value)*100, 1), "%"), 
           paste0(round(posterior*100, 1), "%"))
)

# Create plot
ggplot(df, aes(x = Stage, y = Probability)) +
  geom_bar(stat = "identity", fill = c("lightblue", "orange", "darkgreen"), width = 0.6) +
  geom_text(aes(label = Label), vjust = -0.5, size = 5) +
  ylim(0, 1) +
  labs(title = "The p-value Fallacy",
       subtitle = "p = 0.001 doesn't mean 99.9% chance of true association (^app)",
       x = "", y = "Probability") +
  theme_minimal(base_size = 14)
```
:::
:::

**Key insight**: With a realistic prior of 1/1000, a "significant" p-value of 0.001 only gives ~50% posterior probability of true association!

## The Mathematical Connection

Under certain conditions, p-values can be converted to minimum Bayes factors:

$$BF_{min} ≈ -e \times p \times \log(p)$$

Meaning even the most favorable interpretation of a p-value provides less evidence than typically assumed:

| p-value | Minimum Bayes Factor |
|---------|---------------------|
| 0.05    | 0.37                |
| 0.01    | 0.084               |
| 0.001   | 0.0083              |

## What is the Minimum Bayes Factor?

The Minimum Bayes Factor is calculated as:
MBF≈−e×p×log⁡(p)\text{MBF} \approx -e \times p \times \log(p)MBF≈−e×p×log(p)

This formula (derived by Sellke, Bayarri, and Berger) represents the **smallest possible Bayes factor** 
that could correspond to a given p-value, regardless of the specific alternative hypothesis being tested.
---

## Why "Minimum"?
It's called "minimum" because:

- It assumes the most favorable conditions for the alternative hypothesis
- It represents the strongest possible evidence against the null that could be derived from a p-value
It's the smallest value the Bayes factor could take (stronger evidence against null = smaller Bayes factor)

## Interpretation

The MBF represents the ratio of likelihoods:
MBF=P(data∣H0)P(data∣H1)\text{MBF} = \frac{P(\text{data}|H_0)}{P(\text{data}|H_1)}MBF=P(data∣H1​)P(data∣H0​)​

For example, with p = 0.05:

MBF = 0.37
This means the data are at most 1/0.37 ≈ 2.7 times more likely under the alternative than the null
Even with the most optimistic assumptions, the evidence against the null is modest

With p = 0.001:

MBF = 0.0083
This means the data are at most 1/0.0083 ≈ 120 times more likely under the alternative
Much stronger evidence, but still not as extreme as the very small p-value might suggest

## Why This Conversion Matters
This conversion from p-values to MBF is important because:

It provides a more calibrated interpretation of statistical evidence
It shows that conventional "statistical significance" (p < 0.05) actually represents fairly modest evidence
It helps researchers avoid overinterpreting p-values
It establishes a link between frequentist and Bayesian approaches

The key point is that p-values systematically overstate the evidence against the null hypothesis. 

When converted to the Bayes factor scale, even a seemingly impressive p-value often translates to much more moderate evidence against the null hypothesis than most researchers would expect.

## P-values vs. Bayes Factors in Genomics

```{r}
# Create data
p_values <- 10^seq(-8, -2, 0.5)
log10_p <- -log10(p_values)
min_bf <- -exp(1) * p_values * log(p_values)
log10_min_bf <- -log10(min_bf)

# Create plot data
plot_data <- data.frame(
  log10_p = log10_p,
  log10_min_bf = log10_min_bf
)

# Plot
ggplot(plot_data, aes(x = log10_p, y = log10_min_bf)) +
  geom_line(size = 1.5, color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  geom_vline(xintercept = -log10(5e-8), linetype = "dotted", color = "darkgreen") +
  annotate("text", x = 8, y = 4, label = "GWAS significance\nthreshold", color = "darkgreen") +
  labs(title = "Strength of Evidence: P-values vs. Bayes Factors",
       x = "-log10(p-value)",
       y = "-log10(Minimum Bayes Factor)") +
  theme_minimal(base_size = 14)
```

The GWAS significance threshold of p < 5×10⁻⁸ corresponds to much stronger evidence than p = 0.05.

## Interpreting Bayes Factors

Bayes factors have a natural interpretation:

| Bayes Factor (BF_{10}) | Evidence for H1 |
|---------------------|-----------------|
| 1 - 3               | Barely worth mentioning |
| 3 - 10              | Substantial |
| 10 - 30             | Strong |
| 30 - 100            | Very strong |
| > 100               | Extreme |

A BF_{10} = 10 means the data are 10 times more likely under H1 than H0.

## From Bayes Factor to Posterior Probability

Bayes' theorem connects all the pieces:

$$P(H_1|data) = \frac{P(data|H_1)P(H_1)}{P(data|H_1)P(H_1) + P(data|H_0)P(H_0)}$$

This can be rewritten using the Bayes factor:

$$P(H_1|data) = \frac{BF_{10} \times P(H_1)}{BF_{10} \times P(H_1) + P(H_0)}$$

## Posterior Odds Formulation

A simplified version:

$$\text{Posterior Odds} = \text{Bayes Factor} \times \text{Prior Odds}$$

Or:

$$\frac{P(H_1|data)}{P(H_0|data)} = BF_{10} \times \frac{P(H_1)}{P(H_0)}$$

This clearly shows how Bayes factors calibrate our prior beliefs.

## Practical Example: GWAS

In a genome-wide association study:

- Prior probability of true association: ~1/10,000 per variant
- p-value threshold: 5×10^{-8} 
- Corresponding minimum Bayes factor: ~10^{-6}
- Posterior probability: ~9%

**Interpretation**: Even at genome-wide significance, most "discoveries" may be false positives without additional evidence!


## A useful formula
There is another way of laying out this kind of calculation, which may be slightly easier to interpret and remember, and also has the advantage of holding even when more than two models are under consideration. From Bayes theorem we have

Pr(Zi=1|xi)=Pr(xi|Zi=1)Pr(Zi=1)/Pr(xi).

and

Pr(Zi=0|xi)=Pr(xi|Zi=0)Pr(Zi=0)/Pr(xi).

Taking the ratio of these gives
Pr(Zi=1|xi)/Pr(Zi=0|xi)=[Pr(Zi=1)/Pr(Zi=0)]×[Pr(xi|Zi=1)/Pr(xi|Zi=0)].

This formula can be conveniently stated in words, using the notion of ``odds“, as follows:
Posterior Odds = Prior Odds x LR
or, recalling that the LR is sometimes referred to as the Bayes Factor (BF), we have
Posterior Odds = Prior Odds x BF.

Note that the “Odds” of an event E1 vs an event E2 means the ratio of their probabilities. So Pr(Zi=1)/Pr(Zi=0) is the “Odds” of Zi=1 vs Zi=0. It is referred to as the “Prior Odds”, because it is the odds prior to seeing the data x. Similarly the Posterior Odds refers to the Odds of Zi=1 vs Zi=0 “posterior to” (after) seeing the data x.


## Benefits of Bayes Factors for Genomics

1. **Calibrated evidence**: Direct measure of evidence strength
2. **Multiple testing**: Naturally incorporates prior odds
3. **Replication**: Coherent framework for combining evidence across studies
4. **Diverse hypotheses**: Can compare non-nested models
5. **Positive evidence**: Can support null hypothesis, not just reject it
6. **Study design**: Allows stopping when evidence is sufficient

## Key Takeaways

1. P-values answer a different question than most scientists ask
2. Bayes factors directly compare competing hypotheses
3. Even "significant" p-values provide weaker evidence than typically assumed
4. Bayes factors have a natural interpretation as evidence strength
5. Converting to posterior probabilities requires considering prior odds
6. In genomics, this perspective helps manage false discovery rates

## The Fallacy of P-values

::: {.incremental}
-   P-values answer a **counterfactual question**: "If there were no effect, how surprising would these data be?"

-   But researchers want to know: "**What is the probability this association is real?**"

-   This disconnect leads to systematic misinterpretation
:::


## The Multiple Testing Challenge

Modern genomics routinely tests **thousands to millions** of hypotheses:

- 20,000+ genes in differential expression
- Millions of variants in GWAS
- Billions of potential interactions

**The consequence**: Many "significant" findings are actually false positives.

## The Traditional Approach

When testing m hypotheses at significance level α:

- Expected number of false positives: m × α
- With m = 1,000,000 and α = 0.05: **50,000 false positives!**

**Frequentist solutions**:
- Bonferroni correction: α/m
- False Discovery Rate (FDR) control (Benjamini-Hochberg)
- Family-wise error rate (FWER) control

## The Fundamental Issue

The p-value doesn't tell us what we want to know:

$\text{P-value} = P(\text{data}|\text{null})$

What we want is:

$P(\text{null}|\text{data})$

To bridge this gap, we need **Bayes' theorem**.

## The Bayesian Formulation

We model each test as:

- $H_0$: No effect (null hypothesis)
- $H_1$: Real effect (alternative hypothesis)
- Prior probability: $P(H_1) = \pi_1$
- Likelihood ratio: $BF = \frac{P(\text{data}|H_1)}{P(\text{data}|H_0)}$

The posterior probability of a true finding:

$$P(H_1|\text{data}) = \frac{BF \times \pi_1}{BF \times \pi_1 + (1-\pi_1)}$$

## Visualizing Multiple Hypothesis Testing

```{r}
# Create illustration of multiple testing
set.seed(123)

# Parameters
n_tests <- 10000
pi1 <- 0.05  # Proportion of true effects
effect_size <- 0.5

# Generate true status
is_true <- rbinom(n_tests, 1, pi1)
n_true <- sum(is_true)

# Generate z-scores
z_scores <- rnorm(n_tests, mean = is_true * effect_size, sd = 1)
p_values <- 2 * pnorm(-abs(z_scores))

# Calculate posterior probabilities (simplified)
bf <- exp(0.5 * z_scores^2 * is_true)  # Simplified Bayes factor
post_prob <- (pi1 * bf) / (pi1 * bf + (1 - pi1))

# Apply Benjamini-Hochberg procedure
p_ordered <- order(p_values)
fdr_threshold <- 0.1
bh_significant <- rep(FALSE, n_tests)
for (i in 1:n_tests) {
  if (p_values[p_ordered[i]] <= fdr_threshold * i / n_tests) {
    bh_significant[p_ordered[i]] <- TRUE
  } else {
    break
  }
}

# Define high posterior probability
high_post <- post_prob > 0.9

# Create data for plotting
plot_data <- data.frame(
  z_score = z_scores,
  p_value = p_values,
  is_true = factor(is_true),
  bh_significant = factor(bh_significant),
  high_post = factor(high_post)
)

# Plot
library(ggplot2)
ggplot(plot_data, aes(x = z_score, y = -log10(p_value), color = is_true)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_hline(yintercept = -log10(0.05), linetype = "dashed", color = "red") +
  scale_color_manual(values = c("0" = "gray70", "1" = "blue"),
                    labels = c("Null", "True Effect"),
                    name = "Truth") +
  labs(title = "Multiple Testing in Genomics",
       subtitle = paste0("5% true effects (blue), 95% null (gray)"),
       x = "Z-score", 
       y = "-log10(p-value)") +
  theme_minimal(base_size = 14)
```

The challenge: Separating true signals (blue) from noise (gray) when true effects are rare.

## Local False Discovery Rate (LFDR)

The LFDR for a test with statistic z is:

$$\text{LFDR}(z) = P(H_0|z) = 1 - P(H_1|z)$$

This is a direct Bayesian statement about the probability of false discovery for each individual test.

Benefits:
- Test-specific measure of evidence
- Directly interpretable probability
- Accounts for prior probability of effects

## Empirical Bayes for Multiple Testing

We can estimate components from the data:

1. Proportion of true effects: $\hat{\pi}_1$
2. Distribution of null statistics: $\hat{f}_0(z)$
3. Distribution of all statistics: $\hat{f}(z)$

Then:

$$\widehat{\text{LFDR}}(z) = \frac{(1-\hat{\pi}_1)\hat{f}_0(z)}{\hat{f}(z)}$$

Popular implementations: qvalue package, fdrtool, locfdr

## Comparison with FDR Control

**Frequentist FDR** (Benjamini-Hochberg):
- Controls expected proportion of false discoveries
- Same threshold for all tests based on ranked p-values
- No direct probability interpretation for individual tests

**Bayesian approach**:
- Calculates probability of false discovery for each test
- Can incorporate prior information on effect prevalence
- Naturally addresses multiple testing without penalties

## Example: Genomic Applications

```{r}
# Plot posterior probabilities
ggplot(plot_data, aes(x = z_score)) +
  geom_point(aes(y = post_prob, color = is_true), alpha = 0.5) +
  geom_hline(yintercept = 0.9, linetype = "dashed", color = "darkgreen") +
  scale_color_manual(values = c("0" = "gray70", "1" = "blue"),
                    labels = c("Null", "True Effect"),
                    name = "Truth") +
  labs(title = "Posterior Probability of True Effect",
       subtitle = "Directly interpretable probability for each test",
       x = "Z-score", 
       y = "Posterior Probability") +
  theme_minimal(base_size = 14)
```

Posterior probabilities provide a direct measure of evidence for each test.

## Decision Boundaries Comparison

```{r}
# Create data frame for decisions
decisions <- data.frame(
  z_score = z_scores,
  p_value = p_values,
  is_true = factor(is_true),
  BH_Decision = factor(ifelse(bh_significant, "Significant", "Not Significant")),
  Bayes_Decision = factor(ifelse(post_prob > 0.9, "High PP", "Low PP"))
)

# Count results
bh_true_pos <- sum(bh_significant & is_true == 1)
bh_false_pos <- sum(bh_significant & is_true == 0)
bh_false_neg <- sum(!bh_significant & is_true == 1)

bayes_true_pos <- sum(high_post & is_true == 1)
bayes_false_pos <- sum(high_post & is_true == 0)
bayes_false_neg <- sum(!high_post & is_true == 1)

# Plot
ggplot(decisions, aes(x = p_value, y = post_prob, color = is_true)) +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = max(p_values[bh_significant]), linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.9, linetype = "dashed", color = "darkgreen") +
  scale_color_manual(values = c("0" = "gray70", "1" = "blue"),
                    labels = c("Null", "True Effect"),
                    name = "Truth") +
  scale_x_log10() +
  annotate("text", x = 1e-5, y = 0.2, label = paste0("BH: ", bh_true_pos, " TP, ", bh_false_pos, " FP"), color = "red") +
  annotate("text", x = 1e-5, y = 0.1, label = paste0("PP>0.9: ", bayes_true_pos, " TP, ", bayes_false_pos, " FP"), color = "darkgreen") +
  labs(title = "Comparison of Decision Boundaries",
       subtitle = "FDR control vs. Posterior Probability thresholds",
       x = "P-value (log scale)", 
       y = "Posterior Probability") +
  theme_minimal(base_size = 14)
```

Both approaches control false discoveries but with different decision boundaries.

## Advantages of Bayesian Multiple Testing

1. **Direct probabilistic statements**: "95% probability of true effect"
2. **Incorporates effect prevalence**: Accounts for rarity of true effects
3. **No arbitrary thresholds**: Posterior probabilities are on probability scale
4. **Effect size integration**: Naturally considers both significance and magnitude
5. **Prior information**: Can incorporate domain knowledge
6. **Decision theory integration**: Allows optimal decision-making based on costs

## Extensions

Advanced Bayesian approaches for multiple testing:

1. **Hierarchical models**: Sharing information across tests
2. **Correlated tests**: Modeling dependency structures between tests
3. **Spatial/network priors**: Incorporating biological relationships
4. **Mixture priors**: More flexible alternative distributions
5. **Adaptive shrinkage**: Data-driven prior specification
6. **Full Bayesian decision**: Incorporating asymmetric costs of errors

## Key Takeaways

1. Multiple testing is a core challenge in genomics
2. Bayesian approaches directly address the quantity of interest
3. Local FDR provides test-specific false discovery probabilities
4. Empirical Bayes methods estimate key components from data
5. Posterior probabilities are directly interpretable
6. The approach extends naturally to complex data structures

## Hierarchical Bayesian Models: The Problem of Many Groups

In genomics, we often need to estimate many related parameters:

- Effects of thousands of variants
- Expression levels across multiple tissues
- Population-specific allele frequencies
- Gene-specific parameters

Independent analysis: Ignores similarities between groups
Complete pooling: Ignores differences between groups

**Hierarchical models provide the best of both worlds.**

## The Power of Partial Pooling

```{r}
# Create example data for three approaches
set.seed(123)

# Number of groups
n_groups <- 20
true_effects <- rnorm(n_groups, 0, 0.3)  # True underlying effects
sample_sizes <- sample(c(5, 10, 20, 30, 50), n_groups, replace = TRUE)  # Sample sizes per group

# Generate observed data
observed_effects <- rnorm(n_groups, true_effects, 1/sqrt(sample_sizes))
observed_se <- 1/sqrt(sample_sizes)

# Calculate estimates under different approaches
# 1. No pooling (MLE)
no_pooling <- observed_effects

# 2. Complete pooling
complete_pooling <- rep(mean(observed_effects), n_groups)

# 3. Partial pooling (simplified hierarchical model)
# Estimate variance components
tau_sq <- max(0, var(observed_effects) - mean(1/sample_sizes))
shrinkage_factor <- tau_sq / (tau_sq + 1/sample_sizes)
partial_pooling <- complete_pooling + shrinkage_factor * (observed_effects - complete_pooling)

# Create data frame for plotting
plot_data <- data.frame(
  Group = factor(rep(1:n_groups, 4)),
  Estimate = c(true_effects, no_pooling, complete_pooling, partial_pooling),
  Method = factor(rep(c("True", "No Pooling", "Complete Pooling", "Hierarchical"), each = n_groups),
                levels = c("True", "No Pooling", "Complete Pooling", "Hierarchical"))
)

# Plot
library(ggplot2)
ggplot(plot_data, aes(x = Group, y = Estimate, color = Method, group = Method)) +
  geom_point(size = 3) +
  geom_line(linetype = "dashed") +
  scale_color_manual(values = c("True" = "black", "No Pooling" = "red", 
                               "Complete Pooling" = "blue", "Hierarchical" = "green")) +
  labs(title = "Comparing Estimation Methods",
       subtitle = "Hierarchical models balance between no pooling and complete pooling",
       x = "Group", y = "Effect Estimate") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

**Hierarchical models** adaptively share information across groups: stronger shrinkage for uncertain estimates, less for confident ones.

## Mathematical Structure



A typical two-level hierarchical model:

$$\begin{align}
\text{Data level: } & y_j \sim N(\theta_j, \sigma_j^2) \\
\text{Group level: } & \theta_j \sim N(\mu, \tau^2) \\
\text{Hyperprior level: } & \mu \sim N(\mu_0, \sigma_0^2) \\
& \tau^2 \sim \text{InvGamma}(a, b)
\end{align}$$

Where:
- $y_j$ is the observed data in group $j$
- $\theta_j$ is the group-specific parameter
- $\mu$ and $\tau^2$ are shared across groups
- Hyperpriors allow learning the amount of sharing

## The Beauty of Shrinkage

The posterior mean for group $j$ is:

$$\hat{\theta}_j = \frac{\frac{1}{\sigma_j^2}y_j + \frac{1}{\tau^2}\mu}{\frac{1}{\sigma_j^2} + \frac{1}{\tau^2}} = w_j y_j + (1-w_j)\mu$$

Where $w_j = \frac{\frac{1}{\sigma_j^2}}{\frac{1}{\sigma_j^2} + \frac{1}{\tau^2}}$ is the weight given to the group's data.

Key insights:
- Uncertain estimates ($\sigma_j^2$ large) are shrunk more toward $\mu$
- Heterogeneous groups ($\tau^2$ large) experience less shrinkage
- The amount of sharing is learned from the data itself

## Applications in Genomics

Hierarchical models are ubiquitous in genomics:

- **eQTL Analysis**: Sharing information across tissues
- **Allele Frequencies**: Borrowing strength across populations
- **GWAS Meta-analysis**: Combining studies while accounting for heterogeneity
- **Differential Expression**: Moderation of gene-specific variance estimates
- **Fine Mapping**: Sharing information across correlated variants

## Multivariate Extension: correlated effects

For effects across multiple tissues or conditions:

$$\begin{align}
\text{Data level: } & \mathbf{y}_j \sim N(\boldsymbol{\theta}_j, \mathbf{S}_j) \\
\text{Group level: } & \boldsymbol{\theta}_j \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma}) \\
\end{align}$$

Now $\boldsymbol{\Sigma}$ captures correlations between conditions.

Examples: multivariate adaptive shrinkage (mash), multi-tissue eQTL analysis

## Nonparametric Extensions

We can make hierarchical models more flexible:

- **Mixture of normals** for the prior:
  $$\theta_j \sim \sum_{k=1}^K \pi_k N(\mu_k, \tau_k^2)$$

- **Dirichlet process** priors for unknown grouping:
  $$\theta_j \sim DP(\alpha, G_0)$$

These extensions allow for:
- Multi-modal effect distributions
- Automatic clustering of similar effects
- Robust estimation with outliers

## Example: Moderated t-statistics

Limma's moderated t-statistics for gene expression:

1. **Standard approach**: Estimate variance for each gene independently
2. **Hierarchical approach**: Model gene variances as coming from inverse-gamma prior
3. **Result**: "Borrow information" across genes to improve variance estimates

```{r, echo=FALSE, fig.width=8, fig.height=3.5}
# Simulate data to illustrate moderated t-statistics
set.seed(456)

# Parameters
n_genes <- 1000
sample_size <- 4

# True variances from inverse-gamma
true_var <- 1/rgamma(n_genes, shape = 2, rate = 2)

# Observed variances (with sampling error)
observed_var <- true_var * rchisq(n_genes, df = sample_size - 1) / (sample_size - 1)

# Moderated variances (simplified)
prior_df <- 4
prior_var <- 1
post_var <- (observed_var * (sample_size - 1) + prior_var * prior_df) / 
            (sample_size - 1 + prior_df)

# Create data frame for plotting
var_data <- data.frame(
  GeneRank = 1:n_genes,
  TrueVar = sort(true_var),
  ObservedVar = observed_var[order(true_var)],
  ModeratedVar = post_var[order(true_var)]
)

# Plot
ggplot(var_data, aes(x = GeneRank)) +
  geom_line(aes(y = TrueVar, color = "True"), size = 1) +
  geom_line(aes(y = ObservedVar, color = "Observed"), size = 0.5, alpha = 0.5) +
  geom_line(aes(y = ModeratedVar, color = "Moderated"), size = 1) +
  scale_color_manual(values = c("True" = "black", "Observed" = "red", "Moderated" = "blue")) +
  labs(title = "Variance Moderation in Gene Expression Analysis",
       x = "Gene (ranked by true variance)",
       y = "Variance Estimate",
       color = "Method") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## Computational Approaches

Hierarchical models can be fit using:

1. **Empirical Bayes**: Estimate hyperparameters from data, then obtain group-specific posteriors
   - Fast and scalable
   - Used in tools like limma, edgeR, DESeq2

2. **Full Bayes**: Sample from full posterior using MCMC
   - Complete uncertainty quantification
   - Handles complex dependency structures
   - More computationally intensive

## Practical Advantages

1. **Improved accuracy**: Lower mean squared error than independent estimates
2. **Uncertainty quantification**: Full posterior distributions
3. **Efficiency**: Makes better use of limited sample sizes
4. **Robustness**: Less sensitive to outliers and high-noise groups
5. **Discovery power**: Increases ability to detect real effects
6. **Interpretability**: Reveals relationships between groups

## Key Takeaways

1. Hierarchical models provide a principled approach to sharing information
2. Automatically balance between pooled and unpooled estimates
3. Particularly valuable for "small n, large p" settings common in genomics
4. Allow modeling of data-generating processes at multiple levels
5. Extend naturally to complex, multivariate settings
6. Provide computational flexibility from empirical to fully Bayesian approaches


---

# Conjugate Priors for Genetic Models

---

## Conjugate Priors: Why They're Beautiful

**Definition**: A prior is conjugate when the posterior has the same distribution family

**Why they matter**:
1. **Analytical solutions** - no MCMC needed
2. **Interpretable parameters** - prior as "pseudo-observations"
3. **Sequential updating** - yesterday's posterior is today's prior
4. **Computational efficiency** - critical for large genomic datasets

## figures 

```{r}
#| fig-height: 5
# Create a function to generate conjugate updating visualization
plot_conjugate_updating <- function() {
  # Initial prior
  alpha0 <- 2
  beta0 <- 8
  
  # Data observations (success/failure)
  data <- c(1, 1, 0, 1, 1, 0, 1, 1, 1, 0)
  
  # Create sequence for plotting
  x <- seq(0, 1, length.out = 200)
  
  # Initialize data frame for plotting
  plot_data <- data.frame()
  
  # Initial prior
  prior_y <- dbeta(x, alpha0, beta0)
  plot_data <- rbind(plot_data, data.frame(
    x = x, y = prior_y, 
    step = 0, 
    label = paste0("Prior: Beta(", alpha0, ",", beta0, ")")
  ))
  
  # Update step by step
  alpha <- alpha0
  beta <- beta0
  
  for (i in 1:length(data)) {
    # Update parameters
    if (data[i] == 1) {
      alpha <- alpha + 1
    } else {
      beta <- beta + 1
    }
    
    # Only plot a subset of steps for clarity
    if (i %in% c(1, 3, 5, 10)) {
      posterior_y <- dbeta(x, alpha, beta)
      plot_data <- rbind(plot_data, data.frame(
        x = x, y = posterior_y, 
        step = i, 
        label = paste0("After ", i, " observations: Beta(", alpha, ",", beta, ")")
      ))
    }
  }
  
  # Convert step to factor for proper ordering
  plot_data$step <- factor(plot_data$step, levels = c(0, 1, 3, 5, 10))
  
  # Create plot
  ggplot(plot_data, aes(x = x, y = y, color = step, group = step)) +
    geom_line(size = 1.2) +
    scale_color_brewer(palette = "Set1", 
                      name = "Update Step",
                      labels = c("Prior", "After 1 obs", "After 3 obs", 
                                "After 5 obs", "After 10 obs")) +
    labs(title = "Sequential Updating with Conjugate Prior",
         subtitle = "Beta-Binomial conjugacy for allele frequency estimation",
         x = "Allele Frequency", 
         y = "Density") +
    theme(legend.position = "bottom")
}

# Generate the plot
plot_conjugate_updating()
```
:::
:::::

---

## Beta-Binomial: Perfect for Allele Frequencies

::::: columns
::: {.column width="50%"}
**Model**:
- **Prior**: $\theta \sim \text{Beta}(\alpha, \beta)$
- **Likelihood**: $X|\theta \sim \text{Binomial}(n, \theta)$
- **Posterior**: $\theta|X \sim \text{Beta}(\alpha + X, \beta + n - X)$

**Applications**:
- Allele frequency estimation
- Case-control association
- Heterozygosity estimation
- Sequencing error rates
:::

::: {.column width="50%"}
```{r}
#| fig-height: 5
# Create a visualization of Beta-Binomial for allele frequencies
set.seed(123)

# Parameters
true_af <- 0.3
n_samples <- 20
n_reads_per_sample <- 30

# Generate data
genotypes <- rbinom(n_samples, 2, true_af)
read_counts <- rbinom(n_samples, n_reads_per_sample, genotypes/2)
total_alt <- sum(read_counts)
total_reads <- n_samples * n_reads_per_sample

# Create data for plotting
x <- seq(0, 1, length.out = 200)

# Different priors
priors <- list(
  "Flat" = c(1, 1),
  "Informative" = c(10, 20),
  "Incorrect" = c(20, 5)
)

# Create data frame for plotting
plot_data <- data.frame()

for (prior_name in names(priors)) {
  prior_params <- priors[[prior_name]]
  alpha_prior <- prior_params[1]
  beta_prior <- prior_params[2]
  
  # Prior
  prior_y <- dbeta(x, alpha_prior, beta_prior)
  
  # Posterior
  alpha_post <- alpha_prior + total_alt
  beta_post <- beta_prior + total_reads - total_alt
  posterior_y <- dbeta(x, alpha_post, beta_post)
  
  # Add to data frame
  plot_data <- rbind(plot_data, 
                    data.frame(
                      x = x,
                      y = prior_y,
                      Distribution = "Prior",
                      Prior = prior_name
                    ),
                    data.frame(
                      x = x,
                      y = posterior_y,
                      Distribution = "Posterior",
                      Prior = prior_name
                    ))
}

# Add true value line
plot_data$Distribution <- factor(plot_data$Distribution, 
                               levels = c("Prior", "Posterior"))

# Create plot
ggplot(plot_data, aes(x = x, y = y, color = Distribution, linetype = Prior)) +
  geom_line(size = 1.2) +
  geom_vline(xintercept = true_af, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("Prior" = "blue", "Posterior" = "red")) +
  labs(title = "Beta-Binomial for Allele Frequency Estimation",
       subtitle = paste0("True AF = ", true_af, ", Observed = ", 
                       round(total_alt/total_reads, 3)),
       x = "Allele Frequency", 
       y = "Density") +
  theme(legend.position = "bottom")
```
:::
:::::

---

## Dirichlet-Multinomial: For Multiple Alleles

::::: columns
::: {.column width="50%"}
**Model**:
- **Prior**: $\vec{\theta} \sim \text{Dirichlet}(\vec{\alpha})$
- **Likelihood**: $\vec{X}|\vec{\theta} \sim \text{Multinomial}(n, \vec{\theta})$
- **Posterior**: $\vec{\theta}|\vec{X} \sim \text{Dirichlet}(\vec{\alpha} + \vec{X})$

**Applications**:
- Multiple allele frequencies
- Haplotype frequencies
- Population admixture proportions
- Taxonomic abundances
:::

::: {.column width="50%"}
```{r}
#| fig-height: 5
# Create a visualization of Dirichlet-Multinomial
set.seed(234)

# Function to generate random points from a Dirichlet distribution
rdirichlet_2d <- function(n, alpha) {
  x <- rgamma(n, alpha[1], 1)
  y <- rgamma(n, alpha[2], 1)
  z <- rgamma(n, alpha[3], 1)
  s <- x + y + z
  return(data.frame(x = x/s, y = y/s, z = z/s))
}

# Parameters
prior_alpha <- c(2, 2, 2)  # Symmetric prior
true_props <- c(0.6, 0.3, 0.1)  # True proportions
n_samples <- 50

# Generate data
counts <- rmultinom(1, n_samples, true_props)

# Calculate posterior
posterior_alpha <- prior_alpha + counts

# Generate points for visualization
n_points <- 1000
prior_points <- rdirichlet_2d(n_points, prior_alpha)
posterior_points <- rdirichlet_2d(n_points, posterior_alpha)

# Combine data
prior_points$Distribution <- "Prior"
posterior_points$Distribution <- "Posterior"
all_points <- rbind(prior_points, posterior_points)

# Create ternary plot data
# We'll use a 2D projection since ggtern might not be available
project_ternary <- function(df) {
  # Convert to 2D coordinates
  df$X <- 0.5 * (2 * df$y + df$z) / (df$x + df$y + df$z)
  df$Y <- (sqrt(3)/2) * df$z / (df$x + df$y + df$z)
  return(df)
}

plot_data <- project_ternary(all_points)

# Add true value
true_point <- data.frame(x = true_props[1], y = true_props[2], z = true_props[3],
                        Distribution = "True Value")
true_point <- project_ternary(true_point)

# Create plot
ggplot(plot_data, aes(x = X, y = Y, color = Distribution)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_point(data = true_point, color = "black", size = 5, shape = 8) +
  scale_color_manual(values = c("Prior" = "blue", "Posterior" = "red", "True Value" = "black")) +
  labs(title = "Dirichlet-Multinomial for Multiple Alleles",
       subtitle = paste0("Prior: Dirichlet(", paste(prior_alpha, collapse = ","), 
                       "), Counts: ", paste(counts, collapse = ",")),
       x = "", y = "") +
  theme(legend.position = "bottom",
        axis.text = element_blank(),
        axis.ticks = element_blank()) +
  annotate("text", x = 0, y = 0, label = "A") +
  annotate("text", x = 1, y = 0, label = "B") +
  annotate("text", x = 0.5, y = sqrt(3)/2, label = "C")
```

:::

::::::
---

## Conjugate Normal-Normal Model

- One of the most elegant and widely-used conjugate pairs in Bayesian statistics
- Perfect for analyzing quantitative traits in genomics
- Gives us a mathematical shortcut for updating beliefs

## The Setup

When analyzing a continuous parameter $\mu$ (like an effect size):

- **Prior**: $\mu \sim \mathcal{N}(\mu_0, \sigma_0^2)$
- **Likelihood**: $X \sim \mathcal{N}(\mu, \sigma^2)$ where $\sigma^2$ is known
- **Question**: What is $p(\mu|X)$?
---

## The Mathematical Magic

The elegance is in the algebraic symmetry:

$$
\begin{align}
p(\mu|X) &\propto p(X|\mu) \times p(\mu)\\
&\propto \exp\left(-\frac{(X-\mu)^2}{2\sigma^2}\right) \times \exp\left(-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\right)
\end{align}
$$

Notice the beautiful symmetry: $(X-\mu)^2$ in the likelihood and $(\mu-\mu_0)^2$ in the prior.

## The Key Insight

When we expand these terms:

$$
\begin{align}
p(\mu|X) &\propto \exp\left(-\frac{1}{2}\left[\frac{(X-\mu)^2}{\sigma^2} + \frac{(\mu-\mu_0)^2}{\sigma_0^2}\right]\right)\\
&\propto \exp\left(-\frac{1}{2}\left[\frac{\mu^2 - 2\mu X + X^2}{\sigma^2} + \frac{\mu^2 - 2\mu\mu_0 + \mu_0^2}{\sigma_0^2}\right]\right)
\end{align}
$$

Collecting terms with $\mu^2$ and $\mu$...

## The Posterior Formula

After completing the square, we get:

$$\mu|X \sim \mathcal{N}(\mu_n, \sigma_n^2)$$

Where:

$$\mu_n = \frac{\frac{\mu_0}{\sigma_0^2} + \frac{X}{\sigma^2}}{\frac{1}{\sigma_0^2} + \frac{1}{\sigma^2}} = \frac{\sigma^2\mu_0 + \sigma_0^2 X}{\sigma^2 + \sigma_0^2}$$

$$\frac{1}{\sigma_n^2} = \frac{1}{\sigma_0^2} + \frac{1}{\sigma^2}$$

## A More Intuitive View

The posterior mean is a **precision-weighted average** of the prior mean and the data:

$$\mu_n = w\mu_0 + (1-w)X$$

Where $w = \frac{\sigma^2}{\sigma^2 + \sigma_0^2} = \frac{\text{data precision}}{\text{total precision}}$

- When data is precise (small $\sigma^2$): we trust the data more
- When prior is precise (small $\sigma_0^2$): we trust the prior more

## Multiple Observations

With multiple observations $X_1,...,X_n$, we get:

$$\mu|(X_1,...,X_n) \sim \mathcal{N}\left(\frac{\frac{\mu_0}{\sigma_0^2} + \frac{n\bar{X}}{\sigma^2}}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}, \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1}\right)$$

- The sample mean $\bar{X}$ is a sufficient statistic
- More data increases precision linearly

## Genomics Application: eQTL Effect Sizes

In genomics, we might use this model for:

- **Prior**: Historical effect sizes for similar variants
- **Likelihood**: Observed effect in current study
- **Posterior**: Updated estimate that balances prior knowledge and new data

Example: Effect sizes for expression quantitative trait loci (eQTLs)

## The Power of Conjugate Priors

Advantages of conjugate Normal-Normal:

1. **Analytical solutions** - no MCMC required
2. **Computational efficiency** - critical for genomic scale
3. **Interpretable updates** - precision-weighted averages
4. **Sequential processing** - can update one observation at a time

## Normal-Normal: Key Takeaways

1. The posterior is also Normal - that's conjugacy!
2. The posterior mean is a weighted average of prior mean and data
3. Weights are determined by relative precisions (1/variance)
4. The posterior precision is the sum of the prior and data precisions
5. This model provides the foundation for many advanced Bayesian genomic methods

---

## Extension: Empirical Bayes for Normal Means

- When we don't have a specific prior, we can **estimate it from the data**
- This approach, known as **Empirical Bayes**, is extremely powerful for genomics
- Applications include: multiple testing, sparse signal detection, and effect size estimation

## Methods Using Normal-Normal Conjugacy

- **Adaptive Shrinkage (ash)**: Uses a mixture of normals as the prior
- **Multivariate Adaptive Shrinkage (mash)**: Extends to correlated effects across conditions
- **False Discovery Rate Control**: Through local false discovery rates
- **Hierarchical Models**: Building multi-level models with partially pooled estimates



---

# Mixture Models for Complex Data: What Are Mixture Models?

Mixture models are probabilistic models that represent the presence of subpopulations within an overall population:

- **Used when data come from multiple underlying processes**
- **Represent heterogeneous populations as mixtures of simpler distributions**
- **Allow clustering without hard assignments**
- **Incorporate uncertainty in group membership**

---

## Mixture Model: Mathematical Formulation

A mixture model combines multiple distributions to model complex data:

$$p(x) = \sum_{k=1}^K \pi_k f_k(x|\theta_k)$$

Where:

- $p(x)$ is the overall probability density
- $K$ is the number of components (subpopulations)
- $\pi_k$ are the mixing weights ($\sum_{k=1}^K \pi_k = 1$)
- $f_k(x|\theta_k)$ are the component densities with parameters $\theta_k$

---

## A Closer Look at the Components

::::: columns
::: {.column width="50%"}
**Key components**:

1. **Component distributions** $f_k(x|\theta_k)$
   - Each represents a subpopulation
   - Can be any distribution family
   - Common choices: Gaussian, multinomial, beta
   
2. **Mixing weights** $\pi_k$
   - Proportion of data from each component
   - Must sum to 1: $\sum_{k=1}^K \pi_k = 1$
   - Reflect prior probabilities of group membership
   
3. **Latent variables** $z_i$
   - Unobserved component membership
   - $z_i = k$ means data point $i$ came from component $k$
:::

::: {.column width="50%"}
```{r}
#| fig-height: 6
# Create a simple mixture model visualization
set.seed(123)

# Define mixture parameters
means <- c(-2.5, 1.5)
sds <- c(0.7, 1.2)
weights <- c(0.4, 0.6)

# Generate data from mixture
n_samples <- 1000
component <- sample(1:2, n_samples, replace = TRUE, prob = weights)
x <- rnorm(n_samples, mean = means[component], sd = sds[component])

# Create data frame for histogram
hist_data <- data.frame(x = x, component = factor(component))

# Create data for component densities
x_seq <- seq(-6, 6, length.out = 500)
comp1_density <- weights[1] * dnorm(x_seq, mean = means[1], sd = sds[1])
comp2_density <- weights[2] * dnorm(x_seq, mean = means[2], sd = sds[2])
mixture_density <- comp1_density + comp2_density

density_data <- data.frame(
  x = rep(x_seq, 3),
  y = c(comp1_density, comp2_density, mixture_density),
  Component = factor(rep(c("Component 1", "Component 2", "Mixture"), each = length(x_seq)))
)

# Create plot
ggplot() +
  geom_histogram(data = hist_data, aes(x = x, y = after_stat(density), fill = component), 
                bins = 50, alpha = 0.5, position = "identity") +
  geom_line(data = density_data, aes(x = x, y = y, color = Component), size = 1.2) +
  scale_fill_manual(values = c("lightblue", "lightgreen"), name = "True Component") +
  scale_color_manual(values = c("blue", "darkgreen", "red")) +
  labs(title = "Gaussian Mixture Model",
       subtitle = "Two underlying subpopulations create a complex distribution",
       x = "Value", 
       y = "Density") +
  theme(legend.position = "bottom")
```
:::
:::::

---

## Likelihood Function for Mixture Models

The likelihood of a mixture model for $n$ independent observations $x_1, \ldots, x_n$ is:

$$L(\theta, \pi | x_1, \ldots, x_n) = \prod_{i=1}^n p(x_i) = \prod_{i=1}^n \sum_{k=1}^K \pi_k f_k(x_i|\theta_k)$$

**Challenge**: The sum inside the product makes this difficult to optimize directly

**Solution**: Introduce latent variables $z_i$ and use the EM algorithm 

---

## The EM Algorithm in Detail

The Expectation-Maximization (EM) algorithm is an iterative method for finding maximum likelihood estimates:

**E-step**: Calculate "responsibilities" - the posterior probability that data point $i$ belongs to component $k$:

$$\gamma_{ik} = P(z_i = k | x_i, \theta) = \frac{\pi_k f_k(x_i|\theta_k)}{\sum_{j=1}^K \pi_j f_j(x_i|\theta_j)}$$

**M-step**: Update parameters using weighted maximum likelihood:

$$\pi_k^{new} = \frac{1}{n}\sum_{i=1}^n \gamma_{ik}$$
$$\theta_k^{new} = \arg\max_{\theta_k} \sum_{i=1}^n \gamma_{ik} \log f_k(x_i|\theta_k)$$

---

## EM Algorithm: Step-by-Step Example

```{r}
#| fig-height: 6
# Simplified EM visualization
set.seed(789)

# Generate data from a mixture of two Gaussians
n <- 200
true_means <- c(-2, 2)
true_sds <- c(0.8, 0.8)
true_weights <- c(0.4, 0.6)

z <- sample(1:2, n, replace = TRUE, prob = true_weights)
x <- rnorm(n, mean = true_means[z], sd = true_sds[z])

# Create data for visualization
iterations <- c("Initial", "Iteration 1", "Iteration 3", "Final")
means1 <- c(-1, -1.5, -1.8, -2.0)
means2 <- c(1, 1.4, 1.8, 2.0)
sds1 <- c(1.2, 1.0, 0.9, 0.8)
sds2 <- c(1.2, 1.0, 0.9, 0.8)
weights1 <- c(0.5, 0.45, 0.42, 0.4)
weights2 <- c(0.5, 0.55, 0.58, 0.6)

# Create data frame for plotting
plot_data <- data.frame()
x_seq <- seq(-5, 5, length.out = 200)

for (i in 1:length(iterations)) {
  # Component 1
  comp1 <- data.frame(
    x = x_seq,
    y = weights1[i] * dnorm(x_seq, mean = means1[i], sd = sds1[i]),
    Component = "Component 1",
    Iteration = iterations[i]
  )
  
  # Component 2
  comp2 <- data.frame(
    x = x_seq,
    y = weights2[i] * dnorm(x_seq, mean = means2[i], sd = sds2[i]),
    Component = "Component 2",
    Iteration = iterations[i]
  )
  
  # Mixture
  mixture <- data.frame(
    x = x_seq,
    y = comp1$y + comp2$y,
    Component = "Mixture",
    Iteration = iterations[i]
  )
  
  plot_data <- rbind(plot_data, comp1, comp2, mixture)
}

# Create histogram data
hist_data <- data.frame(x = x, Iteration = "Data")

# Create plot
ggplot() +
  # Add histogram for data
  geom_histogram(data = hist_data, aes(x = x, y = after_stat(density)), 
                bins = 30, fill = "gray80", color = "black", alpha = 0.5) +
  # Add component and mixture densities
  geom_line(data = plot_data, 
           aes(x = x, y = y, color = Component, group = interaction(Component, Iteration)),
           size = 1) +
  # Facet by iteration
  facet_wrap(~ Iteration, ncol = 2) +
  # Customize colors
  scale_color_manual(values = c("Component 1" = "blue", 
                              "Component 2" = "green", 
                              "Mixture" = "red")) +
  # Add labels
  labs(title = "EM Algorithm Convergence",
       subtitle = "Mixture model fit improves with each iteration",
       x = "Value", 
       y = "Density") +
  theme_minimal() +
  theme(legend.position = "bottom")
```


---

## Worked Example: EM Algorithm Step-by-Step

Let's walk through each step of the EM algorithm for a mixture of two Gaussians:

1. **Initialize parameters**:
   - Set initial mixing weights: $\pi_1 = \pi_2 = 0.5$
   - Set initial component means: $\mu_1 = -1, \mu_2 = 1$
   - Set initial component standard deviations: $\sigma_1 = \sigma_2 = 1$

2. **E-step**: For each data point $x_i$, calculate the responsibility of each component:
   - $\gamma_{i1} = \frac{\pi_1 N(x_i|\mu_1,\sigma_1^2)}{\pi_1 N(x_i|\mu_1,\sigma_1^2) + \pi_2 N(x_i|\mu_2,\sigma_2^2)}$
   - $\gamma_{i2} = 1 - \gamma_{i1}$

3. **M-step**: Update the parameters using the responsibilities:
   - $\pi_1^{new} = \frac{1}{n}\sum_{i=1}^n \gamma_{i1}$ (similarly for $\pi_2^{new}$)
   - $\mu_1^{new} = \frac{\sum_{i=1}^n \gamma_{i1}x_i}{\sum_{i=1}^n \gamma_{i1}}$ (similarly for $\mu_2^{new}$)
   - $(\sigma_1^{new})^2 = \frac{\sum_{i=1}^n \gamma_{i1}(x_i-\mu_1^{new})^2}{\sum_{i=1}^n \gamma_{i1}}$ (similarly for $\sigma_2^{new}$)

4. **Repeat** until convergence (parameters stop changing significantly)

---

The EM Algorithm: Mathematical Intuition
Key insight: We're solving a chicken-and-egg problem
If we knew component assignments, parameter estimation would be easy
If we knew parameters, component assignments would be easy
EM iteratively solves both by using expected assignments
E-step (Expectation): Calculate expected component memberships
$$\gamma_{ik} = P(z_i = k | x_i, \theta^{(t)}) = \frac{\pi_k^{(t)} f_k(x_i|\theta_k^{(t)})}{\sum_{j=1}^K \pi_j^{(t)} f_j(x_i|\theta_j^{(t)})}$$
Intuition: "How likely is individual i to belong to population k, given our current parameter estimates?"
M-step (Maximization): Update parameters using weighted averages
For mixing weights:
$$\pi_k^{(t+1)} = \frac{1}{n}\sum_{i=1}^n \gamma_{ik}$$
Intuition: "The new population frequency is the average membership across all individuals"
For component means (Gaussian case):
$$\mu_k^{(t+1)} = \frac{\sum_{i=1}^n \gamma_{ik}x_i}{\sum_{i=1}^n \gamma_{ik}}$$
Intuition: "The new population mean is a weighted average where individuals are weighted by their probability of belonging to this population"

---

## Bayesian Mixture Models

Bayesian mixture models add priors to the parameters:

$p(\theta, \pi | x_1, \ldots, x_n) \propto p(x_1, \ldots, x_n | \theta, \pi) \times p(\theta, \pi)$

Common prior choices:
- $\pi \sim \text{Dirichlet}(\alpha_1, \ldots, \alpha_K)$ for mixing weights
- Component-specific priors for $\theta_k$ (e.g., Normal-Inverse-Gamma for Gaussian components)

**Advantages**:
- Handle uncertainty in the number of components (K)
- Avoid singularities and improve stability
- Allow for informed priors from previous studies
- Provide full posterior distribution rather than point estimates (in full MCMC impelmentation, but SLOW)
-- compromise: EB for some pi, full inference ... 

---

## Mixture Model Applications in Genomics

::::: columns
::: {.column width="50%"}
**1. Population Structure**
- Components = ancestral populations
- Individual genotypes = admixtures of populations
- Example: STRUCTURE, ADMIXTURE software
- Used for: demographic history, association studies, conservation

**2. GEnetic effect estimation**
-- Ash v Mash

**2. Gene Expression Clustering**
- Components = cell types/states
- Expression patterns = signatures of cell types
- Example: Single-cell RNA-seq clustering
- Used for: cell type identification, developmental trajectories
:::


::: {.column width="50%"}
```{r}
#| fig-height: 6
# Create a visualization of population structure
set.seed(456)

# Number of individuals and populations
n_ind <- 60
n_pop <- 3

# Create admixture proportions
# First 20 individuals mostly from pop1, etc.
admixture <- matrix(0, nrow = n_ind, ncol = n_pop)
for (i in 1:n_ind) {
  if (i <= 20) {
    admixture[i,] <- c(0.8, 0.1, 0.1) + rnorm(3, 0, 0.05)
  } else if (i <= 40) {
    admixture[i,] <- c(0.1, 0.8, 0.1) + rnorm(3, 0, 0.05)
  } else {
    admixture[i,] <- c(0.1, 0.1, 0.8) + rnorm(3, 0, 0.05)
  }
  # Ensure proportions are positive and sum to 1
  admixture[i,] <- pmax(admixture[i,], 0)
  admixture[i,] <- admixture[i,] / sum(admixture[i,])
}

# Create data frame for plotting
admix_df <- data.frame(
  Individual = rep(1:n_ind, n_pop),
  Population = factor(rep(paste0("Pop", 1:n_pop), each = n_ind)),
  Proportion = c(admixture)
)

# Create barplot
ggplot(admix_df, aes(x = Individual, y = Proportion, fill = Population)) +
  geom_col(width = 1) +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Population Structure as a Mixture Model",
       subtitle = "Each individual is a mixture of ancestral populations",
       x = "Individual", 
       y = "Ancestry Proportion") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```
:::
:::::

---

## The STRUCTURE Model in Detail

:::::: {.columns}

::: {.column width="50%"}

**STRUCTURE**: A Bayesian mixture model for population genetics

**Key components**:
- Each individual = mixture of $K$ ancestral populations
- Each population = distinct allele frequencies
- Goal: Infer ancestry proportions & population frequencies

**Bayesian formulation**:
- **Prior**: $q_{ik} \sim \text{Dirichlet}(\alpha)$ (ancestry proportions)
- **Prior**: $f_{kj} \sim \text{Beta}(\lambda)$ (allele frequencies)
- **Likelihood**: $P(X_{ij} | q_i, f_j)$ (genotype probabilities)

:::

::: {.column width="50%"}

```{r}
#| fig-height: 6
#| fig-width: 6
#| echo: false
#| warning: false
#| message: false

library(ggplot2)
library(viridis)
library(patchwork)

set.seed(567)

n_ind <- 100
n_pop <- 3
n_markers <- 20

pop_freqs <- matrix(rbeta(n_pop * n_markers, 0.5, 0.5), nrow = n_pop)

# Create ancestry proportions
q <- matrix(0, nrow = n_ind, ncol = n_pop)
q[1:40, 1] <- rbeta(40, 10, 1)
q[41:70, 2] <- rbeta(30, 10, 1)
q[71:100, 3] <- rbeta(30, 10, 1)
q <- t(apply(q + 0.05, 1, function(x) x / sum(x)))

# Dataframes
q_df <- data.frame(
  Individual = rep(1:n_ind, n_pop),
  Population = factor(rep(paste0("Pop", 1:n_pop), each = n_ind)),
  Proportion = as.vector(q)
)

freq_df <- data.frame(
  Population = factor(rep(paste0("Pop", 1:n_pop), each = n_markers)),
  Marker = factor(rep(paste0("M", 1:n_markers), times = n_pop)),
  Frequency = c(pop_freqs)
)

# Plots
p1 <- ggplot(q_df, aes(x = Individual, y = Population, fill = Proportion)) +
  geom_tile() +
  scale_fill_viridis() +
  labs(title = "Ancestry Proportions",
       subtitle = "Each individual's genetic ancestry",
       x = "Individual", y = "Ancestral Population") +
  theme_minimal(base_size = 10) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

p2 <- ggplot(freq_df, aes(x = Marker, y = Population, fill = Frequency)) +
  geom_tile() +
  scale_fill_viridis() +
  labs(title = "Population Allele Frequencies",
       subtitle = "Each population's genetic profile",
       x = "Genetic Marker", y = "Ancestral Population") +
  theme_minimal(base_size = 10)

p1 / p2  # patchwork layout
```


:::

::::::
### **Effect Size Mixtures in GWAS**

**Problem**: Most variants have no effect, but some do

**Solutions**:
- **Spike-and-slab prior**: Mixture of point mass at zero and continuous distribution
- **Scale mixture**: Mixture of normal distributions with different variances
- **Bayesian variable selection**: Latent indicator for whether variant is causal

**Benefits**:
- Controls false discovery rate
- Improves power to detect true associations
- Provides interpretable posterior probabilities
- Naturally handles multiple testing

:::

::: {.column width="50%"}

```{r}
# Create visualization of effect size mixtures
set.seed(789)

# Parameters
n_variants <- 500
pi0 <- 0.95  # Proportion of null effects

# Generate true effects
is_null <- rbinom(n_variants, 1, pi0)
true_effects <- rep(0, n_variants)
true_effects[is_null == 0] <- rnorm(sum(is_null == 0), 0, 0.5)

# Add noise to create observed effects
observed_effects <- true_effects + rnorm(n_variants, 0, 0.2)

# Create data frame for plotting
effect_df <- data.frame(
  Variant = 1:n_variants,
  TrueEffect = true_effects,
  ObservedEffect = observed_effects,
  IsNull = factor(is_null)
)

# Plot
ggplot(effect_df, aes(x = ObservedEffect, fill = IsNull)) +
  geom_histogram(bins = 40, alpha = 0.7, position = "identity") +
  scale_fill_manual(values = c("red", "gray70"), 
                   labels = c("Causal", "Null"),
                   name = "Variant Type") +
  labs(title = "Mixture of Effect Sizes in GWAS",
       subtitle = "Most variants have no effect (null)",
       x = "Observed Effect Size", 
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "bottom")
```
:::
:::::

  
## Multivariate Normal Mixtures: The mash Approach

::::: columns
::: {.column width="50%"}
**Key idea**: Share information across related conditions

**Mathematical model**:
-  $\hat{\beta}_j \sim N(\beta_j, S_j)$ (observed effects)
-  $\beta_j \sim \sum_{k=1}^K \pi_k N(0, U_k)$ (true effects)

---

**Covariance matrices $U_k$ capture patterns**:
- Shared effects across all conditions
- Condition-specific effects
- Structured correlation patterns
- Data-driven patterns

**Benefits**:
- Improves effect estimation through sharing
- Discovers patterns of effect heterogeneity
- Controls false discovery rate
- Provides interpretable multivariate posteriors
:::

::: {.column width="50%"}
```{r}
#| fig-height: 6
# Create a visualization of multivariate effects
set.seed(345)

# Parameters
n_effects <- 200
n_conditions <- 4

# Create different effect patterns
patterns <- list(
  "Shared" = rep(1, n_conditions),
  "Condition1" = c(1, 0, 0, 0),
  "Condition2" = c(0, 1, 0, 0),
  "Conditions1&2" = c(1, 1, 0, 0),
  "Conditions3&4" = c(0, 0, 1, 1)
)

# Assign effects to patterns
n_per_pattern <- n_effects / length(patterns)
true_effects <- matrix(0, nrow = n_effects, ncol = n_conditions)
colnames(true_effects) <- paste0("Condition", 1:n_conditions)

current_idx <- 1
for (p in 1:length(patterns)) {
  pattern_name <- names(patterns)[p]
  pattern <- patterns[[p]]
  
  for (i in 1:n_per_pattern) {
    effect_size <- rnorm(1, 0, 0.5)
    true_effects[current_idx, ] <- pattern * effect_size
    current_idx <- current_idx + 1
  }
}

# Add noise to create observed effects
observed_effects <- true_effects + matrix(rnorm(n_effects * n_conditions, 0, 0.2),
                                        nrow = n_effects)

# Select a few examples for visualization
example_indices <- c(5, 45, 85, 125, 165)  # One from each pattern
example_data <- data.frame(
  Effect = rep(paste0("Effect", example_indices), each = n_conditions),
  Condition = rep(paste0("Condition", 1:n_conditions), times = length(example_indices)),
  TrueEffect = c(t(true_effects[example_indices, ])),
  ObservedEffect = c(t(observed_effects[example_indices, ]))
)

# Reshape for plotting
example_long <- reshape2::melt(example_data, 
                             id.vars = c("Effect", "Condition"),
                             variable.name = "EffectType",
                             value.name = "Value")

# Create plot
ggplot(example_long, aes(x = Condition, y = Value, color = EffectType, group = EffectType)) +
  geom_point(size = 3) +
  geom_line(size = 1) +
  facet_wrap(~ Effect, ncol = 3) +
  scale_color_manual(values = c("TrueEffect" = "blue", "ObservedEffect" = "red"),
                    labels = c("True Effect", "Observed Effect")) +
  labs(title = "Multivariate Effect Patterns",
       subtitle = "mash identifies and leverages these patterns",
       x = "", 
       y = "Effect Size",
       color = "") +
  theme(legend.position = "bottom")
```
:::
:::::

---


## Common Challenges with Mixture Models

1. **Identifiability issues**: Component labels can be permuted (label switching)

2. **Local optima**: EM algorithm may converge to suboptimal solutions

3. **Initialization sensitivity**: Results depend on starting values

4. **Singularities**: Component variance can collapse to zero

5. **Determining number of components**: No single best method

**Solutions**:
- Run algorithm multiple times with different initializations
- Regularization via priors (Bayesian approach)
- Deterministic annealing or other modified EM variants
- Model averaging across different K values
- Cross-validation for model selection


---

## Bayesian Clinical Trials: The Mathematical Foundation

The core idea of Bayesian clinical trials is to use probability distributions to quantify uncertainty about parameters:

**Traditional (frequentist) approach**:
- Fixed design with predetermined sample size
- Binary decision making (significant or not)
- No formal incorporation of prior information

**Bayesian approach** offers several advantages:
- **Sequential analysis**: Update posterior as data accumulates
- **Adaptive design**: Sample size, treatment allocation can change mid-trial
- **Probabilistic conclusions**: Direct statements about treatment effects
- **Prior incorporation**: Previous trial results, biological knowledge

The mathematical framework:
- **Prior distribution**: $p(\theta)$ - Initial beliefs about parameter $\theta$
- **Likelihood**: $p(X|\theta)$ - Probability of data given parameter
- **Posterior distribution**: $p(\theta|X) \propto p(X|\theta)p(\theta)$ - Updated beliefs
---
## Bayesian MOdel Average
---

## The Model Selection Problem

Traditional approach:
1. Define candidate models
2. Select "best" model using AIC, BIC, etc.
3. Make inference based on single chosen model

**The problem**: Ignores model uncertainty!

- What if multiple models fit equally well?
- What if different models make different predictions?
- What if the "best" model changes with small data changes?

## Bayesian Model Averaging

Rather than choosing a single model, we can average over all models:

1. Assign prior probabilities to models: $P(M_k)$
2. Compute posterior model probabilities: $P(M_k|data)$
3. Weight predictions by these probabilities

$$P(\theta|data) = \sum_{k=1}^K P(\theta|M_k, data) \times P(M_k|data)$$

This accounts for uncertainty in both parameters AND model selection.

## The Mathematical Framework

For K candidate models $M_1, M_2, \ldots, M_K$:

- Prior model probabilities: $P(M_k)$
- Posterior model probabilities:
  $$P(M_k|data) = \frac{P(data|M_k)P(M_k)}{\sum_{j=1}^K P(data|M_j)P(M_j)}$$

- Marginal likelihood of model $M_k$:
  $$P(data|M_k) = \int P(data|\theta_k, M_k)P(\theta_k|M_k)d\theta_k$$

## Visualizing BMA

```{r, echo=FALSE, fig.width=8, fig.height=4}
# Simulate data for model averaging
set.seed(123)

# Generate data
x <- seq(0, 10, length.out = 20)
y_true <- 2 + 0.5*x - 0.03*x^2
y <- y_true + rnorm(length(x), 0, 1)
data <- data.frame(x = x, y = y)

# Define models
# M1: Linear
# M2: Quadratic
# M3: Cubic

# Fit models
fit1 <- lm(y ~ x, data = data)
fit2 <- lm(y ~ x + I(x^2), data = data)
fit3 <- lm(y ~ x + I(x^2) + I(x^3), data = data)

# Calculate BIC values
bic1 <- BIC(fit1)
bic2 <- BIC(fit2)
bic3 <- BIC(fit3)

# Convert to approximate posterior probabilities (using BIC approximation)
bic_min <- min(bic1, bic2, bic3)
post_prob1 <- exp(-0.5 * (bic1 - bic_min))
post_prob2 <- exp(-0.5 * (bic2 - bic_min))
post_prob3 <- exp(-0.5 * (bic3 - bic_min))
total <- post_prob1 + post_prob2 + post_prob3
post_prob1 <- post_prob1 / total
post_prob2 <- post_prob2 / total
post_prob3 <- post_prob3 / total

# Generate predictions
x_new <- seq(-1, 12, length.out = 100)
pred1 <- predict(fit1, newdata = data.frame(x = x_new), se.fit = TRUE)
pred2 <- predict(fit2, newdata = data.frame(x = x_new), se.fit = TRUE)
pred3 <- predict(fit3, newdata = data.frame(x = x_new), se.fit = TRUE)

# Create BMA prediction (weighted average)
pred_bma <- post_prob1 * pred1$fit + post_prob2 * pred2$fit + post_prob3 * pred3$fit
# Simplified variance calculation (not exact but illustrative)
var_bma <- post_prob1 * (pred1$se.fit^2) + post_prob2 * (pred2$se.fit^2) + post_prob3 * (pred3$se.fit^2) + 
           post_prob1 * (pred1$fit - pred_bma)^2 + post_prob2 * (pred2$fit - pred_bma)^2 + post_prob3 * (pred3$fit - pred_bma)^2

# Create plot data
plot_data <- data.frame(
  x = x,
  y = y
)

pred_data <- data.frame(
  x = rep(x_new, 4),
  y = c(pred1$fit, pred2$fit, pred3$fit, pred_bma),
  lower = c(pred1$fit - 1.96 * pred1$se.fit, 
            pred2$fit - 1.96 * pred2$se.fit,
            pred3$fit - 1.96 * pred3$se.fit,
            pred_bma - 1.96 * sqrt(var_bma)),
  upper = c(pred1$fit + 1.96 * pred1$se.fit, 
            pred2$fit + 1.96 * pred2$se.fit,
            pred3$fit + 1.96 * pred3$se.fit,
            pred_bma + 1.96 * sqrt(var_bma)),
  Model = factor(rep(c("Linear", "Quadratic", "Cubic", "Model Average"), each = length(x_new)),
                levels = c("Linear", "Quadratic", "Cubic", "Model Average"))
)

# True curve data
true_data <- data.frame(
  x = x_new,
  y = 2 + 0.5*x_new - 0.03*x_new^2
)

# Plot
library(ggplot2)
ggplot() +
  geom_point(data = plot_data, aes(x = x, y = y), alpha = 0.5) +
  geom_line(data = pred_data, aes(x = x, y = y, color = Model), size = 1) +
  geom_line(data = true_data, aes(x = x, y = y), linetype = "dashed", color = "black") +
  geom_ribbon(data = pred_data, aes(x = x, ymin = lower, ymax = upper, fill = Model), alpha = 0.2) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Bayesian Model Averaging",
       subtitle = paste0("Model probabilities: Linear=", round(post_prob1*100), "%, ",
                        "Quadratic=", round(post_prob2*100), "%, ",
                        "Cubic=", round(post_prob3*100), "%"),
       x = "X", y = "Y") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

BMA combines predictions from multiple models, weighted by their posterior probabilities, leading to more robust inference.

## BMA for Fine-Mapping

In genetic fine-mapping, we don't know which variants are causal:

- Many possible causal configurations (models)
- BMA provides a coherent approach to handle this uncertainty

```{r, echo=FALSE, fig.width=8, fig.height=4}
# Create BMA for fine-mapping visualization
set.seed(234)

# Create simulated data for a fine-mapping example
n_variants <- 20
r2_matrix <- matrix(0.9, n_variants, n_variants)
diag(r2_matrix) <- 1
for(i in 1:n_variants) {
  for(j in 1:n_variants) {
    r2_matrix[i,j] <- 0.9^abs(i-j)
  }
}

# True causal variants
causal_idx <- c(5, 12)

# Generate z-scores based on LD
z_scores <- rnorm(n_variants, 0, 0.2)
z_scores[causal_idx] <- rnorm(length(causal_idx), 4, 0.5)
z_scores <- z_scores + rnorm(n_variants, 0, 0.5)  # Add noise

# Convert to p-values
p_values <- 2 * pnorm(-abs(z_scores))

# Calculate approximate Bayes factors
bf <- exp(z_scores^2/2 - 1)

# Simplified FINEMAP-like calculation
# We'll create just a few candidate models for illustration
models <- list(
  model1 = c(5),        # Just variant 5
  model2 = c(12),       # Just variant 12
  model3 = c(5, 12),    # Both 5 and 12
  model4 = c(4, 5),     # 4 and 5
  model5 = c(11, 12)    # 11 and 12
)

# Calculate posterior odds for each model (simplified)
prior_odds <- c(0.1, 0.1, 0.05, 0.05, 0.05)  # Prior model probabilities
post_odds <- rep(1, length(models))

for(i in 1:length(models)) {
  post_odds[i] <- prior_odds[i] * prod(bf[models[[i]]])
}
post_probs <- post_odds / sum(post_odds)

# Calculate marginal inclusion probabilities for each variant
pip <- rep(0, n_variants)
for(i in 1:length(models)) {
  pip[models[[i]]] <- pip[models[[i]]] + post_probs[i]
}

# Create data for plotting
finemap_data <- data.frame(
  Variant = 1:n_variants,
  Z_score = z_scores,
  P_value = p_values,
  PIP = pip,
  Causal = 1:n_variants %in% causal_idx
)

# Plot
ggplot(finemap_data) +
  geom_point(aes(x = Variant, y = -log10(P_value)), color = "blue", size = 3) +
  geom_point(aes(x = Variant, y = PIP), color = "red", size = 3) +
  geom_vline(xintercept = causal_idx, linetype = "dashed", color = "darkgreen") +
  scale_y_continuous(
    name = "-log10(p-value)",
    sec.axis = sec_axis(~., name = "Posterior Inclusion Probability")
  ) +
  labs(title = "Bayesian Fine-Mapping with Model Averaging",
       subtitle = "Green dashed lines indicate true causal variants",
       x = "Variant Position") +
  theme_minimal(base_size = 14) +
  annotate("text", x = 3, y = 0.8, label = "PIPs (red)", color = "red") +
  annotate("text", x = 3, y = 0.7, label = "P-values (blue)", color = "blue")
```

Posterior Inclusion Probabilities (PIPs) from BMA provide a robust way to prioritize likely causal variants.

## Model Averaging in Genomics

Common applications:

1. **Gene-Environment Interactions**: Uncertainty in interaction model forms
2. **Gene-Based Association Tests**: Multiple ways to combine variants
3. **Fine-Mapping**: Uncertainty in causal variant configurations
4. **Gene Set Analysis**: Uncertainty in pathway definitions
5. **eQTL Mapping**: Uncertainty in the appropriate expression model
6. **Polygenic Risk Scores**: Uncertainty in variant selection

## BMA vs. Model Selection

```{r, echo=FALSE, fig.width=8, fig.height=4}
# Create data for model selection vs BMA comparison
set.seed(345)

# Generate some data with model uncertainty
n <- 50
x <- seq(-3, 3, length.out = n)
true_y <- 2 + 0.2*x^2 + 0.1*sin(x*pi)
y <- true_y + rnorm(n, 0, 0.5)

# Fit multiple models
fit1 <- lm(y ~ x)                   # Linear
fit2 <- lm(y ~ x + I(x^2))          # Quadratic
fit3 <- lm(y ~ x + I(x^2) + I(x^3)) # Cubic
fit4 <- lm(y ~ sin(pi*x) + cos(pi*x)) # Trigonometric

# Calculate BIC
bic1 <- BIC(fit1)
bic2 <- BIC(fit2)
bic3 <- BIC(fit3)
bic4 <- BIC(fit4)

# Best model by BIC
bic_values <- c(bic1, bic2, bic3, bic4)
best_model_idx <- which.min(bic_values)
best_model <- c("Linear", "Quadratic", "Cubic", "Trigonometric")[best_model_idx]

# Convert to posterior probabilities
bic_min <- min(bic_values)
post_probs <- exp(-0.5 * (bic_values - bic_min))
post_probs <- post_probs / sum(post_probs)

# Create plot data for model comparison
x_grid <- seq(-4, 4, length.out = 200)
pred1 <- predict(fit1, newdata = data.frame(x = x_grid))
pred2 <- predict(fit2, newdata = data.frame(x = x_grid))
pred3 <- predict(fit3, newdata = data.frame(x = x_grid))
pred4 <- predict(fit4, newdata = data.frame(x = x_grid))

# BMA prediction
pred_bma <- pred1 * post_probs[1] + pred2 * post_probs[2] + 
            pred3 * post_probs[3] + pred4 * post_probs[4]

# True values
true_grid <- 2 + 0.2*x_grid^2 + 0.1*sin(x_grid*pi)

# Plot data
plot_data <- data.frame(
  x = rep(x_grid, 6),
  y = c(pred1, pred2, pred3, pred4, pred_bma, true_grid),
  Model = factor(rep(c("Linear", "Quadratic", "Cubic", "Trigonometric", "BMA", "True"), each = length(x_grid)),
                levels = c("Linear", "Quadratic", "Cubic", "Trigonometric", "BMA", "True"))
)

# Original data
orig_data <- data.frame(x = x, y = y)

# Plot
ggplot() +
  geom_point(data = orig_data, aes(x = x, y = y), alpha = 0.4) +
  geom_line(data = plot_data, aes(x = x, y = y, color = Model, size = Model, linetype = Model)) +
  scale_color_manual(values = c("Linear" = "blue", "Quadratic" = "green", 
                               "Cubic" = "purple", "Trigonometric" = "orange",
                               "BMA" = "red", "True" = "black")) +
  scale_size_manual(values = c("Linear" = 0.7, "Quadratic" = 0.7, 
                              "Cubic" = 0.7, "Trigonometric" = 0.7,
                              "BMA" = 1.2, "True" = 1.2)) +
  scale_linetype_manual(values = c("Linear" = "solid", "Quadratic" = "solid", 
                                  "Cubic" = "solid", "Trigonometric" = "solid",
                                  "BMA" = "solid", "True" = "dashed")) +
  labs(title = "Model Selection vs. Model Averaging",
       subtitle = paste0("Model probabilities: Linear=", round(post_probs[1]*100), "%, ",
                       "Quadratic=", round(post_probs[2]*100), "%, ",
                       "Cubic=", round(post_probs[3]*100), "%, ",
                       "Trig=", round(post_probs[4]*100), "%"),
       x = "X", y = "Y") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

Model averaging often outperforms selection of a single "best" model, especially for prediction.

## Computational Approaches

1. **Exact BMA**: Enumerate all models (feasible for small model spaces)

2. **Markov Chain Monte Carlo Model Composition (MC³)**: 
   - Sample models according to their posterior probabilities
   - Efficient exploration of large model spaces

3. **Approximations**: 
   - BIC approximation to marginal likelihoods
   - Laplace approximation for integration
   - Variational methods for high-dimensional spaces

## Practical Considerations

1. **Prior model probabilities**: 
   - Equal priors for all models?
   - Complexity penalties?
   - Biologically informed priors?

2. **Model space definition**:
   - Which models to include?
   - How to handle model correlation?

3. **Inference targets**:
   - Parameter estimates vs. predictions
   - Variable importance measures
   - Model-averaged confidence intervals

## Key Advantages

1. **Honest uncertainty quantification**: Accounts for model uncertainty
2. **Improved prediction accuracy**: Particularly for out-of-sample prediction
3. **Robust inference**: Less sensitive to model misspecification
4. **Variable importance measures**: Posterior inclusion probabilities
5. **Decision-theoretic optimality**: Under certain loss functions
6. **Natural Occam's razor**: Automatically balances fit and complexity

## Key Takeaways

1. Model selection ignores uncertainty in the model itself
2. Bayesian model averaging provides a principled framework for incorporating model uncertainty
3. Posterior model probabilities balance fit and complexity
4. BMA often produces more robust parameter estimates and predictions
5. Computational methods exist for handling large model spaces
6. In genomics, BMA is particularly valuable for fine-mapping and complex trait analysis

---

## Bayesian Clinical Trials: Direct Probability Statements

Key insight: Even with flat priors, Bayesian methods give you direct probability statements about parameters

Example: For a treatment effect θ with flat prior and normal likelihood:
$$p(\theta|data) \propto \text{Normal}(\hat{\theta}, SE^2)$$
Direct probability statements:
$P(\theta > 0 | data) = 0.97$ → "97% probability the treatment is beneficial"
$P(\theta > 0.2 | data) = 0.63$ → "63% probability the effect exceeds clinically meaningful threshold"
$P(0.1 < \theta < 0.3 | data) = 0.75$ → "75% probability effect is in the moderate range"
Contrast with frequentist:
"p < 0.05" → Reject null hypothesis
"95% CI: [0.1, 0.4]" → In repeated sampling, 95% of intervals would contain true value

**Decision rules** can be based on:
- Posterior probability: $P(\theta > \delta|X) > \gamma$ 
- Predictive probability of success
- Expected utility/loss considerations

---

## Bayesian Meta-Analysis: The Mathematical Framework

**Problem**: Combine evidence across heterogeneous studies

**Model formulation**:
- Let $y_i$ be the observed effect in study $i$
- Let $\sigma_i^2$ be the variance (often known from standard error)
- Let $\theta_i$ be the true effect in study $i$

**Hierarchical model**:
$y_i | \theta_i, \sigma_i^2 \sim N(\theta_i, \sigma_i^2)$
$\theta_i | \mu, \tau^2 \sim N(\mu, \tau^2)$
$\mu \sim N(\mu_0, \sigma_0^2)$
$\tau^2 \sim \text{InvGamma}(a, b)$

Where:
- $\mu$ is the overall mean effect
- $\tau^2$ is the between-study heterogeneity
- $\mu_0, \sigma_0^2, a, b$ are hyperparameters
---

**Key advantages**:
- Naturally accounts for heterogeneity
- Uncertainty in all parameters
- Shrinkage of extreme estimates toward the mean
- Robust to outliers with appropriate priors

---

## Bayesian Meta-Analysis: Visualization

::::: columns
::: {.column width="50%"}
**Traditional approaches**:
- Fixed effects (assumes same effect size)
- Random effects (allows variation in effect size)
- Often sensitive to outliers

**Bayesian advantages**:
- Full posterior distribution for all parameters
- Can incorporate informative priors
- Naturally handles small studies (shrinkage)
- Can model outliers explicitly
- Direct probability statements about effects

**Example interpretation**:
- Posterior probability of benefit = 98%
- 95% credible interval for effect size: [0.1, 0.5]
- 90% probability heterogeneity is moderate to high
:::

::: {.column width="50%"}
```{r}
#| fig-height: 6
# Create a visualization of Bayesian meta-analysis
set.seed(678)

# Parameters
n_studies <- 8
true_effect <- 0.3
heterogeneity <- 0.1

# Generate study-specific effects
study_effects <- rnorm(n_studies, true_effect, heterogeneity)

# Make one study an outlier
study_effects[n_studies] <- -0.2

# Generate observed effects and standard errors
sample_sizes <- round(runif(n_studies, 100, 500))
standard_errors <- 1 / sqrt(sample_sizes)
observed_effects <- rnorm(n_studies, study_effects, standard_errors)

# Create data frame for forest plot
meta_df <- data.frame(
  Study = paste0("Study ", 1:n_studies),
  Effect = observed_effects,
  SE = standard_errors,
  LowerCI = observed_effects - 1.96 * standard_errors,
  UpperCI = observed_effects + 1.96 * standard_errors
)

# Add meta-analysis results
# 1. Fixed effects
fixed_weights <- 1 / standard_errors^2
fixed_effect <- sum(fixed_weights * observed_effects) / sum(fixed_weights)
fixed_se <- sqrt(1 / sum(fixed_weights))

# 2. Random effects (simplified)
tau2 <- max(0, (sum(fixed_weights * (observed_effects - fixed_effect)^2) - (n_studies - 1)) / 
             (sum(fixed_weights) - sum(fixed_weights^2) / sum(fixed_weights)))
random_weights <- 1 / (standard_errors^2 + tau2)
random_effect <- sum(random_weights * observed_effects) / sum(random_weights)
random_se <- sqrt(1 / sum(random_weights))

# 3. Bayesian robust (simplified - downweights outliers)
robust_weights <- 1 / (standard_errors^2 + tau2 + 0.1 * (observed_effects - random_effect)^2)
robust_effect <- sum(robust_weights * observed_effects) / sum(robust_weights)
robust_se <- sqrt(1 / sum(robust_weights))

# Add to data frame
meta_results <- data.frame(
  Study = c("Fixed Effects", "Random Effects", "Bayesian Robust"),
  Effect = c(fixed_effect, random_effect, robust_effect),
  SE = c(fixed_se, random_se, robust_se),
  LowerCI = c(fixed_effect - 1.96 * fixed_se, 
             random_effect - 1.96 * random_se,
             robust_effect - 1.96 * robust_se),
  UpperCI = c(fixed_effect + 1.96 * fixed_se, 
             random_effect + 1.96 * random_se,
             robust_effect + 1.96 * robust_se)
)

# Combine data
plot_df <- rbind(
  meta_df,
  meta_results
)

# Add a grouping variable
plot_df$Group <- ifelse(plot_df$Study %in% meta_results$Study, "Meta-Analysis", "Individual Studies")
plot_df$Group <- factor(plot_df$Group, levels = c("Individual Studies", "Meta-Analysis"))

# Order studies
plot_df$Study <- factor(plot_df$Study, 
                      levels = c(paste0("Study ", 1:n_studies), 
                                "Fixed Effects", "Random Effects", "Bayesian Robust"))

# Create forest plot
ggplot(plot_df, aes(x = Effect, y = Study, color = Group)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +
  geom_vline(xintercept = true_effect, linetype = "dotted", color = "darkgreen") +
  geom_point(aes(size = 1/SE)) +
  geom_errorbarh(aes(xmin = LowerCI, xmax = UpperCI), height = 0.2) +
  scale_color_manual(values = c("Individual Studies" = "blue", "Meta-Analysis" = "red")) +
  scale_size_continuous(guide = "none") +
  labs(title = "Bayesian Meta-Analysis",
       subtitle = "Green dotted line shows true effect; note how Bayesian robust method handles outlier",
       x = "Effect Size", 
       y = "") +
  theme(legend.position = "bottom")
```
:::
:::::

---

## Bayesian Adaptive Trial Designs

A powerful framework that allows studies to **evolve** based on accumulating evidence

- Combines statistical efficiency with ethical considerations
- Particularly powerful for precision medicine approaches
- Provides direct probability statements about treatment effects
- Extends naturally to genetic and genomic applications

## Key Idea: Learning While Doing

Traditional (frequentist) trials:
- Fixed design determined at the outset
- Sample size calculations based on frequentist power
- Analysis only after all data collection is complete

Bayesian adaptive trials:
- Update knowledge continuously as data accumulates
- Modify aspects of the trial in response to emerging data
- Make probability-based decisions at interim points

## The Bayesian Framework

For a treatment effect $\theta$:

- **Prior distribution**: $p(\theta)$ - initial beliefs about the effect
- **Likelihood**: $p(data|\theta)$ - probability of observing the data
- **Posterior distribution**: $p(\theta|data) \propto p(data|\theta)p(\theta)$

As data accumulates, the posterior provides a complete picture of current knowledge.

## What Can Be Adapted?

1. **Sample size** - stop early for efficacy, futility, or continue for more precision
2. **Treatment allocation** - assign more patients to promising treatments
3. **Eligibility criteria** - focus on responsive subgroups
4. **Dosing** - concentrate on optimal doses
5. **Endpoints** - prioritize more informative outcomes

All based on predefined decision rules using posterior probabilities.

## Example: Adaptive Stopping Rules

A trial may stop early if:

- $P(\theta > 0 | data) > 0.99$ (efficacy - treatment works)
- $P(\theta < \delta | data) > 0.95$ (futility - treatment doesn't meet threshold)
- $P(\delta_1 < \theta < \delta_2 | data) > 0.90$ (precision - effect is known with sufficient certainty)

These are direct probability statements about the parameter of interest.

## Example: Adaptive Randomization

As evidence accumulates, allocation probabilities shift:

- Initial: Equal probability to all treatment arms
- Adaptive: Higher probability to treatments with better outcomes

$$P(\text{assign to treatment } j) \propto P(\text{treatment } j \text{ is best}|\text{current data})$$

Balances learning (exploration) with treating patients optimally (exploitation).

## Visualizing Adaptive Trials

```{r, echo=FALSE, fig.height=4}
# Simulate an adaptive trial
set.seed(42)

# Parameters
true_effect <- 0.15  # 15% improvement
max_patients <- 500
interim_points <- seq(100, max_patients, by = 100)
success_threshold <- 0.95

# Generate mock data and posterior probabilities
patients <- seq(10, max_patients, by = 10)
superiority_probs <- c(0.51, 0.55, 0.60, 0.64, 0.67, 0.70, 
                      0.73, 0.75, 0.78, 0.80, 0.82, 0.84,
                      0.85, 0.87, 0.88, 0.89, 0.90, 0.91,
                      0.92, 0.93, 0.94, 0.94, 0.95, 0.95,
                      0.96, 0.96, 0.97, 0.97, 0.97, 0.97,
                      0.97, 0.98, 0.98, 0.98, 0.98, 0.98,
                      0.98, 0.98, 0.98, 0.98, 0.99, 0.99,
                      0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99)

# Plot
plot(patients, superiority_probs, type="l", lwd=2, col="blue",
     main="Bayesian Adaptive Trial Simulation",
     xlab="Number of Patients Enrolled",
     ylab="P(Treatment > Control)",
     ylim=c(0.5, 1))
abline(h=success_threshold, lty=2, col="red")
text(max_patients*0.8, success_threshold+0.02, 
     "Superiority Threshold (95%)", col="red")

# Mark stopping point
stopping_point <- min(which(superiority_probs >= success_threshold))
points(patients[stopping_point], superiority_probs[stopping_point], 
       col="green", pch=19, cex=2)
text(patients[stopping_point]+50, superiority_probs[stopping_point], 
     paste("Trial stops after", patients[stopping_point], "patients"))
```

Early stopping means fewer patients needed and faster results.

## Adaptive Enrichment: Genetics Applications

For trials involving genetic subgroups:

1. Start with broad eligibility
2. As data accumulates, identify responsive genetic profiles
3. Adapt enrollment to focus on promising subgroups
4. Potentially obtain approval for genetically defined population

Direct extension of precision medicine principles.

## Platform Trials: Multiple Treatments

Modern adaptive platform trials can:
- Test multiple treatments simultaneously
- Add new treatment arms as they become available
- Remove ineffective treatments early
- Share control groups across treatments

Examples: I-SPY2 (breast cancer), GBM AGILE (glioblastoma), RECOVERY (COVID-19)

## Benefits of Bayesian Adaptive Designs

1. **Ethical advantages** - fewer patients on ineffective treatments
2. **Efficiency** - often require fewer total patients
3. **Flexibility** - can adapt to unexpected findings
4. **Subgroup identification** - find responsive populations
5. **Interpretability** - direct probability statements
6. **Speed** - potential for earlier conclusions

## Challenges to Consider

1. **Operational complexity** - requires robust infrastructure
2. **Statistical expertise** - more sophisticated than traditional designs
3. **Regulatory considerations** - though acceptance is growing
4. **Prior specification** - sensitivity analyses important
5. **Simulation required** - operating characteristics must be understood

## From Clinical Trials to Genomics

The same principles apply to genomics research:

- **Adaptive sampling** in sequencing studies
- **Sequential testing** with genomic data
- **Multi-stage designs** for genetic discovery
- **Early stopping** for genomic biomarker validation
- **Subgroup enrichment** based on genetic profiles

## Key Takeaways

1. Bayesian adaptive designs provide a formal framework for learning while doing
2. Direct probability statements make interpretation straightforward
3. Adapting aspects of the study can improve efficiency and ethics
4. The approach is particularly valuable for precision medicine
5. Methods extend naturally to genomics applications


---

## The SuSiE Model for Fine-Mapping: Mathematical Framework

**Problem**: Identify causal variants from correlated SNPs in GWAS loci

**SuSiE** (Sum of Single Effects) model:

$\mathbf{y} = \sum_{l=1}^L \mathbf{X} \boldsymbol{\beta}_l + \boldsymbol{\epsilon}$

Where:
- $\mathbf{y}$ is the vector of phenotype values ($n$ individuals)
- $\mathbf{X}$ is the genotype matrix ($n$ individuals × $p$ SNPs)
- $L$ is the number of causal effects (typically small, e.g., 1-10)
- Each $\boldsymbol{\beta}_l$ has exactly one non-zero element (single-effect)
- $\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})$ is the error term
---

**Bayesian formulation**:
- Each $\boldsymbol{\beta}_l$ follows a "spike-and-slab" prior:
  - For each effect $l$, exactly one SNP $j$ is causal
  - Prior probability SNP $j$ is causal: $\pi_j$ (can be uniform or informed)
  - Effect size distribution: $\beta_{lj} \sim N(0, \sigma^2_l)$ if causal, 0 otherwise

**Key outputs**:
- Posterior Inclusion Probability (PIP) for each SNP
- 95% credible sets for each effect
- Posterior distributions of effect sizes

---

## SuSiE Fine-Mapping: Visualization

::::: columns
::: {.column width="50%"}
**Problem**: Identify causal variants in a GWAS locus

**Traditional approach**:
- Select all SNPs with p < threshold
- Often includes many correlated SNPs
- No clear separation of multiple signals

**SuSiE advantages**:
- Accounts for correlation structure (LD)
- Separates overlapping signals
- Provides probabilistic quantification
- Can incorporate functional priors
- Defines credible sets with clear interpretation

**Example interpretation**:
- 95% credible set contains 3 SNPs
- Lead SNP has 80% probability of being causal
- Probability of at least one causal SNP in the region: 99%
:::

::: {.column width="50%"}
```{r}
#| fig-height: 6
# Create a visualization of Bayesian fine-mapping
set.seed(456)

# Parameters
n_snps <- 50
causal_idx <- 25

# Create correlation structure (LD)
pos <- 1:n_snps
ld_matrix <- exp(-abs(outer(pos, pos, "-")) / 10)

# Generate true effects (only one causal variant)
true_effects <- rep(0, n_snps)
true_effects[causal_idx] <- 0.5

# Generate observed effects based on LD
observed_effects <- as.vector(ld_matrix %*% true_effects + rnorm(n_snps, 0, 0.1))

# Calculate p-values
p_values <- 2 * pnorm(-abs(observed_effects))

# Calculate Bayes factors (simplified)
bayes_factors <- exp(observed_effects^2 / 2)

# Calculate posterior probabilities (assuming 1/n_snps prior)
prior <- 1/n_snps
posterior_probs <- (prior * bayes_factors) / sum(prior * bayes_factors)

# Create data frame for plotting
fine_map_df <- data.frame(
  SNP = 1:n_snps,
  Position = pos,
  ObservedEffect = observed_effects,
  PValue = p_values,
  PosteriorProb = posterior_probs,
  Causal = (1:n_snps == causal_idx)
)

# Calculate 95% credible set
sorted_idx <- order(posterior_probs, decreasing = TRUE)
cumulative_prob <- cumsum(posterior_probs[sorted_idx])
credible_set <- sorted_idx[cumulative_prob <= 0.95]
fine_map_df$InCredibleSet <- (1:n_snps %in% credible_set)

# Create plot
ggplot(fine_map_df, aes(x = Position)) +
  geom_point(aes(y = -log10(PValue), color = "P-value"), size = 3, alpha = 0.7) +
  geom_point(aes(y = PosteriorProb * 10, color = "Posterior"), size = 3, alpha = 0.7) +
  geom_vline(xintercept = pos[causal_idx], linetype = "dashed", color = "red") +
  geom_rect(data = subset(fine_map_df, InCredibleSet),
            aes(xmin = Position - 0.4, xmax = Position + 0.4, 
                ymin = 0, ymax = -0.2),
            fill = "darkgreen", alpha = 0.3) +
  scale_color_manual(values = c("P-value" = "blue", "Posterior" = "darkgreen"),
                    name = "Measure") +
  scale_y_continuous(
    name = "-log10(p-value)",
    sec.axis = sec_axis(~./10, name = "Posterior Probability")
  ) +
  annotate("text", x = n_snps * 0.8, y = 1, 
           label = "95% Credible Set", color = "darkgreen") +
  labs(title = "Bayesian Fine-Mapping vs. P-values",
       subtitle = "Green bars indicate 95% credible set; red line is true causal variant",
       x = "Genomic Position") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

:::

::::::
---

## Practical Recommendations for Bayesian Genomics

1. **Start with informative priors when possible**
   - Use previous studies
   - Incorporate functional annotations
   - Consider evolutionary constraints

2. **Report posterior probabilities, not just p-values**
   - P(association | data) is more interpretable than P(data | no association)
   - Provides direct probability statements about hypotheses

3. **Use conjugate models for computational efficiency**
   - Beta-Binomial for allele frequencies
   - Dirichlet-Multinomial for haplotype frequencies
   - Normal-Normal for quantitative traits

4. **Consider mixture models for complex data**
   - Population structure
   - Heterogeneous effect sizes
   - Multiple causal variants

5. **Apply decision theory for optimal designs**
   - Balance false positives and false negatives
   - Consider costs of follow-up studies
   - Optimize sample allocation

---

## Summary: When to Use Bayesian Methods

| Problem | Bayesian Approach | Advantage |
|---------|-------------------|-----------|
| Multiple testing | Posterior probabilities | Direct interpretation, no arbitrary thresholds |
| Sparse effects | Mixture priors | Better power, natural sparsity |
| Heterogeneous effects | Hierarchical models | Borrows strength across contexts |
| Sequential data collection | Adaptive designs | Efficiency, early stopping |
| Prior knowledge integration | Informative priors | Improved accuracy, reduced sample size |
| Complex dependencies | Bayesian networks | Causal inference, missing data handling |

---

## Resources for Bayesian Genomics

**Software**:
- **R packages**: `rstan`, `brms`, `mashr`, `BGLR`, `INLA`
- **Python**: `PyMC3`, `Stan`, `Edward`
- **Specialized**: `STRUCTURE`, `ADMIXTURE`, `SNPTEST`

**Books**:
- "Bayesian Data Analysis" by Gelman et al.
- "Statistical Rethinking" by McElreath
- "Bayesian Methods for Data Analysis" by Carlin & Louis

**Online Resources**:
- [StatisticalGenetics.info](https://stephenslab.github.io/fiveMinuteStats/)
- [Stan User Guide](https://mc-stan.org/users/documentation/)
- [Bayesian Methods for Hackers](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)

---

## Questions?

Thank you!! 

- Pradeep Natarajan, MD MMSc
- Sasha Gusev, PhD
- Giovanni Parmigiani, PhD
---
title: "A Practical Primer on Bayesian Statistics for Population Genomics"
author: "Instructor's Name"
date: "March 29, 2025"
format: 
  revealjs:
    theme: simple
    transition: slide
    slide-number: true
    code-fold: true
    highlight-style: github
    footer: "Population Genomics Methods Seminar"
execute:
  echo: false
  warning: false
---

```{r setup, include=FALSE}
# Load necessary packages
library(tidyverse)
library(ggplot2)
library(reshape2)
library(viridis)
library(gtools)     # For Dirichlet distribution
library(gridExtra)  # For arranging plots

# Set a clean theme for all plots
theme_set(theme_minimal(base_size = 12) + 
          theme(plot.title = element_text(hjust = 0.5),
                plot.subtitle = element_text(hjust = 0.5)))

# Set seed for reproducibility
set.seed(42)
```

## Overview

In this seminar, we'll cover:

1. Introduction to Bayesian thinking and posterior distributions
2. The p-value fallacy and Bayesian alternatives
3. Conjugate priors for genetic models (Beta-Binomial, Dirichlet)
4. Mixture models for population structure 
5. Clinical trials with flat priors - lessons for genomics
6. Multivariate normal mixtures (mash)

---

# Introduction to Bayesian Thinking

---

## Why Bayesian for Population Genomics?

Population genomics presents unique challenges:

- **Multiple testing** across thousands/millions of variants
- **Complex patterns** across populations and traits
- **Prior knowledge** from evolution and previous studies
- **Decision-making** under uncertainty

Bayesian approaches offer elegant solutions to these challenges.

---

## Bayes' Theorem - The Core Idea

$$P(H|D) = \frac{P(D|H) \times P(H)}{P(D)}$$

Where:

- $P(H|D)$ is the **posterior probability** - what we want to know
- $P(D|H)$ is the **likelihood** - how probable the data is under our hypothesis
- $P(H)$ is the **prior probability** - what we knew before
- $P(D)$ is the **evidence** - a normalizing constant

Simply: **Posterior ∝ Likelihood × Prior**

---

## Bayesian Updating: Visual Intuition

```{r bayesian-updating}
# Create data for three distributions
x <- seq(0.001, 0.999, length = 1000)
prior <- data.frame(x = x, y = dbeta(x, 2, 3), Distribution = "Prior: Beta(2,3)")
likelihood <- data.frame(x = x, y = dbeta(x, 7, 3), Distribution = "Likelihood (Data)")
posterior <- data.frame(x = x, y = dbeta(x, 9, 6), Distribution = "Posterior: Beta(9,6)")

# Combine data
all_distributions <- rbind(prior, likelihood, posterior)

# Plot all three distributions
ggplot(all_distributions, aes(x = x, y = y, color = Distribution)) +
  geom_line(size = 1.2) +
  scale_color_manual(values = c("darkgreen", "purple", "red")) +
  labs(title = "Bayesian Updating of Allele Frequency Estimate",
       subtitle = "Combining prior knowledge with new data",
       x = "Allele Frequency", 
       y = "Density") +
  theme(legend.position = "bottom")
```

---

# The p-value Fallacy and Bayesian Solutions

---

## The Problem with p-values

::: {.incremental}
1. **Misinterpreted**: p-values ≠ probability the null is true
2. **Multiple testing chaos**: Testing thousands of variants requires arbitrary corrections
3. **No prior knowledge**: Each test ignores previous research
4. **Binary thinking**: Significance thresholds create false dichotomies
:::

::: {.callout-important}
## The p-value fallacy
"If the null hypothesis is true, the probability of obtaining data as extreme or more extreme than what was observed is p."

NOT: "The probability that the null hypothesis is true is p."
:::

---

## From p-values to Posterior Probabilities

For a genetic association test with Z-score:

**Bayesian posterior**:
$$P(H_1 | Z) = \frac{P(Z | H_1) \times P(H_1)}{P(Z | H_1) \times P(H_1) + P(Z | H_0) \times P(H_0)}$$

Using Bayes Factor (BF):
$$\text{BF} = \frac{P(Z | H_1)}{P(Z | H_0)}$$
$$P(H_1 | Z) = \frac{\text{BF} \times P(H_1)}{\text{BF} \times P(H_1) + P(H_0)}$$

---

## p-values vs. Posterior Probabilities

```{r pvalue-simulation}
# Simulation parameters
n_loci <- 10000     # Number of genetic loci tested
prop_true <- 0.05   # Proportion of truly associated loci
sample_size <- 100  # Number of samples

# Generate true status for each locus
true_status <- runif(n_loci) < prop_true
n_true <- sum(true_status)

# Generate effect sizes for true loci
true_effects <- rnorm(n_loci, 0, 0.1)
true_effects[true_status] <- rnorm(n_true, 0, 0.5)  # Larger effects for true associations

# Generate test statistics and p-values
z_scores <- true_effects + rnorm(n_loci, 0, 1/sqrt(sample_size))
p_values <- 2 * pnorm(-abs(z_scores))  # Two-sided p-values

# Function to calculate Bayes Factors from z-scores
z_to_bf <- function(z, prior_width = 1) {
  sqrt(1/(prior_width^2 + 1)) * exp(0.5 * z^2 * (prior_width^2/(prior_width^2 + 1)))
}

# Calculate Bayes Factors
bayes_factors <- z_to_bf(z_scores)

# Convert to posterior probabilities
post_prob <- (bayes_factors * prop_true) / 
             (bayes_factors * prop_true + (1 - prop_true))

# Create comparison data frame
comparison_df <- data.frame(
  true_status = true_status,
  p_value = p_values,
  post_prob = post_prob,
  z_score = z_scores
)

# Plot p-values vs posterior probabilities
ggplot(comparison_df, aes(x = p_value, y = post_prob, color = true_status)) +
  geom_point(alpha = 0.5) +
  scale_x_log10() +  # Log scale for p-values
  scale_color_manual(values = c("gray60", "firebrick"), 
                    labels = c("Null", "True Association")) +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "blue") +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "blue") +
  labs(title = "p-values vs. Posterior Probabilities of Association",
       subtitle = "Bayesian approach directly answers what we want to know",
       x = "p-value (log scale)", 
       y = "Posterior Probability",
       color = "True Status") +
  theme(legend.position = "bottom")
```

---

## Bayesian Decision Theory

```{r bayesian-decisions}
# Define different decision thresholds for posterior probability
thresholds <- c(0.5, 0.8, 0.9, 0.95, 0.99)

# Calculate metrics for each threshold
decision_metrics <- lapply(thresholds, function(thresh) {
  decisions <- post_prob > thresh
  tp <- sum(decisions & true_status)
  fp <- sum(decisions & !true_status)
  fn <- sum(!decisions & true_status)
  tn <- sum(!decisions & !true_status)
  
  data.frame(
    Threshold = thresh,
    Discoveries = sum(decisions),
    TruePositives = tp,
    FalsePositives = fp,
    FDR = ifelse(sum(decisions) > 0, fp/sum(decisions), 0),
    Power = tp/sum(true_status)
  )
})

decision_df <- do.call(rbind, decision_metrics)

# Plot trade-off between false discovery rate and power
ggplot(decision_df, aes(x = FDR, y = Power)) +
  geom_path(size = 1.2, color = "purple") +
  geom_point(aes(size = Discoveries), color = "purple") +
  geom_text(aes(label = Threshold), hjust = -0.3, vjust = -0.3) +
  labs(title = "Trade-off Between False Discovery Rate and Power",
       subtitle = "Different posterior probability thresholds offer different trade-offs",
       x = "False Discovery Rate", 
       y = "Power (Sensitivity)",
       size = "Number of\nDiscoveries") +
  theme(legend.position = "bottom")
```

---

# Conjugate Priors: Elegant Bayesian Updating

---

## The Beta-Binomial Model for Allele Frequencies

For allele frequency $\theta$ with Beta prior and binomial data:

- **Prior**: $\theta \sim \text{Beta}(\alpha, \beta)$
- **Likelihood**: $x | \theta \sim \text{Binomial}(n, \theta)$
- **Posterior**: $\theta | x \sim \text{Beta}(\alpha + x, \beta + n - x)$

The posterior mean is a weighted average:
$$E[\theta|x] = \frac{\alpha+x}{\alpha+\beta+n}$$

---

## Beta Updating with More Data

```{r beta-updating}
# Create a simple visual of beta updating
make_beta_frame <- function(prior_alpha, prior_beta, data_vec, subset_steps) {
  result <- NULL
  current_alpha <- prior_alpha
  current_beta <- prior_beta
  
  # Start with just the prior
  df <- data.frame(
    x = seq(0.001, 0.999, length = 500),
    y = dbeta(seq(0.001, 0.999, length = 500), current_alpha, current_beta),
    step = 0,
    alpha = current_alpha,
    beta = current_beta,
    data_point = NA,
    label = paste0("Prior: Beta(", current_alpha, ",", current_beta, ")")
  )
  result <- df
  
  # Add each data point one at a time
  for(i in seq_along(data_vec)) {
    # Update parameters
    if(data_vec[i] == 1) {
      current_alpha <- current_alpha + 1
    } else {
      current_beta <- current_beta + 1
    }
    
    if(i %in% subset_steps) {
      df <- data.frame(
        x = seq(0.001, 0.999, length = 500),
        y = dbeta(seq(0.001, 0.999, length = 500), current_alpha, current_beta),
        step = i,
        alpha = current_alpha,
        beta = current_beta,
        data_point = data_vec[i],
        label = paste0("After ", i, " observations: Beta(", current_alpha, ",", current_beta, ")")
      )
      result <- rbind(result, df)
    }
  }
  
  return(result)
}

# Generate some data (biased coin flips - like alleles in a population)
true_freq <- 0.7
n_samples <- 20
data_vec <- rbinom(n_samples, 1, true_freq)
subset_steps <- c(5, 10, 20)  # Only show these steps for clarity

# Create our animation data frame
animation_df <- make_beta_frame(1, 1, data_vec, subset_steps)

# Plot the frames in a grid
plot_frames <- lapply(c(0, subset_steps), function(step) {
  subset_df <- animation_df[animation_df$step == step, ]
  
  ggplot(subset_df, aes(x = x, y = y)) +
    geom_line(size = 1.5, color = "firebrick") +
    labs(title = subset_df$label[1],
         subtitle = if(step > 0) {
                      paste0("Observed data: ", 
                            paste(data_vec[1:step], collapse = ", "))
                    } else {""},
         x = "Allele Frequency", 
         y = "Density") +
    geom_vline(xintercept = true_freq, linetype = "dashed", color = "darkblue") +
    annotate("text", x = true_freq + 0.05, y = max(subset_df$y) * 0.9, 
             label = "True frequency", color = "darkblue") +
    xlim(0, 1) + ylim(0, max(animation_df$y) * 1.1)
})

# Display plots in a grid
gridExtra::grid.arrange(grobs = plot_frames, ncol = 2)
```

---

## Dirichlet-Multinomial for Population Structure

The multivariate extension of Beta-Binomial:

- **Prior**: $\mathbf{q} \sim \text{Dirichlet}(\alpha_1, \ldots, \alpha_K)$
  - $p(\mathbf{q}) \propto \prod_{k=1}^K q_k^{\alpha_k-1}$

- **Likelihood**: $\mathbf{x} | \mathbf{q} \sim \text{Multinomial}(n, \mathbf{q})$
  - $p(\mathbf{x} | \mathbf{q}) = {n \choose {x_1, \ldots, x_K}} \prod_{k=1}^K q_k^{x_k}$

- **Posterior**: $\mathbf{q} | \mathbf{x} \sim \text{Dirichlet}(\alpha_1 + x_1, \ldots, \alpha_K + x_K)$
  - $p(\mathbf{q} | \mathbf{x}) \propto \prod_{k=1}^K q_k^{\alpha_k + x_k - 1}$

---

## Dirichlet Distributions Visualized

```{r dirichlet-visualization}
# Function to plot Dirichlet samples in 2D (for 3 populations)
plot_dirichlet_samples <- function(alpha, n_samples = 1000, title = "Dirichlet Distribution") {
  samples <- rdirichlet(n_samples, alpha)
  
  # Create a data frame for ggplot
  df <- data.frame(
    Pop1 = samples[, 1],
    Pop2 = samples[, 2],
    Pop3 = samples[, 3]
  )
  
  # Create a 2D plot (using first two dimensions)
  # The third dimension is implied since Pop1 + Pop2 + Pop3 = 1
  p <- ggplot(df, aes(x = Pop1, y = Pop2)) +
    geom_point(alpha = 0.5, color = "firebrick", size = 1) +
    # Add contour to show the constraint (Pop1 + Pop2 <= 1)
    geom_abline(intercept = 1, slope = -1, linetype = "dashed") +
    geom_text(aes(x = 0.5, y = 0.6), 
              label = paste0("Dirichlet", "(", paste(alpha, collapse = ","), ")"),
              size = 4) +
    labs(title = title,
         x = "Population 1 Proportion", 
         y = "Population 2 Proportion") +
    coord_equal() +  # Equal aspect ratio
    xlim(0, 1) + 
    ylim(0, 1) +
    theme_minimal()
  
  return(p)
}

# Visualize different Dirichlet distributions
p1 <- plot_dirichlet_samples(c(1, 1, 1), title = "Uniform on the simplex")
p2 <- plot_dirichlet_samples(c(5, 5, 5), title = "Concentrated in center")
p3 <- plot_dirichlet_samples(c(0.5, 0.5, 0.5), title = "Concentrated at corners")
p4 <- plot_dirichlet_samples(c(10, 2, 2), title = "Favors first population")

# Display the plots
gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
```

---

# Mixture Models for Population Structure

---

## The Mathematics of Mixture Models

A mixture model with $K$ components is defined as:

$$p(x) = \sum_{k=1}^K \pi_k \, f_k(x | \theta_k)$$

Where:
- $\pi_k$ are mixing proportions ($\sum_{k=1}^K \pi_k = 1$)
- $f_k(x | \theta_k)$ are component densities
- $\theta_k$ are parameters for each component

For population structure:
- Components = ancestral populations
- Mixing proportions = ancestry percentages
- Component densities = population-specific allele frequencies

---

## Simulating Population-Specific Allele Frequencies

```{r mixture-model-visualization}
# Simulate data from a mixture of 3 populations
n_loci <- 500
n_pops <- 3
n_individuals <- 150  # Make divisible by 3

# Generate distinct allele frequencies for each source population
pop_allele_freqs <- matrix(0, nrow = n_pops, ncol = n_loci)
pop_allele_freqs[1,] <- rbeta(n_loci, 1, 5)    # Skewed toward low frequencies
pop_allele_freqs[2,] <- rbeta(n_loci, 5, 1)    # Skewed toward high frequencies
pop_allele_freqs[3,] <- rbeta(n_loci, 2, 2)    # Centered around 0.5

# Visualize the allele frequency distributions
pop_freqs_df <- as.data.frame(t(pop_allele_freqs))
names(pop_freqs_df) <- paste0("Population_", 1:n_pops)
pop_freqs_melt <- reshape2::melt(pop_freqs_df)

ggplot(pop_freqs_melt, aes(x = value, fill = variable)) +
  geom_density(alpha = 0.5) +
  scale_fill_discrete() +
  labs(title = "Allele Frequency Distributions in Source Populations",
       subtitle = "Each population has a distinct genetic profile",
       x = "Allele Frequency", y = "Density",
       fill = "Population") +
  theme(legend.position = "bottom")
```

---

## Admixed Populations - The Model

For an individual with admixture proportions $q = (q_1, q_2, \ldots, q_K)$:

The effective allele frequency at locus $j$ is:
$$p_{ij} = \sum_{k=1}^K q_{ik} \cdot f_{kj}$$

Where:
- $p_{ij}$ = effective allele frequency for individual $i$ at locus $j$
- $q_{ik}$ = ancestry proportion from population $k$
- $f_{kj}$ = allele frequency in population $k$ at locus $j$

Genotypes are generated as:
$$g_{ij} \sim \text{Bernoulli}(p_{ij})$$

---

## Generated Admixed Individuals

```{r admixture-simulation}
# Generate admixture proportions for each individual from Dirichlet distribution
# Make three clusters with different characteristic admixture patterns
n_per_cluster <- floor(n_individuals / 3)  # Ensure it's an integer
n_individuals = n_per_cluster * 3  # Adjust n_individuals to match

# Cluster 1: Mostly population 1 with some admixture
admix_cluster1 <- rdirichlet(n_per_cluster, c(10, 2, 2))

# Cluster 2: Mostly population 2 with some admixture
admix_cluster2 <- rdirichlet(n_per_cluster, c(2, 10, 2))

# Cluster 3: Mostly population 3 with some admixture
admix_cluster3 <- rdirichlet(n_per_cluster, c(2, 2, 10))

# Combine all admixture proportions
admixture_props <- rbind(admix_cluster1, admix_cluster2, admix_cluster3)
colnames(admixture_props) <- paste0("Pop", 1:n_pops)

# Generate genotypes based on admixture proportions
genotypes <- matrix(0, nrow = n_individuals, ncol = n_loci)

for(i in 1:n_individuals) {
  # For each locus, compute the effective allele frequency as the weighted average
  effective_freqs <- as.numeric(admixture_props[i,] %*% pop_allele_freqs)
  
  # Generate genotype by sampling from Bernoulli with the effective frequency
  genotypes[i,] <- rbinom(n_loci, 1, effective_freqs)
}

# Visualize the true admixture proportions
admix_df <- as.data.frame(admixture_props)
admix_df$Individual <- 1:n_individuals
admix_df$Cluster <- rep(1:3, each = n_per_cluster)
admix_long <- pivot_longer(admix_df, cols = starts_with("Pop"), 
                          names_to = "Population", values_to = "Proportion")

ggplot(admix_long, aes(x = Individual, y = Proportion, fill = Population)) +
  geom_col(width = 1) +
  scale_fill_discrete() +
  facet_grid(. ~ Cluster, scales = "free_x", space = "free_x") +
  labs(title = "True Admixture Proportions of Simulated Individuals",
       subtitle = "Individuals grouped into three clusters with characteristic admixture patterns",
       x = "Individual", y = "Ancestry Proportion") +
  theme(axis.text.x = element_blank(), 
        panel.spacing = unit(0.1, "lines"),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
```

---

## Visualizing with PCA

```{r pca-visualization}
# Perform PCA on the genotype data
pca_result <- prcomp(genotypes, center = TRUE, scale. = FALSE)

# Create a data frame for plotting
pca_df <- data.frame(
  PC1 = pca_result$x[, 1],
  PC2 = pca_result$x[, 2],
  Cluster = as.factor(rep(1:3, each = n_per_cluster))
)

# Calculate the most dominant ancestry for each individual
dominant_pop <- apply(admixture_props, 1, which.max)
dominant_pop_names <- paste0("Pop", dominant_pop)

# Add this information to the PCA data frame
pca_df$DominantPop <- as.factor(dominant_pop_names)

# Plot PCA colored by dominant ancestry
ggplot(pca_df, aes(x = PC1, y = PC2, color = DominantPop)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_discrete() +
  labs(title = "PCA of Simulated Genotype Data",
       subtitle = "Each point represents an individual colored by dominant ancestry",
       x = paste0("PC1 (", round(summary(pca_result)$importance[2, 1] * 100, 1), "% variance)"),
       y = paste0("PC2 (", round(summary(pca_result)$importance[2, 2] * 100, 1), "% variance)"),
       color = "Dominant\nAncestry") +
  theme(legend.position = "right")
```

---

## Bayesian Inference for Mixture Models

The Bayesian framework:

**Latent variables**: 
- $z_i$ = population assignment for individual $i$
- $P_{kj}$ = allele frequency for population $k$ at locus $j$

**Likelihood**:
$$p(g_{ij} | z_i = k, P_{kj}) = P_{kj}^{g_{ij}} (1 - P_{kj})^{1 - g_{ij}}$$

**Priors**:
- $P_{kj} \sim \text{Beta}(a, b)$ (e.g., $\text{Beta}(1,1)$ = uniform)
- $z_i \sim \text{Categorical}(\pi_1, \ldots, \pi_K)$

**Posterior**: 
$$p(Z, P | G) \propto p(G | Z, P) p(Z) p(P)$$

---

## Gibbs Sampling Implementation

We can implement a Gibbs sampler with two steps:

1. Update $z_i | P, G$ for each individual:
$$p(z_i = k | P, G) \propto \pi_k \prod_{j=1}^J P_{kj}^{g_{ij}} (1 - P_{kj})^{1 - g_{ij}}$$

2. Update $P_{kj} | z, G$ for each population and locus:
$$P_{kj} | z, G \sim \text{Beta}(a + \sum_{i: z_i=k} g_{ij}, b + \sum_{i: z_i=k} (1 - g_{ij}))$$

This gives us a Markov Chain that converges to the posterior distribution.

---

# Clinical Trials and Non-Informative Priors

---

## The Value of Flat Priors

In both clinical trials and genomics, flat priors are useful when:

1. Limited prior knowledge exists
2. You want the data to "speak for itself"
3. Results need to be defensible to skeptical audiences

For a proportion parameter (e.g., allele frequency):

- **Uniform prior**: $\theta \sim \text{Beta}(1,1)$
- **Posterior**: $\theta | y \sim \text{Beta}(1+y, 1+n-y)$

---

## Example: Clinical Trial with Flat Prior

```{r clinical-trial-flat-prior}
# Simulate a clinical trial comparing two treatments
set.seed(123)

# Trial parameters
n_patients <- 200
true_effect_size <- 0.15  # 15% improvement with treatment

# Generate outcomes
control_outcomes <- rbinom(n_patients/2, 1, 0.30)       # 30% success rate
treatment_outcomes <- rbinom(n_patients/2, 1, 0.30 + true_effect_size)  # 45% success rate

# Traditional frequentist analysis
control_success <- mean(control_outcomes)
treatment_success <- mean(treatment_outcomes)
difference <- treatment_success - control_success

# Perform Chi-square test
contingency_table <- matrix(
  c(sum(treatment_outcomes), length(treatment_outcomes) - sum(treatment_outcomes),
    sum(control_outcomes), length(control_outcomes) - sum(control_outcomes)),
  nrow = 2
)
chi_square_test <- chisq.test(contingency_table, correct = FALSE)
p_value <- chi_square_test$p.value

# Bayesian analysis with flat prior (Beta(1,1) = uniform)
prior_alpha <- 1
prior_beta <- 1

# Update with data
post_alpha_control <- prior_alpha + sum(control_outcomes)
post_beta_control <- prior_beta + length(control_outcomes) - sum(control_outcomes)

post_alpha_treatment <- prior_alpha + sum(treatment_outcomes)
post_beta_treatment <- prior_beta + length(treatment_outcomes) - sum(treatment_outcomes)

# Plot the posterior distributions
x <- seq(0, 1, length = 1000)
control_density <- dbeta(x, post_alpha_control, post_beta_control)
treatment_density <- dbeta(x, post_alpha_treatment, post_beta_treatment)

posterior_df <- data.frame(
  x = rep(x, 2),
  density = c(control_density, treatment_density),
  group = rep(c("Control", "Treatment"), each = 1000)
)

ggplot(posterior_df, aes(x = x, y = density, fill = group)) +
  geom_area(alpha = 0.6, position = "identity") +
  scale_fill_manual(values = c("Control" = "royalblue", "Treatment" = "firebrick")) +
  labs(title = "Posterior Distributions of Response Rates",
       subtitle = paste0("p-value = ", round(p_value, 3), 
                       ", Frequentist difference = ", round(difference*100, 1), "%"),
       x = "Response Rate", y = "Posterior Density",
       fill = "Group") +
  theme(legend.position = "bottom")
```

---

## Direct Probability of Treatment Benefit

```{r posterior-probability-superiority}
# Monte Carlo estimation of P(treatment > control)
n_samples <- 100000
control_samples <- rbeta(n_samples, post_alpha_control, post_beta_control)
treatment_samples <- rbeta(n_samples, post_alpha_treatment, post_beta_treatment)

# Calculate the probability that treatment is better than control
prob_superiority <- mean(treatment_samples > control_samples)

# Calculate the distribution of differences
diff_samples <- treatment_samples - control_samples

# Create a data frame for the difference distribution
diff_df <- data.frame(difference = diff_samples)

# Plot the distribution of differences
ggplot(diff_df, aes(x = difference)) +
  geom_histogram(bins = 50, fill = "purple", alpha = 0.7) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  geom_vline(xintercept = quantile(diff_samples, c(0.025, 0.975)), 
             linetype = "dotted", color = "red") +
  annotate("text", x = 0.1, y = n_samples/15, 
           label = paste0("P(Treatment > Control) = ", round(prob_superiority*100, 1), "%"),
           color = "blue") +
  labs(title = "Posterior Distribution of Treatment Effect",
       subtitle = "Difference between treatment and control response rates",
       x = "Treatment Effect (Treatment - Control)", 
       y = "Frequency") +
  xlim(-0.3, 0.3)
```

---

# Multivariate Normal Mixtures and mash

---

## The Multivariate Challenge in Genomics

Genomic studies often examine effects across multiple:

- Traits (height, weight, BMI)
- Tissues (liver, muscle, brain)
- Populations (African, European, Asian)
- Conditions (different environments/exposures)

Existing approaches have limitations:
- Single-trait analysis wastes information
- Fixed-effects meta-analysis ignores heterogeneity
- Random-effects models are too simplistic

---

## Multivariate Adaptive Shrinkage (mash)

**mash** provides a flexible Bayesian framework for:

1. **Sharing information** across related phenotypes/tissues
2. **Identifying consistent vs. context-specific effects**
3. **Handling multiple testing** across thousands/millions of loci
4. **Improving estimation accuracy** by borrowing strength

For variant $j$ across $R$ conditions:

$\hat{\beta}_j \sim N(\beta_j, S_j)$

With prior on true effects:

$\beta_j \sim \sum_{k=1}^K \pi_k N(0, U_k)$

---

## Key Covariance Matrices in mash

mash uses a mixture of covariance matrices to capture different patterns:

::: {.incremental}
- **Identity:** $U_I = \sigma^2 I$ (Independent effects)
- **Rank-1:** $U_R = \sigma^2 \mathbf{1}\mathbf{1}^T$ (Perfectly correlated effects)
- **Condition-specific:** $U_r = \sigma^2 \mathbf{e}_r\mathbf{e}_r^T$ (Effect only in condition $r$)
- **Data-driven:** Learned from patterns in the data
:::

---

## Simulated Multivariate Effects

```{r mash-simulation}
set.seed(789)

# Simulation parameters
n_variants <- 500
n_traits <- 5
n_null <- 400  # Number of variants with no effect
n_shared <- 50  # Number of variants with shared effects across traits
n_specific <- 50  # Number of variants with trait-specific effects

# Create empty effect size matrix (variants × traits)
true_effects <- matrix(0, nrow = n_variants, ncol = n_traits)
colnames(true_effects) <- paste0("Trait", 1:n_traits)

# Assign effects:
# 1. Shared effects (same direction and similar magnitude across traits)
shared_idx <- 1:n_shared
shared_magnitude <- rnorm(n_shared, mean = 0, sd = 0.5)
true_effects[shared_idx, ] <- matrix(
  rep(shared_magnitude, each = n_traits), 
  nrow = n_shared
)

# 2. Trait-specific effects
specific_idx <- (n_shared + 1):(n_shared + n_specific)
for(i in 1:n_specific) {
  trait <- sample(1:n_traits, 1)
  true_effects[n_shared + i, trait] <- rnorm(1, mean = 0, sd = 0.8)
}

# Generate observed effect estimates with noise
standard_errors <- matrix(runif(n_variants * n_traits, 0.1, 0.3), 
                         nrow = n_variants, ncol = n_traits)
observed_effects <- true_effects + matrix(rnorm(n_variants * n_traits), 
                                        nrow = n_variants) * standard_errors

# Calculate correlation structure between traits
true_cor <- cor(true_effects)
obs_cor <- cor(observed_effects)

# Visualize correlation structures
plot_corr_matrices <- function(m1, m2, title1, title2) {
  df1 <- reshape2::melt(m1)
  df2 <- reshape2::melt(m2)
  
  p1 <- ggplot(df1, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), name = "Correlation") +
    theme_minimal() +
    labs(title = title1, x = "", y = "")
  
  p2 <- ggplot(df2, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), name = "Correlation") +
    theme_minimal() +
    labs(title = title2, x = "", y = "")
  
  gridExtra::grid.arrange(p1, p2, ncol = 2)
}

plot_corr_matrices(true_cor, obs_cor, 
                  "True Correlation Structure", 
                  "Observed Correlation Structure")
```

---

## mash Posterior Calculations

For each covariance matrix $U_k$, the posterior distribution is:

$\beta_j | \hat{\beta}_j, U_k, S_j \sim N(\mu_{jk}, V_{jk})$

Where:
- $V_{jk} = S_j - S_j(S_j + U_k)^{-1}S_j$
- $\mu_{jk} = V_{jk}S_j^{-1}\hat{\beta}_j$

The posterior mean under the mixture:

$E[\beta_j | \hat{\beta}_j] = \sum_{k=1}^K w_{jk} \mu_{jk}$

Where weights come from the marginal likelihood:

$w_{jk} \propto \pi_k (2\pi)^{-d/2}|S_j + U_k|^{-1/2}\exp\left(-\frac{1}{2}\hat{\beta}_j^T(S_j + U_k)^{-1}\hat{\beta}_j\right)$

---

## Improved Estimation with mash

```{r mash-visualization}
# Define simple examples for visualization
example_variants <- list(
  # A variant with shared effects
  list(
    name = "Shared Effect Variant",
    observed = c(0.5, 0.6, 0.4, 0.5, 0.6),
    se = c(0.2, 0.2, 0.2, 0.2, 0.2),
    true = c(0.4, 0.4, 0.4, 0.4, 0.4)
  ),
  # A variant with trait-specific effect
  list(
    name = "Trait-Specific Variant",
    observed = c(0.1, 0.8, 0.1, 0.1, 0.1),
    se = c(0.2, 0.2, 0.2, 0.2, 0.2),
    true = c(0.0, 0.7, 0.0, 0.0, 0.0)
  )
)

# Define covariance matrices
U_list <- list(
  "Independent" = diag(n_traits) * 0.25,
  "Shared" = matrix(0.25, n_traits, n_traits),
  "Null" = diag(n_traits) * 0.01
)

# Function to calculate multivariate normal likelihood 
mvn_likelihood <- function(beta, Sigma, mu = rep(0, length(beta))) {
  k <- length(mu)
  dev <- beta - mu
  # Handle numerical issues
  tryCatch({
    quadform <- t(dev) %*% solve(Sigma, dev)
    return(exp(-0.5 * as.numeric(quadform)) / sqrt((2*pi)^k * det(Sigma)))
  }, error = function(e) {
    return(0)  # Return 0 if there's an error
  })
}

# Calculate and plot posteriors for example variants
plot_variant_posteriors <- function(variant) {
  obs <- variant$observed
  se <- variant$se
  S <- diag(se^2)
  true <- variant$true
  
  # Calculate weights for each covariance
  weights <- sapply(U_list, function(U) {
    Sigma <- S + U
    mvn_likelihood(obs, Sigma)
  })
  
  # Normalize weights
  weights <- weights / sum(weights)
  
  # Calculate posteriors
  posteriors <- list()
  for(k in 1:length(U_list)) {
    U <- U_list[[k]]
    Sigma <- S + U
    
    # Posterior variance (simplified)
    V <- U - U %*% solve(Sigma) %*% U
    
    # Posterior mean
    mu <- U %*% solve(Sigma) %*% obs
    
    posteriors[[k]] <- list(mu = mu, V = V)
  }
  
  # Calculate weighted posterior
  weighted_mean <- rep(0, n_traits)
  for(k in 1:length(U_list)) {
    weighted_mean <- weighted_mean + weights[k] * posteriors[[k]]$mu
  }
  
  # Create data frame for plotting
  df <- data.frame(
    Trait = factor(paste0("Trait", 1:n_traits)),
    Observed = obs,
    True = true,
    Posterior = weighted_mean
  )
  
  # Reshape for plotting
  df_long <- reshape2::melt(df, id.vars = "Trait")
  
  # Create bar plot
  ggplot(df_long, aes(x = Trait, y = value, fill = variable)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    scale_fill_manual(values = c("Observed" = "lightblue", 
                                "True" = "darkgreen", 
                                "Posterior" = "firebrick")) +
    labs(title = variant$name,
         subtitle = paste0("Top component: ", 
                         names(weights)[which.max(weights)], 
                         " (weight = ", round(max(weights), 2), ")"),
         x = "", y = "Effect Size") +
    theme_minimal()
}

# Plot examples
example_plots <- lapply(example_variants, plot_variant_posteriors)
gridExtra::grid.arrange(grobs = example_plots, ncol = 2)
```

---

# Adaptive Designs in Population Genomics

---

## Sequential Analysis and Sample Size Optimization

```{r adaptive-design}
# Simulate a genetic association study with sequential sampling
set.seed(456)

# Parameters
n_loci <- 1000
true_effect_size <- 0.3
max_samples <- 500
interim_points <- seq(100, max_samples, by = 100)

# Set 5% of loci to have true effects
n_causal <- floor(n_loci * 0.05)
causal_loci <- sample(1:n_loci, n_causal)

# Simplified simulation
interim_summary <- data.frame(
  SampleSize = interim_points,
  Discoveries_50 = c(30, 42, 48, 53, 55),
  Discoveries_80 = c(15, 24, 32, 38, 42),
  Discoveries_95 = c(8, 15, 22, 28, 32),
  TruePositives_95 = c(7, 12, 18, 23, 26),
  FalsePositives_95 = c(1, 3, 4, 5, 6)
)

interim_summary$FDR_95 <- interim_summary$FalsePositives_95 / 
                          interim_summary$Discoveries_95
interim_summary$Power_95 <- interim_summary$TruePositives_95 / n_causal

# Create long format data for plotting
interim_long <- pivot_longer(
  interim_summary, 
  cols = c(Discoveries_50, Discoveries_80, Discoveries_95),
  names_to = "Threshold",
  values_to = "Discoveries"
)
interim_long$Threshold <- factor(
  interim_long$Threshold,
  levels = c("Discoveries_50", "Discoveries_80", "Discoveries_95"),
  labels = c("PP > 0.5", "PP > 0.8", "PP > 0.95")
)

# Plot discovery trajectories
ggplot(interim_long, aes(x = SampleSize, y = Discoveries, color = Threshold, group = Threshold)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  scale_color_discrete() +
  labs(title = "Number of Discoveries vs. Sample Size",
       subtitle = "Results at different posterior probability thresholds",
       x = "Sample Size", y = "Number of Discoveries") +
  theme(legend.position = "bottom")
```

---

## Decision-Theoretic Framework

```{r decision-theory}
# Define costs and benefits
cost_per_sample <- 1        # Cost per additional sample
cost_false_positive <- 50   # Cost of following up a false lead
cost_false_negative <- 100  # Cost of missing a true association
benefit_true_positive <- 200  # Benefit of discovering a true association

# Calculate expected utility at each sample size
utility_df <- data.frame(
  SampleSize = interim_summary$SampleSize,
  SamplingCost = -interim_summary$SampleSize * cost_per_sample,
  FalsePositiveCost = -interim_summary$FalsePositives_95 * cost_false_positive,
  FalseNegativeCost = -(n_causal - interim_summary$TruePositives_95) * cost_false_negative,
  TruePositiveBenefit = interim_summary$TruePositives_95 * benefit_true_positive
)

utility_df$TotalUtility <- utility_df$SamplingCost + 
                         utility_df$FalsePositiveCost + 
                         utility_df$FalseNegativeCost +
                         utility_df$TruePositiveBenefit

# Plot total utility 
optimal_n <- utility_df$SampleSize[which.max(utility_df$TotalUtility)]

ggplot(utility_df, aes(x = SampleSize)) +
  geom_line(aes(y = TotalUtility), size = 1.2, color = "blue") +
  geom_point(aes(y = TotalUtility), size = 3, color = "blue") +
  geom_vline(xintercept = optimal_n, linetype = "dotted", color = "red") +
  annotate("text", x = optimal_n + 20, y = max(utility_df$TotalUtility) * 0.9, 
           label = paste("Optimal n =", optimal_n), color = "red") +
  labs(title = "Total Utility by Sample Size",
       subtitle = "Bayesian decision theory identifies optimal design",
       x = "Sample Size", 
       y = "Total Utility") +
  theme_minimal()
```

---

# Conclusion and Resources

---

## Key Takeaways

- Bayesian statistics offers a coherent framework for population genomics
- Posterior probabilities directly answer: "What's the probability of association?"
- Conjugate priors provide elegant solutions for allele frequencies and population structure
- Decision theory allows optimal tradeoffs between false discoveries and missed associations
- Multivariate models like mash leverage information across related traits
- Bayesian adaptive designs optimize resource allocation

---

## Software Tools and Resources

::: {.incremental}
- **R Packages**
  - `rstan`, `brms`: General Bayesian modeling
  - `mashr`: Multivariate adaptive shrinkage
  - `structure`, `admixture`: Population structure

- **Books & Resources**
  - "Statistical Rethinking" by Richard McElreath
  - "Bayesian Data Analysis" by Gelman et al.
  - [Five Minute Statistics](http://stephens999.github.io/fiveMinuteStats/): Short tutorials on Bayesian concepts

- **Applications**
  - [GWAS with Bayesian priors](https://stephenslab.github.io/fiveMinuteStats/random_directions.html)
  - [Sharing information across studies](https://stephenslab.github.io/mashr/articles/intro_mash.html)
:::

---

## Questions?

Thank you for your attention!

<div style="text-align: center; margin-top: 2em;">
<img src="https://bayes.wustl.edu/Images/Bayes.jpg" width="200">
</div>
<!DOCTYPE html>
<html lang="en"><head>
<script src="cleaned_mpg5_files/libs/clipboard/clipboard.min.js"></script>
<script src="cleaned_mpg5_files/libs/quarto-html/tabby.min.js"></script>
<script src="cleaned_mpg5_files/libs/quarto-html/popper.min.js"></script>
<script src="cleaned_mpg5_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="cleaned_mpg5_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="cleaned_mpg5_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="cleaned_mpg5_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="cleaned_mpg5_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <meta name="author" content="Sarah Urbut, MD PhD">
  <meta name="dcterms.date" content="2025-03-29">
  <title>A Practical Primer on Bayesian Statistics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="cleaned_mpg5_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="cleaned_mpg5_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="cleaned_mpg5_files/libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="custom.css">
  <link href="cleaned_mpg5_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="cleaned_mpg5_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="cleaned_mpg5_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="cleaned_mpg5_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">A Practical Primer on Bayesian Statistics</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Sarah Urbut, MD PhD 
</div>
</div>
</div>

  <p class="date">2025-03-29</p>
</section>
<section id="overview" class="slide level2">
<h2>Overview</h2>
<p>In this seminar, we’ll cover:</p>
<ol type="1">
<li>Introduction to Bayesian thinking and posterior distributions</li>
<li>The p-value fallacy and Bayesian alternatives</li>
<li>Conjugate priors for genetic models (Beta-Binomial, Dirichlet)</li>
<li>Mixture models for population structure</li>
<li>Clinical trials with flat priors - lessons for genomics</li>
<li>Multivariate normal mixtures (mash)</li>
</ol>
</section>
<section>
<section id="introduction-to-bayesian-thinking" class="title-slide slide level1 center">
<h1>Introduction to Bayesian Thinking</h1>

</section>
<section id="why-bayesian-for-population-genomics" class="slide level2">
<h2>Why Bayesian for Population Genomics?</h2>
<p>Population genomics presents unique challenges:</p>
<ul>
<li><strong>Multiple testing</strong> across thousands/millions of variants</li>
<li><strong>Complex patterns</strong> across populations and traits</li>
<li><strong>Prior knowledge</strong> from evolution and previous studies</li>
<li><strong>Decision-making</strong> under uncertainty</li>
</ul>
<p>Bayesian approaches offer elegant solutions to these challenges.</p>
</section>
<section id="probabilistic-interpretation-of-estimates" class="slide level2">
<h2>Probabilistic Interpretation of Estimates</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>In the Bayesian framework:</p>
<ul>
<li><strong>Parameters are random variables</strong> with distributions, not fixed values</li>
<li><strong>Uncertainty is represented directly</strong> through probability distributions</li>
<li><strong>All evidence is integrated coherently</strong> within a probability framework</li>
<li><strong>Natural quantification of uncertainty</strong> without hypothetical repeated sampling</li>
<li><strong>Interpretation is direct and intuitive</strong> for researchers and clinicians</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-1-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="bayesian-vs.-frequentist-intervals" class="slide level2">
<h2>Bayesian vs.&nbsp;Frequentist Intervals</h2>
<table class="caption-top">
<colgroup>
<col style="width: 46%">
<col style="width: 53%">
</colgroup>
<thead>
<tr class="header">
<th>Bayesian Credible Interval</th>
<th>Frequentist Confidence Interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“95% probability the parameter is between a and b”</td>
<td>“If we repeated the experiment many times, 95% of intervals constructed would contain the true parameter”</td>
</tr>
<tr class="even">
<td>Directly interpretable as probability statement about the parameter</td>
<td>Cannot be interpreted as probability statement about the parameter</td>
</tr>
<tr class="odd">
<td>Incorporates prior information</td>
<td>No mechanism to incorporate prior information</td>
</tr>
<tr class="even">
<td>Can be asymmetric, reflecting asymmetric uncertainty</td>
<td>Typically symmetric by construction</td>
</tr>
<tr class="odd">
<td>Conditioning on the observed data</td>
<td>Based on hypothetical repeated sampling</td>
</tr>
</tbody>
</table>
</section>
<section id="overview-1" class="slide level2">
<h2>Overview</h2>
<p>In this primer, we’ll explore key Bayesian concepts critical for modern genomics:</p>
<ol type="1">
<li><strong>P-values vs.&nbsp;Posterior Probabilities</strong>: Why Bayesian thinking helps avoid misinterpretations<br>
</li>
<li><strong>Conjugate Models</strong>: Elegant solutions for population genetic inference<br>
</li>
<li><strong>Mixture Models</strong>: Powerful tools for complex genomic data<br>
</li>
<li><strong>Bayesian Clinical &amp; Adaptive Designs</strong>: Learning and adapting as data accumulates</li>
</ol>
</section>
<section id="bayes-theorem---the-core-idea" class="slide level2">
<h2>Bayes’ Theorem - The Core Idea</h2>
<p><span class="math display">\[P(H|D) = \frac{P(D|H) \times P(H)}{P(D)}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(P(H|D)\)</span> is the <strong>posterior probability</strong> - what we want to know</li>
<li><span class="math inline">\(P(D|H)\)</span> is the <strong>likelihood</strong> - how probable the data is under our hypothesis</li>
<li><span class="math inline">\(P(H)\)</span> is the <strong>prior probability</strong> - what we knew before</li>
<li><span class="math inline">\(P(D)\)</span> is the <strong>evidence</strong> - a normalizing constant</li>
</ul>
<p>Simply: <strong>Posterior ∝ Likelihood × Prior</strong></p>
</section>
<section id="bayesian-updating-visual-intuition" class="slide level2">
<h2>Bayesian Updating: Visual Intuition</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/bayesian-updating-1.png" width="960" class="r-stretch"></section></section>
<section>
<section id="p-values-vs.-posterior-probabilities" class="title-slide slide level1 center">
<h1>P-values vs.&nbsp;Posterior Probabilities</h1>

</section>
<section id="the-question" class="slide level2">
<h2>The Question</h2>
<p>As scientists, we want to know:</p>
<blockquote>
<p>“What is the probability that my hypothesis is true, given my data?”</p>
</blockquote>
<p>But traditional p-values answer a different question:</p>
<blockquote>
<p>“What is the probability of observing data this extreme or more extreme, if the null hypothesis is true?”</p>
</blockquote>
<p>This mismatch causes persistent misinterpretations.</p>
</section>
<section id="p-values-vs.-bayes-factors-definitions" class="slide level2">
<h2>P-values vs.&nbsp;Bayes Factors: Definitions</h2>
<p><strong>P-value</strong>:<br>
- <span class="math inline">\(P(data|H_0)\)</span> - probability of data given null hypothesis<br>
- Measures compatibility of data with null hypothesis<br>
- Does not directly measure evidence for alternative</p>
<p><strong>Bayes Factor</strong>:<br>
- <span class="math inline">\(BF_{10} = \frac{P(data|H_1)}{P(data|H_0)}\)</span> - ratio of likelihoods<br>
- Directly compares evidence for alternative vs.&nbsp;null<br>
- Tells you how much to update your beliefs</p>
</section>
<section id="the-p-value-fallacy" class="slide level2">
<h2>The P-value Fallacy</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Scenario</strong>: Testing a SNP for disease association</p>
<p><strong>Traditional approach</strong>:<br>
- Obtain p = 0.001<br>
- Declare “significant association”<br>
- Publish result</p>
<p><strong>The fallacy</strong>:<br>
- p = 0.001 means “1 in 1000 chance of seeing this data if no association exists”<br>
- NOT “999 in 1000 chance the association is real”</p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-2-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
<p><strong>Key insight</strong>: With a realistic prior of 1/1000, a “significant” p-value of 0.001 only gives ~50% posterior probability of true association!</p>
</section>
<section id="the-mathematical-connection" class="slide level2">
<h2>The Mathematical Connection</h2>
<p>Under certain conditions, p-values can be converted to minimum Bayes factors:</p>
<p><span class="math display">\[BF_{min} ≈ -e \times p \times \log(p)\]</span></p>
<p>Meaning even the most favorable interpretation of a p-value provides less evidence than typically assumed:</p>
<table class="caption-top">
<thead>
<tr class="header">
<th>p-value</th>
<th>Minimum Bayes Factor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.05</td>
<td>0.37</td>
</tr>
<tr class="even">
<td>0.01</td>
<td>0.084</td>
</tr>
<tr class="odd">
<td>0.001</td>
<td>0.0083</td>
</tr>
</tbody>
</table>
</section>
<section id="what-is-the-minimum-bayes-factor" class="slide level2">
<h2>What is the Minimum Bayes Factor?</h2>
<p>The Minimum Bayes Factor is calculated as:<br>
<span class="math inline">\(\text{MBF} \approx -e \times p \times \log(p)\)</span></p>
<p>This formula (derived by Sellke, Bayarri, and Berger) represents the <strong>smallest possible Bayes factor</strong> that could correspond to a given p-value, regardless of the specific alternative hypothesis being tested.</p>
</section>
<section id="why-minimum" class="slide level2">
<h2>Why “Minimum”?</h2>
<p>It’s called “minimum” because:</p>
<ul>
<li>It assumes the most favorable conditions for the alternative hypothesis<br>
</li>
<li>It represents the strongest possible evidence against the null that could be derived from a p-value<br>
</li>
<li>It’s the smallest value the Bayes factor could take (stronger evidence against null = smaller Bayes factor)</li>
</ul>
</section>
<section id="interpretation" class="slide level2">
<h2>Interpretation</h2>
<p>The MBF represents the ratio of likelihoods:<br>
<span class="math inline">\(\text{MBF} = \frac{P(\text{data}|H_0)}{P(\text{data}|H_1)}\)</span></p>
<p>For example, with p = 0.05:</p>
<p>MBF = 0.37<br>
This means the data are at most 1/0.37 ≈ 2.7 times more likely under the alternative than the null<br>
Even with the most optimistic assumptions, the evidence against the null is modest</p>
<p>With p = 0.001:</p>
<p>MBF = 0.0083<br>
This means the data are at most 1/0.0083 ≈ 120 times more likely under the alternative<br>
Much stronger evidence, but still not as extreme as the very small p-value might suggest</p>
</section>
<section id="why-this-conversion-matters" class="slide level2">
<h2>Why This Conversion Matters</h2>
<p>This conversion from p-values to MBF is important because:</p>
<ul>
<li>It provides a more calibrated interpretation of statistical evidence<br>
</li>
<li>It shows that conventional “statistical significance” (p &lt; 0.05) actually represents fairly modest evidence<br>
</li>
<li>It helps researchers avoid overinterpreting p-values<br>
</li>
<li>It establishes a link between frequentist and Bayesian approaches</li>
</ul>
<p>The key point is that p-values systematically overstate the evidence against the null hypothesis.</p>
<p>When converted to the Bayes factor scale, even a seemingly impressive p-value often translates to much more moderate evidence against the null hypothesis than most researchers would expect.</p>
</section>
<section id="p-values-vs.-bayes-factors-in-genomics" class="slide level2">
<h2>P-values vs.&nbsp;Bayes Factors in Genomics</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-3-1.png" width="960" class="r-stretch"><p>The GWAS significance threshold of p &lt; 5×10⁻⁸ corresponds to much stronger evidence than p = 0.05.</p>
</section>
<section id="interpreting-bayes-factors" class="slide level2">
<h2>Interpreting Bayes Factors</h2>
<p>Bayes factors have a natural interpretation:</p>
<table class="caption-top">
<thead>
<tr class="header">
<th>Bayes Factor (<span class="math inline">\(BF_{10}\)</span>)</th>
<th>Evidence for H1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1 - 3</td>
<td>Barely worth mentioning</td>
</tr>
<tr class="even">
<td>3 - 10</td>
<td>Substantial</td>
</tr>
<tr class="odd">
<td>10 - 30</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>30 - 100</td>
<td>Very strong</td>
</tr>
<tr class="odd">
<td>&gt; 100</td>
<td>Extreme</td>
</tr>
</tbody>
</table>
<p>A <span class="math inline">\(BF_{10} = 10\)</span> means the data are 10 times more likely under H1 than H0.</p>
</section>
<section id="from-bayes-factor-to-posterior-probability" class="slide level2">
<h2>From Bayes Factor to Posterior Probability</h2>
<p>Bayes’ theorem connects all the pieces:</p>
<p><span class="math display">\[P(H_1|data) = \frac{P(data|H_1)P(H_1)}{P(data|H_1)P(H_1) + P(data|H_0)P(H_0)}\]</span></p>
<p>This can be rewritten using the Bayes factor:</p>
<p><span class="math display">\[P(H_1|data) = \frac{BF_{10} \times P(H_1)}{BF_{10} \times P(H_1) + P(H_0)}\]</span></p>
</section>
<section id="posterior-odds-formulation" class="slide level2">
<h2>Posterior Odds Formulation</h2>
<p>A simplified version:</p>
<p><span class="math display">\[\text{Posterior Odds} = \text{Bayes Factor} \times \text{Prior Odds}\]</span></p>
<p>Or:</p>
<p><span class="math display">\[\frac{P(H_1|data)}{P(H_0|data)} = BF_{10} \times \frac{P(H_1)}{P(H_0)}\]</span></p>
<p>This clearly shows how Bayes factors calibrate our prior beliefs.</p>
</section>
<section id="practical-example-gwas" class="slide level2">
<h2>Practical Example: GWAS</h2>
<p>In a genome-wide association study:</p>
<ul>
<li>Prior probability of true association: ~1/10,000 per variant<br>
</li>
<li>p-value threshold: 5×10^{-8}<br>
</li>
<li>Corresponding minimum Bayes factor: ~10^{-6}<br>
</li>
<li>Posterior probability: ~9%</li>
</ul>
<p><strong>Interpretation</strong>: Even at genome-wide significance, most “discoveries” may be false positives without additional evidence!</p>
</section>
<section id="a-useful-formula" class="slide level2">
<h2>A useful formula</h2>
<p>There is another way of laying out this kind of calculation, which may be slightly easier to interpret and remember, and also has the advantage of holding even when more than two models are under consideration. From Bayes theorem we have</p>
<p><span class="math inline">\(\Pr(Z_i=1|x_i)=\Pr(x_i|Z_i=1)\Pr(Z_i=1)/\Pr(x_i)\)</span>.</p>
<p>and</p>
<p><span class="math inline">\(\Pr(Z_i=0|x_i)=\Pr(x_i|Z_i=0)\Pr(Z_i=0)/\Pr(x_i)\)</span>.</p>
<p>Taking the ratio of these gives<br>
<span class="math inline">\(\Pr(Z_i=1|x_i)/\Pr(Z_i=0|x_i)=[\Pr(Z_i=1)/\Pr(Z_i=0)]×[\Pr(x_i|Z_i=1)/\Pr(x_i|Z_i=0)]\)</span>.</p>
<p>This formula can be conveniently stated in words, using the notion of <em>odds</em>, as follows:<br>
Posterior Odds = Prior Odds × LR<br>
or, recalling that the LR is sometimes referred to as the Bayes Factor (BF), we have<br>
Posterior Odds = Prior Odds × BF.</p>
<p>Note that the “Odds” of an event E1 vs an event E2 means the ratio of their probabilities. So <span class="math inline">\(\Pr(Z_i=1)/\Pr(Z_i=0)\)</span> is the “Prior Odds”, because it is the odds prior to seeing the data <span class="math inline">\(x\)</span>. Similarly the Posterior Odds refers to the Odds of <span class="math inline">\(Z_i=1\)</span> vs <span class="math inline">\(Z_i=0\)</span> “posterior to” (after) seeing the data <span class="math inline">\(x\)</span>.</p>
</section>
<section id="benefits-of-bayes-factors-for-genomics" class="slide level2">
<h2>Benefits of Bayes Factors for Genomics</h2>
<ol type="1">
<li><strong>Calibrated evidence</strong>: Direct measure of evidence strength<br>
</li>
<li><strong>Multiple testing</strong>: Naturally incorporates prior odds<br>
</li>
<li><strong>Replication</strong>: Coherent framework for combining evidence across studies<br>
</li>
<li><strong>Diverse hypotheses</strong>: Can compare non-nested models<br>
</li>
<li><strong>Positive evidence</strong>: Can support null hypothesis, not just reject it<br>
</li>
<li><strong>Study design</strong>: Allows stopping when evidence is sufficient</li>
</ol>
</section>
<section id="key-takeaways" class="slide level2">
<h2>Key Takeaways</h2>
<ol type="1">
<li>P-values answer a different question than most scientists ask<br>
</li>
<li>Bayes factors directly compare competing hypotheses<br>
</li>
<li>Even “significant” p-values provide weaker evidence than typically assumed<br>
</li>
<li>Bayes factors have a natural interpretation as evidence strength<br>
</li>
<li>Converting to posterior probabilities requires considering prior odds<br>
</li>
<li>In genomics, this perspective helps manage false discovery rates</li>
</ol>
</section>
<section id="the-fallacy-of-p-values" class="slide level2">
<h2>The Fallacy of P-values</h2>
<div>
<ul>
<li class="fragment"><p>P-values answer a <strong>counterfactual question</strong>: “If there were no effect, how surprising would these data be?”</p></li>
<li class="fragment"><p>But researchers want to know: “<strong>What is the probability this association is real?</strong>”</p></li>
<li class="fragment"><p>This disconnect leads to systematic misinterpretation</p></li>
</ul>
</div>
</section>
<section id="the-multiple-testing-challenge" class="slide level2">
<h2>The Multiple Testing Challenge</h2>
<p>Modern genomics routinely tests <strong>thousands to millions</strong> of hypotheses:</p>
<ul>
<li>20,000+ genes in differential expression<br>
</li>
<li>Millions of variants in GWAS<br>
</li>
<li>Billions of potential interactions</li>
</ul>
<p><strong>The consequence</strong>: Many “significant” findings are actually false positives.</p>
</section>
<section id="the-traditional-approach" class="slide level2">
<h2>The Traditional Approach</h2>
<p>When testing m hypotheses at significance level α:</p>
<ul>
<li>Expected number of false positives: m × α<br>
</li>
<li>With m = 1,000,000 and α = 0.05: <strong>50,000 false positives!</strong></li>
</ul>
<p><strong>Frequentist solutions</strong>:<br>
- Bonferroni correction: α/m<br>
- False Discovery Rate (FDR) control (Benjamini-Hochberg)<br>
- Family-wise error rate (FWER) control</p>
</section>
<section id="the-fundamental-issue" class="slide level2">
<h2>The Fundamental Issue</h2>
<p>The p-value doesn’t tell us what we want to know:</p>
<p><span class="math inline">\(\text{P-value} = P(\text{data}|\text{null})\)</span></p>
<p>What we want is:</p>
<p><span class="math inline">\(P(\text{null}|\text{data})\)</span></p>
<p>To bridge this gap, we need <strong>Bayes’ theorem</strong>.</p>
</section>
<section id="the-bayesian-formulation" class="slide level2">
<h2>The Bayesian Formulation</h2>
<p>We model each test as:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: No effect (null hypothesis)<br>
</li>
<li><span class="math inline">\(H_1\)</span>: Real effect (alternative hypothesis)<br>
</li>
<li>Prior probability: <span class="math inline">\(P(H_1) = \pi_1\)</span><br>
</li>
<li>Likelihood ratio: <span class="math inline">\(BF = \frac{P(\text{data}|H_1)}{P(\text{data}|H_0)}\)</span></li>
</ul>
<p>The posterior probability of a true finding:</p>
<p><span class="math display">\[P(H_1|\text{data}) = \frac{BF \times \pi_1}{BF \times \pi_1 + (1-\pi_1)}\]</span></p>
</section>
<section id="visualizing-multiple-hypothesis-testing" class="slide level2">
<h2>Visualizing Multiple Hypothesis Testing</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-4-1.png" width="960" class="r-stretch"><p>The challenge: Separating true signals (blue) from noise (gray) when true effects are rare.</p>
</section>
<section id="local-false-discovery-rate-lfdr" class="slide level2">
<h2>Local False Discovery Rate (LFDR)</h2>
<p>The LFDR for a test with statistic z is:</p>
<p><span class="math display">\[\text{LFDR}(z) = P(H_0|z) = 1 - P(H_1|z)\]</span></p>
<p>This is a direct Bayesian statement about the probability of false discovery for each individual test.</p>
<p>Benefits:<br>
- Test-specific measure of evidence<br>
- Directly interpretable probability<br>
- Accounts for prior probability of effects</p>
</section>
<section id="empirical-bayes-for-multiple-testing" class="slide level2">
<h2>Empirical Bayes for Multiple Testing</h2>
<p>We can estimate components from the data:</p>
<ol type="1">
<li>Proportion of true effects: <span class="math inline">\(\hat{\pi}_1\)</span><br>
</li>
<li>Distribution of null statistics: <span class="math inline">\(\hat{f}_0(z)\)</span><br>
</li>
<li>Distribution of all statistics: <span class="math inline">\(\hat{f}(z)\)</span></li>
</ol>
<p>Then:</p>
<p><span class="math display">\[\widehat{\text{LFDR}}(z) = \frac{(1-\hat{\pi}_1)\hat{f}_0(z)}{\hat{f}(z)}\]</span></p>
<p>Popular implementations: <code>qvalue</code> package, <code>fdrtool</code>, <code>locfdr</code></p>
</section>
<section id="comparison-with-fdr-control" class="slide level2">
<h2>Comparison with FDR Control</h2>
<p><strong>Frequentist FDR</strong> (Benjamini-Hochberg):<br>
- Controls expected proportion of false discoveries<br>
- Same threshold for all tests based on ranked p-values<br>
- No direct probability interpretation for individual tests</p>
<p><strong>Bayesian approach</strong>:<br>
- Calculates probability of false discovery for each test<br>
- Can incorporate prior information on effect prevalence<br>
- Naturally addresses multiple testing without penalties</p>
</section>
<section id="example-genomic-applications" class="slide level2">
<h2>Example: Genomic Applications</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-5-1.png" width="960" class="r-stretch"><p>Posterior probabilities provide a direct measure of evidence for each test.</p>
</section>
<section id="decision-boundaries-comparison" class="slide level2">
<h2>Decision Boundaries Comparison</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-6-1.png" width="960" class="r-stretch"><p>Both approaches control false discoveries but with different decision boundaries.</p>
</section>
<section id="advantages-of-bayesian-multiple-testing" class="slide level2">
<h2>Advantages of Bayesian Multiple Testing</h2>
<ol type="1">
<li><strong>Direct probabilistic statements</strong>: “95% probability of true effect”<br>
</li>
<li><strong>Incorporates effect prevalence</strong>: Accounts for rarity of true effects<br>
</li>
<li><strong>No arbitrary thresholds</strong>: Posterior probabilities are on probability scale<br>
</li>
<li><strong>Effect size integration</strong>: Naturally considers both significance and magnitude<br>
</li>
<li><strong>Prior information</strong>: Can incorporate domain knowledge<br>
</li>
<li><strong>Decision theory integration</strong>: Allows optimal decision-making based on costs</li>
</ol>
</section>
<section id="extensions" class="slide level2">
<h2>Extensions</h2>
<p>Advanced Bayesian approaches for multiple testing:</p>
<ol type="1">
<li><strong>Hierarchical models</strong>: Sharing information across tests<br>
</li>
<li><strong>Correlated tests</strong>: Modeling dependency structures between tests<br>
</li>
<li><strong>Spatial/network priors</strong>: Incorporating biological relationships<br>
</li>
<li><strong>Mixture priors</strong>: More flexible alternative distributions<br>
</li>
<li><strong>Adaptive shrinkage</strong>: Data-driven prior specification<br>
</li>
<li><strong>Full Bayesian decision</strong>: Incorporating asymmetric costs of errors</li>
</ol>
</section>
<section id="key-takeaways-1" class="slide level2">
<h2>Key Takeaways</h2>
<ol type="1">
<li>Multiple testing is a core challenge in genomics<br>
</li>
<li>Bayesian approaches directly address the quantity of interest<br>
</li>
<li>Local FDR provides test-specific false discovery probabilities<br>
</li>
<li>Empirical Bayes methods estimate key components from data<br>
</li>
<li>Posterior probabilities are directly interpretable<br>
</li>
<li>The approach extends naturally to complex data structures</li>
</ol>
</section>
<section id="hierarchical-bayesian-models-the-problem-of-many-groups" class="slide level2">
<h2>Hierarchical Bayesian Models: The Problem of Many Groups</h2>
<p>In genomics, we often need to estimate many related parameters:</p>
<ul>
<li>Effects of thousands of variants<br>
</li>
<li>Expression levels across multiple tissues<br>
</li>
<li>Population-specific allele frequencies<br>
</li>
<li>Gene-specific parameters</li>
</ul>
<p>Independent analysis: Ignores similarities between groups<br>
Complete pooling: Ignores differences between groups</p>
<p><strong>Hierarchical models provide the best of both worlds.</strong></p>
</section>
<section id="the-power-of-partial-pooling" class="slide level2">
<h2>The Power of Partial Pooling</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-7-1.png" width="960" class="r-stretch"><p><strong>Hierarchical models</strong> adaptively share information across groups: stronger shrinkage for uncertain estimates, less for confident ones.</p>
</section>
<section id="mathematical-structure" class="slide level2">
<h2>Mathematical Structure</h2>
<p>A typical two-level hierarchical model:</p>
<p><span class="math display">\[
\begin{align}
\text{Data level: } &amp; y_j \sim N(\theta_j, \sigma_j^2) \\
\text{Group level: } &amp; \theta_j \sim N(\mu, \tau^2) \\
\text{Hyperprior level: } &amp; \mu \sim N(\mu_0, \sigma_0^2) \\
&amp; \tau^2 \sim \text{InvGamma}(a, b)
\end{align}
\]</span></p>
<p>Where:<br>
- <span class="math inline">\(y_j\)</span> is the observed data in group <span class="math inline">\(j\)</span><br>
- <span class="math inline">\(\theta_j\)</span> is the group-specific parameter<br>
- <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau^2\)</span> are shared across groups<br>
- Hyperpriors allow learning the amount of sharing</p>
</section>
<section id="the-beauty-of-shrinkage" class="slide level2">
<h2>The Beauty of Shrinkage</h2>
<p>The posterior mean for group <span class="math inline">\(j\)</span> is:</p>
<p><span class="math display">\[\hat{\theta}_j = \frac{\frac{1}{\sigma_j^2}y_j + \frac{1}{\tau^2}\mu}{\frac{1}{\sigma_j^2} + \frac{1}{\tau^2}} = w_j y_j + (1-w_j)\mu\]</span></p>
<p>Where <span class="math inline">\(w_j = \frac{\frac{1}{\sigma_j^2}}{\frac{1}{\sigma_j^2} + \frac{1}{\tau^2}}\)</span> is the weight given to the group’s data.</p>
<p>Key insights:<br>
- Uncertain estimates (<span class="math inline">\(\sigma_j^2\)</span> large) are shrunk more toward <span class="math inline">\(\mu\)</span><br>
- Heterogeneous groups (<span class="math inline">\(\tau^2\)</span> large) experience less shrinkage<br>
- The amount of sharing is learned from the data itself</p>
</section>
<section id="applications-in-genomics" class="slide level2">
<h2>Applications in Genomics</h2>
<p>Hierarchical models are ubiquitous in genomics:</p>
<ul>
<li><strong>eQTL Analysis</strong>: Sharing information across tissues<br>
</li>
<li><strong>Allele Frequencies</strong>: Borrowing strength across populations<br>
</li>
<li><strong>GWAS Meta-analysis</strong>: Combining studies while accounting for heterogeneity<br>
</li>
<li><strong>Differential Expression</strong>: Moderation of gene-specific variance estimates<br>
</li>
<li><strong>Fine Mapping</strong>: Sharing information across correlated variants</li>
</ul>
</section>
<section id="multivariate-extension-correlated-effects" class="slide level2">
<h2>Multivariate Extension: correlated effects</h2>
<p>For effects across multiple tissues or conditions:</p>
<p><span class="math display">\[
\begin{align}
\text{Data level: } &amp; \mathbf{y}_j \sim N(\boldsymbol{\theta}_j, \mathbf{S}_j) \\
\text{Group level: } &amp; \boldsymbol{\theta}_j \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma}) \\
\end{align}
\]</span></p>
<p>Now <span class="math inline">\(\boldsymbol{\Sigma}\)</span> captures correlations between conditions.</p>
<p>Examples: multivariate adaptive shrinkage (mash), multi-tissue eQTL analysis</p>
</section>
<section id="nonparametric-extensions" class="slide level2">
<h2>Nonparametric Extensions</h2>
<p>We can make hierarchical models more flexible:</p>
<ul>
<li><p><strong>Mixture of normals</strong> for the prior:<br>
<span class="math display">\[\theta_j \sim \sum_{k=1}^K \pi_k N(\mu_k, \tau_k^2)\]</span></p></li>
<li><p><strong>Dirichlet process</strong> priors for unknown grouping:<br>
<span class="math display">\[\theta_j \sim DP(\alpha, G_0)\]</span></p></li>
</ul>
<p>These extensions allow for:<br>
- Multi-modal effect distributions<br>
- Automatic clustering of similar effects<br>
- Robust estimation with outliers</p>
</section>
<section id="example-moderated-t-statistics" class="slide level2">
<h2>Example: Moderated t-statistics</h2>
<p>Limma’s moderated t-statistics for gene expression:</p>
<ol type="1">
<li><strong>Standard approach</strong>: Estimate variance for each gene independently<br>
</li>
<li><strong>Hierarchical approach</strong>: Model gene variances as coming from inverse-gamma prior<br>
</li>
<li><strong>Result</strong>: “Borrow information” across genes to improve variance estimates</li>
</ol>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-8-1.png" width="768" class="r-stretch"></section>
<section id="computational-approaches" class="slide level2">
<h2>Computational Approaches</h2>
<p>Hierarchical models can be fit using:</p>
<ol type="1">
<li><strong>Empirical Bayes</strong>: Estimate hyperparameters from data, then obtain group-specific posteriors
<ul>
<li>Fast and scalable<br>
</li>
<li>Used in tools like limma, edgeR, DESeq2</li>
</ul></li>
<li><strong>Full Bayes</strong>: Sample from full posterior using MCMC
<ul>
<li>Complete uncertainty quantification<br>
</li>
<li>Handles complex dependency structures<br>
</li>
<li>More computationally intensive</li>
</ul></li>
</ol>
</section>
<section id="practical-advantages" class="slide level2">
<h2>Practical Advantages</h2>
<ol type="1">
<li><strong>Improved accuracy</strong>: Lower mean squared error than independent estimates<br>
</li>
<li><strong>Uncertainty quantification</strong>: Full posterior distributions<br>
</li>
<li><strong>Efficiency</strong>: Makes better use of limited sample sizes<br>
</li>
<li><strong>Robustness</strong>: Less sensitive to outliers and high-noise groups<br>
</li>
<li><strong>Discovery power</strong>: Increases ability to detect real effects<br>
</li>
<li><strong>Interpretability</strong>: Reveals relationships between groups</li>
</ol>
</section>
<section id="key-takeaways-2" class="slide level2">
<h2>Key Takeaways</h2>
<ol type="1">
<li>Hierarchical models provide a principled approach to sharing information<br>
</li>
<li>Automatically balance between pooled and unpooled estimates<br>
</li>
<li>Particularly valuable for “small n, large p” settings common in genomics<br>
</li>
<li>Allow modeling of data-generating processes at multiple levels<br>
</li>
<li>Extend naturally to complex, multivariate settings<br>
</li>
<li>Provide computational flexibility from empirical to fully Bayesian approaches</li>
</ol>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="conjugate-priors-for-genetic-models" class="title-slide slide level1 center">
<h1>Conjugate Priors for Genetic Models</h1>

</section>
<section id="conjugate-priors-why-theyre-beautiful" class="slide level2">
<h2>Conjugate Priors: Why They’re Beautiful</h2>
<p><strong>Definition</strong>: A prior is conjugate when the posterior has the same distribution family</p>
<p><strong>Why they matter</strong>:<br>
1. <strong>Analytical solutions</strong> – no MCMC needed<br>
2. <strong>Interpretable parameters</strong> – prior as “pseudo-observations”<br>
3. <strong>Sequential updating</strong> – yesterday’s posterior is today’s prior<br>
4. <strong>Computational efficiency</strong> – critical for large genomic datasets</p>
</section>
<section id="figures" class="slide level2">
<h2>figures</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-9-1.png" width="960" class="r-stretch"><p>::::</p>
</section>
<section id="beta-binomial-perfect-for-allele-frequencies" class="slide level2">
<h2>Beta-Binomial: Perfect for Allele Frequencies</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Model</strong>:<br>
- <strong>Prior</strong>: <span class="math inline">\(\theta \sim \text{Beta}(\alpha, \beta)\)</span><br>
- <strong>Likelihood</strong>: <span class="math inline">\(X|\theta \sim \text{Binomial}(n, \theta)\)</span><br>
- <strong>Posterior</strong>: <span class="math inline">\(\theta|X \sim \text{Beta}(\alpha + X, \beta + n - X)\)</span></p>
<p><strong>Applications</strong>:<br>
- Allele frequency estimation<br>
- Case-control association<br>
- Heterozygosity estimation<br>
- Sequencing error rates</p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-10-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="dirichlet-multinomial-for-multiple-alleles" class="slide level2">
<h2>Dirichlet-Multinomial: For Multiple Alleles</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Model</strong>:<br>
- <strong>Prior</strong>: <span class="math inline">\(\vec{\theta} \sim \text{Dirichlet}(\vec{\alpha})\)</span><br>
- <strong>Likelihood</strong>: <span class="math inline">\(\vec{X}|\vec{\theta} \sim \text{Multinomial}(n, \vec{\theta})\)</span><br>
- <strong>Posterior</strong>: <span class="math inline">\(\vec{\theta}|\vec{X} \sim \text{Dirichlet}(\vec{\alpha} + \vec{X})\)</span></p>
<p><strong>Applications</strong>:<br>
- Multiple allele frequencies<br>
- Haplotype frequencies<br>
- Population admixture proportions<br>
- Taxonomic abundances</p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-11-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
<p>::::</p>
</section>
<section id="conjugate-normal-normal-model" class="slide level2">
<h2>Conjugate Normal-Normal Model</h2>
<ul>
<li>One of the most elegant and widely-used conjugate pairs in Bayesian statistics<br>
</li>
<li>Perfect for analyzing quantitative traits in genomics<br>
</li>
<li>Gives us a mathematical shortcut for updating beliefs</li>
</ul>
</section>
<section id="the-setup" class="slide level2">
<h2>The Setup</h2>
<p>When analyzing a continuous parameter <span class="math inline">\(\mu\)</span> (like an effect size):</p>
<ul>
<li><strong>Prior</strong>: <span class="math inline">\(\mu \sim \mathcal{N}(\mu_0, \sigma_0^2)\)</span><br>
</li>
<li><strong>Likelihood</strong>: <span class="math inline">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span> where <span class="math inline">\(\sigma^2\)</span> is known<br>
</li>
<li><strong>Question</strong>: What is <span class="math inline">\(p(\mu|X)\)</span>?</li>
</ul>
</section>
<section id="the-mathematical-magic" class="slide level2">
<h2>The Mathematical Magic</h2>
<p>The elegance is in the algebraic symmetry:</p>
<p><span class="math display">\[
\begin{align}
p(\mu|X) &amp;\propto p(X|\mu) \times p(\mu)\\
&amp;\propto \exp\left(-\frac{(X-\mu)^2}{2\sigma^2}\right) \times \exp\left(-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\right)
\end{align}
\]</span></p>
<p>Notice the beautiful symmetry: <span class="math inline">\((X-\mu)^2\)</span> in the likelihood and <span class="math inline">\((\mu-\mu_0)^2\)</span> in the prior.</p>
</section>
<section id="the-key-insight" class="slide level2">
<h2>The Key Insight</h2>
<p>When we expand these terms:</p>
<p><span class="math display">\[
\begin{align}
p(\mu|X) &amp;\propto \exp\left(-\frac{1}{2}\left[\frac{(X-\mu)^2}{\sigma^2} + \frac{(\mu-\mu_0)^2}{\sigma_0^2}\right]\right)\\
&amp;\propto \exp\left(-\frac{1}{2}\left[\frac{\mu^2 - 2\mu X + X^2}{\sigma^2} + \frac{\mu^2 - 2\mu\mu_0 + \mu_0^2}{\sigma_0^2}\right]\right)
\end{align}
\]</span></p>
<p>Collecting terms with <span class="math inline">\(\mu^2\)</span> and <span class="math inline">\(\mu\)</span>…</p>
</section>
<section id="the-posterior-formula" class="slide level2">
<h2>The Posterior Formula</h2>
<p>After completing the square, we get:</p>
<p><span class="math display">\[\mu|X \sim \mathcal{N}(\mu_n, \sigma_n^2)\]</span></p>
<p>Where:</p>
<p><span class="math display">\[\mu_n = \frac{\frac{\mu_0}{\sigma_0^2} + \frac{X}{\sigma^2}}{\frac{1}{\sigma_0^2} + \frac{1}{\sigma^2}} = \frac{\sigma^2\mu_0 + \sigma_0^2 X}{\sigma^2 + \sigma_0^2}\]</span></p>
<p><span class="math display">\[\frac{1}{\sigma_n^2} = \frac{1}{\sigma_0^2} + \frac{1}{\sigma^2}\]</span></p>
</section>
<section id="a-more-intuitive-view" class="slide level2">
<h2>A More Intuitive View</h2>
<p>The posterior mean is a <strong>precision-weighted average</strong> of the prior mean and the data:</p>
<p><span class="math display">\[\mu_n = w\mu_0 + (1-w)X\]</span></p>
<p>Where <span class="math inline">\(w = \frac{\sigma^2}{\sigma^2 + \sigma_0^2} = \frac{\text{data precision}}{\text{total precision}}\)</span></p>
<ul>
<li>When data is precise (small <span class="math inline">\(\sigma^2\)</span>): we trust the data more<br>
</li>
<li>When prior is precise (small <span class="math inline">\(\sigma_0^2\)</span>): we trust the prior more</li>
</ul>
</section>
<section id="multiple-observations" class="slide level2">
<h2>Multiple Observations</h2>
<p>With multiple observations <span class="math inline">\(X_1,...,X_n\)</span>, we get:</p>
<p><span class="math display">\[\mu|(X_1,...,X_n) \sim \mathcal{N}\left(\frac{\frac{\mu_0}{\sigma_0^2} + \frac{n\bar{X}}{\sigma^2}}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}, \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1}\right)\]</span></p>
<ul>
<li>The sample mean <span class="math inline">\(\bar{X}\)</span> is a sufficient statistic<br>
</li>
<li>More data increases precision linearly</li>
</ul>
</section>
<section id="genomics-application-eqtl-effect-sizes" class="slide level2">
<h2>Genomics Application: eQTL Effect Sizes</h2>
<p>In genomics, we might use this model for:</p>
<ul>
<li><strong>Prior</strong>: Historical effect sizes for similar variants<br>
</li>
<li><strong>Likelihood</strong>: Observed effect in current study<br>
</li>
<li><strong>Posterior</strong>: Updated estimate that balances prior knowledge and new data</li>
</ul>
<p>Example: Effect sizes for expression quantitative trait loci (eQTLs)</p>
</section>
<section id="the-power-of-conjugate-priors" class="slide level2">
<h2>The Power of Conjugate Priors</h2>
<p>Advantages of conjugate Normal-Normal:</p>
<ol type="1">
<li><strong>Analytical solutions</strong> – no MCMC required<br>
</li>
<li><strong>Computational efficiency</strong> – critical for genomic scale<br>
</li>
<li><strong>Interpretable updates</strong> – precision-weighted averages<br>
</li>
<li><strong>Sequential processing</strong> – can update one observation at a time</li>
</ol>
</section>
<section id="normal-normal-key-takeaways" class="slide level2">
<h2>Normal-Normal: Key Takeaways</h2>
<ol type="1">
<li>The posterior is also Normal – that’s conjugacy!<br>
</li>
<li>The posterior mean is a weighted average of prior mean and data<br>
</li>
<li>Weights are determined by relative precisions (1/variance)<br>
</li>
<li>The posterior precision is the sum of the prior and data precisions<br>
</li>
<li>This model provides the foundation for many advanced Bayesian genomic methods</li>
</ol>
</section>
<section id="extension-empirical-bayes-for-normal-means" class="slide level2">
<h2>Extension: Empirical Bayes for Normal Means</h2>
<ul>
<li>When we don’t have a specific prior, we can <strong>estimate it from the data</strong><br>
</li>
<li>This approach, known as <strong>Empirical Bayes</strong>, is extremely powerful for genomics<br>
</li>
<li>Applications include: multiple testing, sparse signal detection, and effect size estimation</li>
</ul>
</section>
<section id="methods-using-normal-normal-conjugacy" class="slide level2">
<h2>Methods Using Normal-Normal Conjugacy</h2>
<ul>
<li><strong>Adaptive Shrinkage (ash)</strong>: Uses a mixture of normals as the prior<br>
</li>
<li><strong>Multivariate Adaptive Shrinkage (mash)</strong>: Extends to correlated effects across conditions<br>
</li>
<li><strong>False Discovery Rate Control</strong>: Through local false discovery rates<br>
</li>
<li><strong>Hierarchical Models</strong>: Building multi-level models with partially pooled estimates</li>
</ul>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="mixture-models-for-complex-data-what-are-mixture-models" class="title-slide slide level1 center">
<h1>Mixture Models for Complex Data: What Are Mixture Models?</h1>
<p>Mixture models are probabilistic models that represent the presence of subpopulations within an overall population:</p>
<ul>
<li><strong>Used when data come from multiple underlying processes</strong><br>
</li>
<li><strong>Represent heterogeneous populations as mixtures of simpler distributions</strong><br>
</li>
<li><strong>Allow clustering without hard assignments</strong><br>
</li>
<li><strong>Incorporate uncertainty in group membership</strong></li>
</ul>
</section>
<section id="mixture-model-mathematical-formulation" class="slide level2">
<h2>Mixture Model: Mathematical Formulation</h2>
<p>A mixture model combines multiple distributions to model complex data:</p>
<p><span class="math display">\[p(x) = \sum_{k=1}^K \pi_k f_k(x|\theta_k)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(p(x)\)</span> is the overall probability density<br>
</li>
<li><span class="math inline">\(K\)</span> is the number of components (subpopulations)<br>
</li>
<li><span class="math inline">\(\pi_k\)</span> are the mixing weights (<span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span>)<br>
</li>
<li><span class="math inline">\(f_k(x|\theta_k)\)</span> are the component densities with parameters <span class="math inline">\(\theta_k\)</span></li>
</ul>
</section>
<section id="a-closer-look-at-the-components" class="slide level2">
<h2>A Closer Look at the Components</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Key components</strong>:</p>
<ol type="1">
<li><strong>Component distributions</strong> <span class="math inline">\(f_k(x|\theta_k)\)</span>
<ul>
<li>Each represents a subpopulation<br>
</li>
<li>Can be any distribution family<br>
</li>
<li>Common choices: Gaussian, multinomial, beta</li>
</ul></li>
<li><strong>Mixing weights</strong> <span class="math inline">\(\pi_k\)</span>
<ul>
<li>Proportion of data from each component<br>
</li>
<li>Must sum to 1: <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span><br>
</li>
<li>Reflect prior probabilities of group membership</li>
</ul></li>
<li><strong>Latent variables</strong> <span class="math inline">\(z_i\)</span>
<ul>
<li>Unobserved component membership<br>
</li>
<li><span class="math inline">\(z_i = k\)</span> means data point <span class="math inline">\(i\)</span> came from component <span class="math inline">\(k\)</span></li>
</ul></li>
</ol>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-12-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="likelihood-function-for-mixture-models" class="slide level2">
<h2>Likelihood Function for Mixture Models</h2>
<p>The likelihood of a mixture model for <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(x_1, \ldots, x_n\)</span> is:</p>
<p><span class="math display">\[L(\theta, \pi | x_1, \ldots, x_n) = \prod_{i=1}^n p(x_i) = \prod_{i=1}^n \sum_{k=1}^K \pi_k f_k(x_i|\theta_k)\]</span></p>
<p><strong>Challenge</strong>: The sum inside the product makes this difficult to optimize directly</p>
<p><strong>Solution</strong>: Introduce latent variables <span class="math inline">\(z_i\)</span> and use the EM algorithm</p>
</section>
<section id="the-em-algorithm-in-detail" class="slide level2">
<h2>The EM Algorithm in Detail</h2>
<p>The Expectation-Maximization (EM) algorithm is an iterative method for finding maximum likelihood estimates:</p>
<p><strong>E-step</strong>: Calculate “responsibilities” – the posterior probability that data point <span class="math inline">\(i\)</span> belongs to component <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[\gamma_{ik} = P(z_i = k | x_i, \theta) = \frac{\pi_k f_k(x_i|\theta_k)}{\sum_{j=1}^K \pi_j f_j(x_i|\theta_j)}\]</span></p>
<p><strong>M-step</strong>: Update parameters using weighted maximum likelihood:</p>
<p><span class="math display">\[\pi_k^{new} = \frac{1}{n}\sum_{i=1}^n \gamma_{ik}\]</span><br>
<span class="math display">\[\theta_k^{new} = \arg\max_{\theta_k} \sum_{i=1}^n \gamma_{ik} \log f_k(x_i|\theta_k)\]</span></p>
</section>
<section id="em-algorithm-step-by-step-example" class="slide level2">
<h2>EM Algorithm: Step-by-Step Example</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-13-1.png" width="960" class="r-stretch"></section>
<section id="worked-example-em-algorithm-step-by-step" class="slide level2">
<h2>Worked Example: EM Algorithm Step-by-Step</h2>
<p>Let’s walk through each step of the EM algorithm for a mixture of two Gaussians:</p>
<ol type="1">
<li><strong>Initialize parameters</strong>:
<ul>
<li>Set initial mixing weights: <span class="math inline">\(\pi_1 = \pi_2 = 0.5\)</span><br>
</li>
<li>Set initial component means: <span class="math inline">\(\mu_1 = -1, \mu_2 = 1\)</span><br>
</li>
<li>Set initial component standard deviations: <span class="math inline">\(\sigma_1 = \sigma_2 = 1\)</span></li>
</ul></li>
<li><strong>E-step</strong>: For each data point <span class="math inline">\(x_i\)</span>, calculate the responsibility of each component:
<ul>
<li><span class="math inline">\(\gamma_{i1} = \frac{\pi_1 N(x_i|\mu_1,\sigma_1^2)}{\pi_1 N(x_i|\mu_1,\sigma_1^2) + \pi_2 N(x_i|\mu_2,\sigma_2^2)}\)</span><br>
</li>
<li><span class="math inline">\(\gamma_{i2} = 1 - \gamma_{i1}\)</span></li>
</ul></li>
<li><strong>M-step</strong>: Update the parameters using the responsibilities:
<ul>
<li><span class="math inline">\(\pi_1^{new} = \frac{1}{n}\sum_{i=1}^n \gamma_{i1}\)</span> (similarly for <span class="math inline">\(\pi_2^{new}\)</span>)<br>
</li>
<li><span class="math inline">\(\mu_1^{new} = \frac{\sum_{i=1}^n \gamma_{i1}x_i}{\sum_{i=1}^n \gamma_{i1}}\)</span> (similarly for <span class="math inline">\(\mu_2^{new}\)</span>)<br>
</li>
<li><span class="math inline">\((\sigma_1^{new})^2 = \frac{\sum_{i=1}^n \gamma_{i1}(x_i-\mu_1^{new})^2}{\sum_{i=1}^n \gamma_{i1}}\)</span> (similarly for <span class="math inline">\(\sigma_2^{new}\)</span>)</li>
</ul></li>
<li><strong>Repeat</strong> until convergence (parameters stop changing significantly)</li>
</ol>
</section>
<section class="slide level2">

<p>The EM Algorithm: Mathematical Intuition</p>
<p>Key insight: We’re solving a chicken-and-egg problem<br>
If we knew component assignments, parameter estimation would be easy<br>
If we knew parameters, component assignments would be easy<br>
EM iteratively solves both by using expected assignments</p>
<p>E-step (Expectation): Calculate expected component memberships<br>
<span class="math display">\[\gamma_{ik} = P(z_i = k | x_i, \theta^{(t)}) = \frac{\pi_k^{(t)} f_k(x_i|\theta_k^{(t)})}{\sum_{j=1}^K \pi_j^{(t)} f_j(x_i|\theta_j^{(t)})}\]</span><br>
Intuition: “How likely is individual i to belong to population k, given our current parameter estimates?”</p>
<p>M-step (Maximization): Update parameters using weighted averages<br>
For mixing weights:<br>
<span class="math display">\[\pi_k^{(t+1)} = \frac{1}{n}\sum_{i=1}^n \gamma_{ik}\]</span><br>
Intuition: “The new population frequency is the average membership across all individuals”</p>
<p>For component means (Gaussian case):<br>
<span class="math display">\[\mu_k^{(t+1)} = \frac{\sum_{i=1}^n \gamma_{ik}x_i}{\sum_{i=1}^n \gamma_{ik}}\]</span><br>
Intuition: “The new population mean is a weighted average where individuals are weighted by their probability of belonging to this population”</p>
</section>
<section id="bayesian-mixture-models" class="slide level2">
<h2>Bayesian Mixture Models</h2>
<p>Bayesian mixture models add priors to the parameters:</p>
<p><span class="math inline">\(p(\theta, \pi | x_1, \ldots, x_n) \propto p(x_1, \ldots, x_n | \theta, \pi) \times p(\theta, \pi)\)</span></p>
<p>Common prior choices:<br>
- <span class="math inline">\(\pi \sim \text{Dirichlet}(\alpha_1, \ldots, \alpha_K)\)</span> for mixing weights<br>
- Component-specific priors for <span class="math inline">\(\theta_k\)</span> (e.g., Normal-Inverse-Gamma for Gaussian components)</p>
<p><strong>Advantages</strong>:<br>
- Handle uncertainty in the number of components (K)<br>
- Avoid singularities and improve stability<br>
- Allow for informed priors from previous studies<br>
- Provide full posterior distribution rather than point estimates (in full MCMC implementation, but SLOW)</p>
</section>
<section id="mixture-model-applications-in-genomics" class="slide level2">
<h2>Mixture Model Applications in Genomics</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>1. Population Structure</strong><br>
- Components = ancestral populations<br>
- Individual genotypes = admixtures of populations<br>
- Example: STRUCTURE, ADMIXTURE software<br>
- Used for: demographic history, association studies, conservation</p>
<p><strong>2. Genetic effect estimation</strong><br>
- (e.g., adaptive shrinkage methods like ash/mash for multiple conditions)</p>
<p><strong>3. Gene Expression Clustering</strong><br>
- Components = cell types/states<br>
- Expression patterns = signatures of cell types<br>
- Example: Single-cell RNA-seq clustering<br>
- Used for: cell type identification, developmental trajectories</p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-14-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="the-structure-model-in-detail" class="slide level2">
<h2>The STRUCTURE Model in Detail</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>STRUCTURE</strong>: A Bayesian mixture model for population genetics</p>
<p><strong>Key components</strong>:<br>
- Each individual = mixture of <span class="math inline">\(K\)</span> ancestral populations<br>
- Each population = distinct allele frequencies<br>
- Goal: Infer ancestry proportions &amp; population frequencies</p>
<p><strong>Bayesian formulation</strong>:<br>
- <strong>Prior</strong>: <span class="math inline">\(q_{ik} \sim \text{Dirichlet}(\alpha)\)</span> (ancestry proportions)<br>
- <strong>Prior</strong>: <span class="math inline">\(f_{kj} \sim \text{Beta}(\lambda)\)</span> (allele frequencies)<br>
- <strong>Likelihood</strong>: <span class="math inline">\(P(X_{ij} | q_i, f_j)\)</span> (genotype probabilities)</p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-15-1.png" width="576"></p>
</figure>
</div>
</div>
</div>
</div></div>
<h3 id="effect-size-mixtures-in-gwas"><strong>Effect Size Mixtures in GWAS</strong></h3>
<p><strong>Problem</strong>: Most variants have no effect, but some do</p>
<p><strong>Solutions</strong>:<br>
- <strong>Spike-and-slab prior</strong>: Mixture of point mass at zero and continuous distribution<br>
- <strong>Scale mixture</strong>: Mixture of normal distributions with different variances<br>
- <strong>Bayesian variable selection</strong>: Latent indicator for whether variant is causal</p>
<p><strong>Benefits</strong>:<br>
- Controls false discovery rate<br>
- Improves power to detect true associations<br>
- Provides interpretable posterior probabilities<br>
- Naturally handles multiple testing</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><em>(Illustration of spike-and-slab concept could go here in text form)</em></p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-16-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="multivariate-normal-mixtures-the-mash-approach" class="slide level2">
<h2>Multivariate Normal Mixtures: The mash Approach</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Key idea</strong>: Share information across related conditions</p>
<p><strong>Mathematical model</strong>:<br>
- <span class="math inline">\(\hat{\beta}_j \sim N(\beta_j, S_j)\)</span> (observed effects)<br>
- <span class="math inline">\(\beta_j \sim \sum_{k=1}^K \pi_k N(0, U_k)\)</span> (true effects)</p>
<hr>
<p><strong>Covariance matrices <span class="math inline">\(U_k\)</span> capture patterns</strong>:<br>
- Shared effects across all conditions<br>
- Condition-specific effects<br>
- Structured correlation patterns<br>
- Data-driven patterns</p>
<p><strong>Benefits</strong>:<br>
- Improves effect estimation through sharing<br>
- Discovers patterns of effect heterogeneity<br>
- Controls false discovery rate<br>
- Provides interpretable multivariate posteriors</p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-17-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
<p>::::</p>
</section>
<section id="common-challenges-with-mixture-models" class="slide level2">
<h2>Common Challenges with Mixture Models</h2>
<ol type="1">
<li><strong>Identifiability issues</strong>: Component labels can be permuted (label switching)<br>
</li>
<li><strong>Local optima</strong>: EM algorithm may converge to suboptimal solutions<br>
</li>
<li><strong>Initialization sensitivity</strong>: Results depend on starting values<br>
</li>
<li><strong>Singularities</strong>: Component variance can collapse to zero<br>
</li>
<li><strong>Determining number of components</strong>: No single best method</li>
</ol>
<p><strong>Solutions</strong>:<br>
- Run algorithm multiple times with different initializations<br>
- Regularization via priors (Bayesian approach)<br>
- Deterministic annealing or other modified EM variants<br>
- Model averaging across different K values<br>
- Cross-validation for model selection</p>
</section>
<section id="bayesian-clinical-trials-the-mathematical-foundation" class="slide level2">
<h2>Bayesian Clinical Trials: The Mathematical Foundation</h2>
<p>The core idea of Bayesian clinical trials is to use probability distributions to quantify uncertainty about parameters:</p>
<p><strong>Traditional (frequentist) approach</strong>:<br>
- Fixed design with predetermined sample size<br>
- Binary decision making (significant or not)<br>
- No formal incorporation of prior information</p>
<p><strong>Bayesian approach</strong> offers several advantages:<br>
- <strong>Sequential analysis</strong>: Update posterior as data accumulates<br>
- <strong>Adaptive design</strong>: Sample size, treatment allocation can change mid-trial<br>
- <strong>Probabilistic conclusions</strong>: Direct statements about treatment effects<br>
- <strong>Prior incorporation</strong>: Previous trial results, biological knowledge</p>
<p>The mathematical framework:<br>
- <strong>Prior distribution</strong>: <span class="math inline">\(p(\theta)\)</span> – Initial beliefs about parameter <span class="math inline">\(\theta\)</span><br>
- <strong>Likelihood</strong>: <span class="math inline">\(p(X|\theta)\)</span> – Probability of data given parameter<br>
- <strong>Posterior distribution</strong>: <span class="math inline">\(p(\theta|X) \propto p(X|\theta)p(\theta)\)</span> – Updated beliefs</p>
</section>
<section id="bayesian-model-averaging" class="slide level2">
<h2>Bayesian Model Averaging</h2>
</section>
<section id="the-model-selection-problem" class="slide level2">
<h2>The Model Selection Problem</h2>
<p>Traditional approach:<br>
1. Define candidate models<br>
2. Select “best” model using AIC, BIC, etc.<br>
3. Make inference based on single chosen model</p>
<p><strong>The problem</strong>: Ignores model uncertainty!</p>
<ul>
<li>What if multiple models fit equally well?<br>
</li>
<li>What if different models make different predictions?<br>
</li>
<li>What if the “best” model changes with small data changes?</li>
</ul>
</section>
<section id="bayesian-model-averaging-1" class="slide level2">
<h2>Bayesian Model Averaging</h2>
<p>Rather than choosing a single model, we can average over all models:</p>
<ol type="1">
<li>Assign prior probabilities to models: <span class="math inline">\(P(M_k)\)</span><br>
</li>
<li>Compute posterior model probabilities: <span class="math inline">\(P(M_k|data)\)</span><br>
</li>
<li>Weight predictions by these probabilities</li>
</ol>
<p><span class="math display">\[P(\theta|data) = \sum_{k=1}^K P(\theta|M_k, data) \times P(M_k|data)\]</span></p>
<p>This accounts for uncertainty in both parameters AND model selection.</p>
</section>
<section id="the-mathematical-framework" class="slide level2">
<h2>The Mathematical Framework</h2>
<p>For K candidate models <span class="math inline">\(M_1, M_2, \ldots, M_K\)</span>:</p>
<ul>
<li><p>Prior model probabilities: <span class="math inline">\(P(M_k)\)</span><br>
</p></li>
<li><p>Posterior model probabilities:<br>
<span class="math display">\[P(M_k|data) = \frac{P(data|M_k)P(M_k)}{\sum_{j=1}^K P(data|M_j)P(M_j)}\]</span></p></li>
<li><p>Marginal likelihood of model <span class="math inline">\(M_k\)</span>:<br>
<span class="math display">\[P(data|M_k) = \int P(data|\theta_k, M_k)P(\theta_k|M_k)d\theta_k\]</span></p></li>
</ul>
</section>
<section id="visualizing-bma" class="slide level2">
<h2>Visualizing BMA</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-18-1.png" width="768" class="r-stretch"><p>BMA combines predictions from multiple models, weighted by their posterior probabilities, leading to more robust inference.</p>
</section>
<section id="bma-for-fine-mapping" class="slide level2">
<h2>BMA for Fine-Mapping</h2>
<p>In genetic fine-mapping, we don’t know which variants are causal:</p>
<ul>
<li>Many possible causal configurations (models)<br>
</li>
<li>BMA provides a coherent approach to handle this uncertainty</li>
</ul>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-19-1.png" width="768" class="r-stretch"><p>Posterior Inclusion Probabilities (PIPs) from BMA provide a robust way to prioritize likely causal variants.</p>
</section>
<section id="model-averaging-in-genomics" class="slide level2">
<h2>Model Averaging in Genomics</h2>
<p>Common applications:</p>
<ol type="1">
<li><strong>Gene-Environment Interactions</strong>: Uncertainty in interaction model forms<br>
</li>
<li><strong>Gene-Based Association Tests</strong>: Multiple ways to combine variants<br>
</li>
<li><strong>Fine-Mapping</strong>: Uncertainty in causal variant configurations<br>
</li>
<li><strong>Gene Set Analysis</strong>: Uncertainty in pathway definitions<br>
</li>
<li><strong>eQTL Mapping</strong>: Uncertainty in the appropriate expression model<br>
</li>
<li><strong>Polygenic Risk Scores</strong>: Uncertainty in variant selection</li>
</ol>
</section>
<section id="bma-vs.-model-selection" class="slide level2">
<h2>BMA vs.&nbsp;Model Selection</h2>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-20-1.png" width="768" class="r-stretch"><p>Model averaging often outperforms selection of a single “best” model, especially for prediction.</p>
</section>
<section id="computational-approaches-1" class="slide level2">
<h2>Computational Approaches</h2>
<ol type="1">
<li><strong>Exact BMA</strong>: Enumerate all models (feasible for small model spaces)<br>
</li>
<li><strong>Markov Chain Monte Carlo Model Composition (MC³)</strong>:
<ul>
<li>Sample models according to their posterior probabilities<br>
</li>
<li>Efficient exploration of large model spaces<br>
</li>
</ul></li>
<li><strong>Approximations</strong>:
<ul>
<li>BIC approximation to marginal likelihoods<br>
</li>
<li>Laplace approximation for integration<br>
</li>
<li>Variational methods for high-dimensional spaces</li>
</ul></li>
</ol>
</section>
<section id="practical-considerations" class="slide level2">
<h2>Practical Considerations</h2>
<ol type="1">
<li><strong>Prior model probabilities</strong>:
<ul>
<li>Equal priors for all models?<br>
</li>
<li>Complexity penalties?<br>
</li>
<li>Biologically informed priors?</li>
</ul></li>
<li><strong>Model space definition</strong>:
<ul>
<li>Which models to include?<br>
</li>
<li>How to handle model correlation?</li>
</ul></li>
<li><strong>Inference targets</strong>:
<ul>
<li>Parameter estimates vs.&nbsp;predictions<br>
</li>
<li>Variable importance measures<br>
</li>
<li>Model-averaged confidence intervals</li>
</ul></li>
</ol>
</section>
<section id="key-advantages" class="slide level2">
<h2>Key Advantages</h2>
<ol type="1">
<li><strong>Honest uncertainty quantification</strong>: Accounts for model uncertainty<br>
</li>
<li><strong>Improved prediction accuracy</strong>: Particularly for out-of-sample prediction<br>
</li>
<li><strong>Robust inference</strong>: Less sensitive to model misspecification<br>
</li>
<li><strong>Variable importance measures</strong>: Posterior inclusion probabilities<br>
</li>
<li><strong>Decision-theoretic optimality</strong>: Under certain loss functions<br>
</li>
<li><strong>Natural Occam’s razor</strong>: Automatically balances fit and complexity</li>
</ol>
</section>
<section id="key-takeaways-3" class="slide level2">
<h2>Key Takeaways</h2>
<ol type="1">
<li>Model selection ignores uncertainty in the model itself<br>
</li>
<li>Bayesian model averaging provides a principled framework for incorporating model uncertainty<br>
</li>
<li>Posterior model probabilities balance fit and complexity<br>
</li>
<li>BMA often produces more robust parameter estimates and predictions<br>
</li>
<li>Computational methods exist for handling large model spaces<br>
</li>
<li>In genomics, BMA is particularly valuable for fine-mapping and complex trait analysis</li>
</ol>
</section>
<section id="bayesian-clinical-trials-direct-probability-statements" class="slide level2">
<h2>Bayesian Clinical Trials: Direct Probability Statements</h2>
<p>Key insight: Even with flat priors, Bayesian methods give you direct probability statements about parameters</p>
<p>Example: For a treatment effect θ with flat prior and normal likelihood:<br>
<span class="math inline">\(p(\theta|data) \propto \text{Normal}(\hat{\theta}, SE^2)\)</span></p>
<p>Direct probability statements:<br>
<span class="math inline">\(P(\theta &gt; 0 | data) = 0.97\)</span> → “97% probability the treatment is beneficial”<br>
<span class="math inline">\(P(\theta &gt; 0.2 | data) = 0.63\)</span> → “63% probability the effect exceeds clinically meaningful threshold”<br>
<span class="math inline">\(P(0.1 &lt; \theta &lt; 0.3 | data) = 0.75\)</span> → “75% probability effect is in the moderate range”</p>
<p>Contrast with frequentist:<br>
“p &lt; 0.05” → Reject null hypothesis<br>
“95% CI: [0.1, 0.4]” → In repeated sampling, 95% of intervals would contain true value</p>
<p><strong>Decision rules</strong> can be based on:<br>
- Posterior probability: <span class="math inline">\(P(\theta &gt; \delta|X) &gt; \gamma\)</span><br>
- Predictive probability of success<br>
- Expected utility/loss considerations</p>
</section>
<section id="bayesian-meta-analysis-the-mathematical-framework" class="slide level2">
<h2>Bayesian Meta-Analysis: The Mathematical Framework</h2>
<p><strong>Problem</strong>: Combine evidence across heterogeneous studies</p>
<p><strong>Model formulation</strong>:<br>
- Let <span class="math inline">\(y_i\)</span> be the observed effect in study <span class="math inline">\(i\)</span><br>
- Let <span class="math inline">\(\sigma_i^2\)</span> be the variance (often known from standard error)<br>
- Let <span class="math inline">\(\theta_i\)</span> be the true effect in study <span class="math inline">\(i\)</span></p>
<p><strong>Hierarchical model</strong>:<br>
<span class="math inline">\(y_i | \theta_i, \sigma_i^2 \sim N(\theta_i, \sigma_i^2)\)</span><br>
<span class="math inline">\(\theta_i | \mu, \tau^2 \sim N(\mu, \tau^2)\)</span><br>
<span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span><br>
<span class="math inline">\(\tau^2 \sim \text{InvGamma}(a, b)\)</span></p>
<p>Where:<br>
- <span class="math inline">\(\mu\)</span> is the overall mean effect<br>
- <span class="math inline">\(\tau^2\)</span> is the between-study heterogeneity<br>
- <span class="math inline">\(\mu_0, \sigma_0^2, a, b\)</span> are hyperparameters</p>
</section>
<section class="slide level2">

<p><strong>Key advantages</strong>:<br>
- Naturally accounts for heterogeneity<br>
- Uncertainty in all parameters<br>
- Shrinkage of extreme estimates toward the mean<br>
- Robust to outliers with appropriate priors</p>
</section>
<section id="bayesian-meta-analysis-visualization" class="slide level2">
<h2>Bayesian Meta-Analysis: Visualization</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Traditional approaches</strong>:<br>
- Fixed effects (assumes same effect size)<br>
- Random effects (allows variation in effect size)<br>
- Often sensitive to outliers</p>
<p><strong>Bayesian advantages</strong>:<br>
- Full posterior distribution for all parameters<br>
- Can incorporate informative priors<br>
- Naturally handles small studies (shrinkage)<br>
- Can model outliers explicitly<br>
- Direct probability statements about effects</p>
<p><strong>Example interpretation</strong>:<br>
- Posterior probability of benefit = 98%<br>
- 95% credible interval for effect size: [0.1, 0.5]<br>
- 90% probability heterogeneity is moderate to high</p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-21-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="bayesian-adaptive-trial-designs" class="slide level2">
<h2>Bayesian Adaptive Trial Designs</h2>
<p>A powerful framework that allows studies to <strong>evolve</strong> based on accumulating evidence</p>
<ul>
<li>Combines statistical efficiency with ethical considerations<br>
</li>
<li>Particularly powerful for precision medicine approaches<br>
</li>
<li>Provides direct probability statements about treatment effects<br>
</li>
<li>Extends naturally to genetic and genomic applications</li>
</ul>
</section>
<section id="key-idea-learning-while-doing" class="slide level2">
<h2>Key Idea: Learning While Doing</h2>
<p>Traditional (frequentist) trials:<br>
- Fixed design determined at the outset<br>
- Sample size calculations based on frequentist power<br>
- Analysis only after all data collection is complete</p>
<p>Bayesian adaptive trials:<br>
- Update knowledge continuously as data accumulates<br>
- Modify aspects of the trial in response to emerging data<br>
- Make probability-based decisions at interim points</p>
</section>
<section id="the-bayesian-framework" class="slide level2">
<h2>The Bayesian Framework</h2>
<p>For a treatment effect <span class="math inline">\(\theta\)</span>:</p>
<ul>
<li><strong>Prior distribution</strong>: <span class="math inline">\(p(\theta)\)</span> – initial beliefs about the effect<br>
</li>
<li><strong>Likelihood</strong>: <span class="math inline">\(p(data|\theta)\)</span> – probability of observing the data<br>
</li>
<li><strong>Posterior distribution</strong>: <span class="math inline">\(p(\theta|data) \propto p(data|\theta)p(\theta)\)</span></li>
</ul>
<p>As data accumulates, the posterior provides a complete picture of current knowledge.</p>
</section>
<section id="what-can-be-adapted" class="slide level2">
<h2>What Can Be Adapted?</h2>
<ol type="1">
<li><strong>Sample size</strong> – stop early for efficacy, futility, or continue for more precision<br>
</li>
<li><strong>Treatment allocation</strong> – assign more patients to promising treatments<br>
</li>
<li><strong>Eligibility criteria</strong> – focus on responsive subgroups<br>
</li>
<li><strong>Dosing</strong> – concentrate on optimal doses<br>
</li>
<li><strong>Endpoints</strong> – prioritize more informative outcomes</li>
</ol>
<p>All based on predefined decision rules using posterior probabilities.</p>
</section>
<section id="example-adaptive-stopping-rules" class="slide level2">
<h2>Example: Adaptive Stopping Rules</h2>
<p>A trial may stop early if:</p>
<ul>
<li><span class="math inline">\(P(\theta &gt; 0 | data) &gt; 0.99\)</span> (efficacy – treatment works)<br>
</li>
<li><span class="math inline">\(P(\theta &lt; \delta | data) &gt; 0.95\)</span> (futility – treatment doesn’t meet threshold)<br>
</li>
<li><span class="math inline">\(P(\delta_1 &lt; \theta &lt; \delta_2 | data) &gt; 0.90\)</span> (precision – effect is known with sufficient certainty)</li>
</ul>
<p>These are direct probability statements about the parameter of interest.</p>
</section>
<section id="example-adaptive-randomization" class="slide level2">
<h2>Example: Adaptive Randomization</h2>
<p>As evidence accumulates, allocation probabilities shift:</p>
<ul>
<li>Initial: Equal probability to all treatment arms<br>
</li>
<li>Adaptive: Higher probability to treatments with better outcomes</li>
</ul>
<p><span class="math display">\[P(\text{assign to treatment } j) \propto P(\text{treatment } j \text{ is best}|\text{current data})\]</span></p>
<p>Balances learning (exploration) with treating patients optimally (exploitation). ## Visualizing Adaptive Trials</p>

<img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-22-1.png" width="960" class="r-stretch"><p>Early stopping means fewer patients needed and faster results.</p>
</section>
<section id="adaptive-enrichment-genetics-applications" class="slide level2">
<h2>Adaptive Enrichment: Genetics Applications</h2>
<p>For trials involving genetic subgroups:</p>
<ol type="1">
<li>Start with broad eligibility<br>
</li>
<li>As data accumulates, identify responsive genetic profiles<br>
</li>
<li>Adapt enrollment to focus on promising subgroups<br>
</li>
<li>Potentially obtain approval for genetically defined population</li>
</ol>
<p>Direct extension of precision medicine principles.</p>
</section>
<section id="platform-trials-multiple-treatments" class="slide level2">
<h2>Platform Trials: Multiple Treatments</h2>
<p>Modern adaptive platform trials can:<br>
- Test multiple treatments simultaneously<br>
- Add new treatment arms as they become available<br>
- Remove ineffective treatments early<br>
- Share control groups across treatments</p>
<p>Examples: I-SPY2 (breast cancer), GBM AGILE (glioblastoma), RECOVERY (COVID-19)</p>
</section>
<section id="benefits-of-bayesian-adaptive-designs" class="slide level2">
<h2>Benefits of Bayesian Adaptive Designs</h2>
<ol type="1">
<li><strong>Ethical advantages</strong> – fewer patients on ineffective treatments<br>
</li>
<li><strong>Efficiency</strong> – often require fewer total patients<br>
</li>
<li><strong>Flexibility</strong> – can adapt to unexpected findings<br>
</li>
<li><strong>Subgroup identification</strong> – find responsive populations<br>
</li>
<li><strong>Interpretability</strong> – direct probability statements<br>
</li>
<li><strong>Speed</strong> – potential for earlier conclusions</li>
</ol>
</section>
<section id="challenges-to-consider" class="slide level2">
<h2>Challenges to Consider</h2>
<ol type="1">
<li><strong>Operational complexity</strong> – requires robust infrastructure<br>
</li>
<li><strong>Statistical expertise</strong> – more sophisticated than traditional designs<br>
</li>
<li><strong>Regulatory considerations</strong> – though acceptance is growing<br>
</li>
<li><strong>Prior specification</strong> – sensitivity analyses important<br>
</li>
<li><strong>Simulation required</strong> – operating characteristics must be understood</li>
</ol>
</section>
<section id="from-clinical-trials-to-genomics" class="slide level2">
<h2>From Clinical Trials to Genomics</h2>
<p>The same principles apply to genomics research:</p>
<ul>
<li><strong>Adaptive sampling</strong> in sequencing studies<br>
</li>
<li><strong>Sequential testing</strong> with genomic data<br>
</li>
<li><strong>Multi-stage designs</strong> for genetic discovery<br>
</li>
<li><strong>Early stopping</strong> for genomic biomarker validation<br>
</li>
<li><strong>Subgroup enrichment</strong> based on genetic profiles</li>
</ul>
</section>
<section id="key-takeaways-4" class="slide level2">
<h2>Key Takeaways</h2>
<ol type="1">
<li>Bayesian adaptive designs provide a formal framework for learning while doing<br>
</li>
<li>Direct probability statements make interpretation straightforward<br>
</li>
<li>Adapting aspects of the study can improve efficiency and ethics<br>
</li>
<li>The approach is particularly valuable for precision medicine<br>
</li>
<li>Methods extend naturally to genomics applications</li>
</ol>
</section>
<section id="the-susie-model-for-fine-mapping-mathematical-framework" class="slide level2">
<h2>The SuSiE Model for Fine-Mapping: Mathematical Framework</h2>
<p><strong>Problem</strong>: Identify causal variants from correlated SNPs in GWAS loci</p>
<p><strong>SuSiE</strong> (Sum of Single Effects) model:</p>
<p><span class="math inline">\(\mathbf{y} = \sum_{l=1}^L \mathbf{X} \boldsymbol{\beta}_l + \boldsymbol{\epsilon}\)</span></p>
<p>Where:<br>
- <span class="math inline">\(\mathbf{y}\)</span> is the vector of phenotype values (<span class="math inline">\(n\)</span> individuals)<br>
- <span class="math inline">\(\mathbf{X}\)</span> is the genotype matrix (<span class="math inline">\(n\)</span> individuals × <span class="math inline">\(p\)</span> SNPs)<br>
- <span class="math inline">\(L\)</span> is the number of causal effects (typically small, e.g., 1-10)<br>
- Each <span class="math inline">\(\boldsymbol{\beta}_l\)</span> has exactly one non-zero element (single-effect)<br>
- <span class="math inline">\(\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})\)</span> is the error term</p>
</section>
<section class="slide level2">

<p><strong>Bayesian formulation</strong>:<br>
- Each <span class="math inline">\(\boldsymbol{\beta}_l\)</span> follows a “spike-and-slab” prior:<br>
- For each effect <span class="math inline">\(l\)</span>, exactly one SNP <span class="math inline">\(j\)</span> is causal<br>
- Prior probability SNP <span class="math inline">\(j\)</span> is causal: <span class="math inline">\(\pi_j\)</span> (can be uniform or informed)<br>
- Effect size distribution: <span class="math inline">\(\beta_{lj} \sim N(0, \sigma^2_l)\)</span> if causal, 0 otherwise</p>
<p><strong>Key outputs</strong>:<br>
- Posterior Inclusion Probability (PIP) for each SNP<br>
- 95% credible sets for each effect<br>
- Posterior distributions of effect sizes</p>
</section>
<section id="susie-fine-mapping-visualization" class="slide level2">
<h2>SuSiE Fine-Mapping: Visualization</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Problem</strong>: Identify causal variants in a GWAS locus</p>
<p><strong>Traditional approach</strong>:<br>
- Select all SNPs with p &lt; threshold<br>
- Often includes many correlated SNPs<br>
- No clear separation of multiple signals</p>
<p><strong>SuSiE advantages</strong>:<br>
- Accounts for correlation structure (LD)<br>
- Separates overlapping signals<br>
- Provides probabilistic quantification<br>
- Can incorporate functional priors<br>
- Defines credible sets with clear interpretation</p>
<p><strong>Example interpretation</strong>:<br>
- 95% credible set contains 3 SNPs<br>
- Lead SNP has 80% probability of being causal<br>
- Probability of at least one causal SNP in the region: 99%</p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="cleaned_mpg5_files/figure-revealjs/unnamed-chunk-23-1.png" width="960"></p>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="practical-recommendations-for-bayesian-genomics" class="slide level2">
<h2>Practical Recommendations for Bayesian Genomics</h2>
<ol type="1">
<li><strong>Start with informative priors when possible</strong>
<ul>
<li>Use previous studies<br>
</li>
<li>Incorporate functional annotations<br>
</li>
<li>Consider evolutionary constraints</li>
</ul></li>
<li><strong>Report posterior probabilities, not just p-values</strong>
<ul>
<li><span class="math inline">\(P(\text{association} | \text{data})\)</span> is more interpretable than <span class="math inline">\(P(\text{data} | \text{no association})\)</span><br>
</li>
<li>Provides direct probability statements about hypotheses</li>
</ul></li>
<li><strong>Use conjugate models for computational efficiency</strong>
<ul>
<li>Beta-Binomial for allele frequencies<br>
</li>
<li>Dirichlet-Multinomial for haplotype frequencies<br>
</li>
<li>Normal-Normal for quantitative traits</li>
</ul></li>
<li><strong>Consider mixture models for complex data</strong>
<ul>
<li>Population structure<br>
</li>
<li>Heterogeneous effect sizes<br>
</li>
<li>Multiple causal variants</li>
</ul></li>
<li><strong>Apply decision theory for optimal designs</strong>
<ul>
<li>Balance false positives and false negatives<br>
</li>
<li>Consider costs of follow-up studies<br>
</li>
<li>Optimize sample allocation</li>
</ul></li>
</ol>
</section>
<section id="summary-when-to-use-bayesian-methods" class="slide level2">
<h2>Summary: When to Use Bayesian Methods</h2>
<table class="caption-top">
<colgroup>
<col style="width: 20%">
<col style="width: 33%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Problem</th>
<th>Bayesian Approach</th>
<th>Advantage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Multiple testing</td>
<td>Posterior probabilities</td>
<td>Direct interpretation, no arbitrary thresholds</td>
</tr>
<tr class="even">
<td>Sparse effects</td>
<td>Mixture priors</td>
<td>Better power, natural sparsity</td>
</tr>
<tr class="odd">
<td>Heterogeneous effects</td>
<td>Hierarchical models</td>
<td>Borrows strength across contexts</td>
</tr>
<tr class="even">
<td>Sequential data</td>
<td>Adaptive designs</td>
<td>Efficiency, early stopping</td>
</tr>
<tr class="odd">
<td>Prior knowledge integration</td>
<td>Informative priors</td>
<td>Improved accuracy, reduced sample size</td>
</tr>
<tr class="even">
<td>Complex dependencies</td>
<td>Bayesian networks</td>
<td>Causal inference, missing data handling</td>
</tr>
</tbody>
</table>
</section>
<section id="resources-for-bayesian-genomics" class="slide level2">
<h2>Resources for Bayesian Genomics</h2>
<p><strong>Software</strong>:<br>
- <strong>R packages</strong>: <code>rstan</code>, <code>brms</code>, <code>mashr</code>, <code>BGLR</code>, <code>INLA</code><br>
- <strong>Python</strong>: <code>PyMC3</code>, <code>Stan</code>, <code>Edward</code><br>
- <strong>Specialized</strong>: <code>STRUCTURE</code>, <code>ADMIXTURE</code>, <code>SNPTEST</code></p>
<p><strong>Books</strong>:<br>
- “Bayesian Data Analysis” by Gelman et al.<br>
- “Statistical Rethinking” by McElreath<br>
- “Bayesian Methods for Data Analysis” by Carlin &amp; Louis</p>
<p><strong>Online Resources</strong>:<br>
- <a href="https://stephenslab.github.io/fiveMinuteStats/">StatisticalGenetics.info</a><br>
- <a href="https://mc-stan.org/users/documentation/">Stan User Guide</a><br>
- <a href="https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Bayesian Methods for Hackers</a></p>
</section>
<section id="questions" class="slide level2">
<h2>Questions?</h2>
<p>Thank you!!</p>
<ul>
<li>Pradeep Natarajan, MD MMSc<br>
</li>
<li>Sasha Gusev, PhD ```</li>
</ul>
<div class="quarto-auto-generated-content">
<div class="footer footer-default">

</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="cleaned_mpg5_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="cleaned_mpg5_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="cleaned_mpg5_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="cleaned_mpg5_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="cleaned_mpg5_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="cleaned_mpg5_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="cleaned_mpg5_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="cleaned_mpg5_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="cleaned_mpg5_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="cleaned_mpg5_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>